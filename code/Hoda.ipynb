{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hoda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, MaxPooling2D, Conv2D\n",
    "\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2142 entries, 0 to 2141\n",
      "Data columns (total 13 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   Online Post ID          2142 non-null   int64  \n",
      " 1   Twitter ID              1673 non-null   float64\n",
      " 2   Related Online Post ID  333 non-null    float64\n",
      " 3   Source ID               2142 non-null   object \n",
      " 4   Online Post Text        469 non-null    object \n",
      " 5   Subjectivity            2142 non-null   int64  \n",
      " 6   Sentiment Polarity      2142 non-null   object \n",
      " 7   Emotion                 2142 non-null   object \n",
      " 8   Sarcasm                 2142 non-null   int64  \n",
      " 9   Irony                   2142 non-null   int64  \n",
      " 10  Negation                2142 non-null   int64  \n",
      " 11  Off-topic               2142 non-null   int64  \n",
      " 12  Language                2142 non-null   int64  \n",
      "dtypes: float64(2), int64(7), object(4)\n",
      "memory usage: 217.7+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2316 entries, 0 to 2315\n",
      "Data columns (total 13 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   Online Post ID          2316 non-null   int64  \n",
      " 1   Twitter ID              1677 non-null   float64\n",
      " 2   Related Online Post ID  448 non-null    float64\n",
      " 3   Source ID               2316 non-null   object \n",
      " 4   Online Post Text        639 non-null    object \n",
      " 5   Subjectivity            2316 non-null   int64  \n",
      " 6   Sentiment Polarity      2316 non-null   object \n",
      " 7   Emotion                 2316 non-null   object \n",
      " 8   Sarcasm                 2316 non-null   int64  \n",
      " 9   Irony                   2316 non-null   int64  \n",
      " 10  Negation                2316 non-null   int64  \n",
      " 11  Off-topic               2316 non-null   int64  \n",
      " 12  Language                2316 non-null   int64  \n",
      "dtypes: float64(2), int64(7), object(4)\n",
      "memory usage: 235.3+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1929 entries, 0 to 1928\n",
      "Data columns (total 13 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   Online Post ID          1929 non-null   int64  \n",
      " 1   Twitter ID              1314 non-null   float64\n",
      " 2   Related Online Post ID  396 non-null    float64\n",
      " 3   Source ID               1929 non-null   object \n",
      " 4   Online Post Text        615 non-null    object \n",
      " 5   Subjectivity            1929 non-null   int64  \n",
      " 6   Sentiment Polarity      1929 non-null   object \n",
      " 7   Emotion                 1929 non-null   object \n",
      " 8   Sarcasm                 1929 non-null   int64  \n",
      " 9   Irony                   1929 non-null   int64  \n",
      " 10  Negation                1929 non-null   int64  \n",
      " 11  Off-topic               1929 non-null   int64  \n",
      " 12  Language                1929 non-null   int64  \n",
      "dtypes: float64(2), int64(7), object(4)\n",
      "memory usage: 196.0+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6387 entries, 0 to 6386\n",
      "Data columns (total 13 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   Online Post ID          6387 non-null   int64  \n",
      " 1   Twitter ID              4664 non-null   float64\n",
      " 2   Related Online Post ID  1177 non-null   float64\n",
      " 3   Source ID               6387 non-null   object \n",
      " 4   Online Post Text        1723 non-null   object \n",
      " 5   Subjectivity            6387 non-null   int64  \n",
      " 6   Sentiment Polarity      6387 non-null   object \n",
      " 7   Emotion                 6387 non-null   object \n",
      " 8   Sarcasm                 6387 non-null   int64  \n",
      " 9   Irony                   6387 non-null   int64  \n",
      " 10  Negation                6387 non-null   int64  \n",
      " 11  Off-topic               6387 non-null   int64  \n",
      " 12  Language                6387 non-null   int64  \n",
      "dtypes: float64(2), int64(7), object(4)\n",
      "memory usage: 648.8+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>Sentiment Polarity</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Sarcasm</th>\n",
       "      <th>Irony</th>\n",
       "      <th>Negation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20180001</td>\n",
       "      <td>Great BUDGET . Even cigarettes were not touche...</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>trust</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20180002</td>\n",
       "      <td>I haven't exactly scanned the budget throughou...</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>disgust</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20180003</td>\n",
       "      <td>Thereâ€™s already smoking cessation programs for...</td>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20180004</td>\n",
       "      <td>So should alcohol and fuel for private vehicle...</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20180005</td>\n",
       "      <td>Practical? You should say that in a third worl...</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>anger</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID                                               Text  Subjectivity  \\\n",
       "0  20180001  Great BUDGET . Even cigarettes were not touche...             1   \n",
       "1  20180002  I haven't exactly scanned the budget throughou...             1   \n",
       "2  20180003  Thereâ€™s already smoking cessation programs for...             1   \n",
       "3  20180004  So should alcohol and fuel for private vehicle...             1   \n",
       "4  20180005  Practical? You should say that in a third worl...             1   \n",
       "\n",
       "  Sentiment Polarity       Emotion  Sarcasm  Irony  Negation  \n",
       "0           positive         trust        0      0         1  \n",
       "1           negative       disgust        0      0         1  \n",
       "2            neutral  anticipation        0      0         0  \n",
       "3           negative       sadness        0      0         0  \n",
       "4           negative         anger        0      0         1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "malta_loc_18 = '../data/Malta-Budget-2018-dataset-v1.csv'\n",
    "malta_loc_19 = '../data/Malta-Budget-2019-dataset-v1.csv'\n",
    "malta_loc_20 = '../data/Malta-Budget-2020-dataset-v1.csv'\n",
    "\n",
    "malta_data_18 = pd.read_csv(malta_loc_18)\n",
    "malta_data_19 = pd.read_csv(malta_loc_19)\n",
    "malta_data_20 = pd.read_csv(malta_loc_20)\n",
    "\n",
    "print(malta_data_18.info())\n",
    "print(malta_data_19.info())\n",
    "print(malta_data_20.info())\n",
    "\n",
    "malta_data_19 = malta_data_19.rename(columns={'Off-topic ':'Off-topic'})\n",
    "combined_data = pd.concat([malta_data_18, malta_data_19, malta_data_20], ignore_index=True)\n",
    "combined_data.info()\n",
    "\n",
    "clean_data = combined_data.dropna(subset=['Online Post Text'])\n",
    "clean_data = clean_data.drop(['Twitter ID', 'Related Online Post ID', 'Source ID','Off-topic'], axis=1)\n",
    "clean_data = clean_data[clean_data['Language'] == 0] # get all data that is in english \n",
    "clean_data = clean_data.drop(['Language'], axis=1)\n",
    "clean_data = clean_data.rename(columns={'Online Post ID':'ID','Online Post Text':'Text'})\n",
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest record is : 171 words\n",
      "The longest record is : 171 tokens\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>Sentiment Polarity</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Sarcasm</th>\n",
       "      <th>Irony</th>\n",
       "      <th>Negation</th>\n",
       "      <th>tokens</th>\n",
       "      <th>padded</th>\n",
       "      <th>masks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20180001</td>\n",
       "      <td>great budget even cigarette touched great work...</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>trust</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[great, budget, even, cigarette, touched, grea...</td>\n",
       "      <td>[101, 2307, 5166, 2130, 9907, 5028, 2307, 2147...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20180002</td>\n",
       "      <td>exactly scanned budget throughout earth make i...</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>disgust</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[exactly, scanned, budget, throughout, earth, ...</td>\n",
       "      <td>[101, 3599, 11728, 5166, 2802, 3011, 2191, 166...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20180003</td>\n",
       "      <td>already smoking cessation program people want ...</td>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[already, smoking, ce, ##ssa, ##tion, program,...</td>\n",
       "      <td>[101, 2525, 9422, 8292, 11488, 3508, 2565, 211...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20180004</td>\n",
       "      <td>alcohol fuel private vehicle raising tax cigar...</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[alcohol, fuel, private, vehicle, raising, tax...</td>\n",
       "      <td>[101, 6544, 4762, 2797, 4316, 6274, 4171, 9907...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20180005</td>\n",
       "      <td>practical say third world country supposed eur...</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>anger</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[practical, say, third, world, country, suppos...</td>\n",
       "      <td>[101, 6742, 2360, 2353, 2088, 2406, 4011, 2647...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID                                               Text  Subjectivity  \\\n",
       "0  20180001  great budget even cigarette touched great work...             1   \n",
       "1  20180002  exactly scanned budget throughout earth make i...             1   \n",
       "2  20180003  already smoking cessation program people want ...             1   \n",
       "3  20180004  alcohol fuel private vehicle raising tax cigar...             1   \n",
       "4  20180005  practical say third world country supposed eur...             1   \n",
       "\n",
       "  Sentiment Polarity       Emotion  Sarcasm  Irony  Negation  \\\n",
       "0           positive         trust        0      0         1   \n",
       "1           negative       disgust        0      0         1   \n",
       "2            neutral  anticipation        0      0         0   \n",
       "3           negative       sadness        0      0         0   \n",
       "4           negative         anger        0      0         1   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [great, budget, even, cigarette, touched, grea...   \n",
       "1  [exactly, scanned, budget, throughout, earth, ...   \n",
       "2  [already, smoking, ce, ##ssa, ##tion, program,...   \n",
       "3  [alcohol, fuel, private, vehicle, raising, tax...   \n",
       "4  [practical, say, third, world, country, suppos...   \n",
       "\n",
       "                                              padded  \\\n",
       "0  [101, 2307, 5166, 2130, 9907, 5028, 2307, 2147...   \n",
       "1  [101, 3599, 11728, 5166, 2802, 3011, 2191, 166...   \n",
       "2  [101, 2525, 9422, 8292, 11488, 3508, 2565, 211...   \n",
       "3  [101, 6544, 4762, 2797, 4316, 6274, 4171, 9907...   \n",
       "4  [101, 6742, 2360, 2353, 2088, 2406, 4011, 2647...   \n",
       "\n",
       "                                               masks  \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import contractions\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "processed_data = clean_data.copy(deep=True)\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    pattern = re.compile(r'[^a-zA-Z\\s]')\n",
    "    return pattern.sub('', text)\n",
    "\n",
    "# Remove URLs and HTML tags\n",
    "processed_data['Text'] = processed_data['Text'].str.replace(r'http\\S+|www\\S+|https\\S+', '', regex=True)\n",
    "processed_data['Text'] = processed_data['Text'].str.replace(r'<.*?>', '', regex=True)\n",
    "\n",
    "# Expand contractions\n",
    "processed_data['Text'] = processed_data['Text'].apply(lambda x: contractions.fix(x))\n",
    "\n",
    "# Convert to lowercase\n",
    "processed_data['Text'] = processed_data['Text'].str.lower()\n",
    "\n",
    "# Remove punctuation\n",
    "processed_data['Text'] = processed_data['Text'].str.replace(f\"[{string.punctuation}]\", \" \", regex=True)\n",
    "\n",
    "# Remove numbers\n",
    "processed_data['Text'] = processed_data['Text'].str.replace(r'\\d+', '', regex=True)\n",
    "\n",
    "# Remove special characters\n",
    "processed_data['Text'] = processed_data['Text'].apply(remove_special_characters)\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "processed_data['Text'] = processed_data['Text'].apply(lambda x: ' '.join(word for word in x.split() if word not in stop_words))\n",
    "\n",
    "# Remove extra whitespace\n",
    "processed_data['Text'] = processed_data['Text'].str.strip()\n",
    "processed_data['Text'] = processed_data['Text'].str.replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "# Lemmatize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "processed_data['Text'] = processed_data['Text'].apply(lambda x: ' '.join(lemmatizer.lemmatize(word) for word in x.split()))\n",
    "\n",
    "# Tokenize\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\") #96574\n",
    "# tokenizer_features = 110000\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') #30522 \n",
    "tokenizer_features = 30522\n",
    "processed_data['tokens'] = processed_data['Text'].apply(lambda x: tokenizer.tokenize(x)) \n",
    "\n",
    "max_words = processed_data['Text'].apply(lambda x: len(x.split())).max()\n",
    "print(f\"The longest record is : {max_words} words\")\n",
    "\n",
    "max_tokens = processed_data['tokens'].apply(lambda x: len(x)).max()\n",
    "print(f\"The longest record is : {max_words} tokens\")\n",
    "\n",
    "def encode_texts(texts, tokenizer, max_len): \n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for text in texts:\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=False,\n",
    "            truncation=True\n",
    "        )\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "    \n",
    "    return input_ids, attention_masks\n",
    "\n",
    "processed_data['padded'], processed_data['masks'] = encode_texts(processed_data['Text'].tolist(), tokenizer, 100)\n",
    "\n",
    "# Encode\n",
    "# processed_data['encoded'] = processed_data['tokens'].apply(lambda x: tokenizer.convert_tokens_to_ids(x))\n",
    "\n",
    "processed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
