{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, MaxPooling2D, Conv2D\n",
    "\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from keras.layers import LSTM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2142 entries, 0 to 2141\n",
      "Data columns (total 13 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   Online Post ID          2142 non-null   int64  \n",
      " 1   Twitter ID              1673 non-null   float64\n",
      " 2   Related Online Post ID  333 non-null    float64\n",
      " 3   Source ID               2142 non-null   object \n",
      " 4   Online Post Text        469 non-null    object \n",
      " 5   Subjectivity            2142 non-null   int64  \n",
      " 6   Sentiment Polarity      2142 non-null   object \n",
      " 7   Emotion                 2142 non-null   object \n",
      " 8   Sarcasm                 2142 non-null   int64  \n",
      " 9   Irony                   2142 non-null   int64  \n",
      " 10  Negation                2142 non-null   int64  \n",
      " 11  Off-topic               2142 non-null   int64  \n",
      " 12  Language                2142 non-null   int64  \n",
      "dtypes: float64(2), int64(7), object(4)\n",
      "memory usage: 217.7+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2316 entries, 0 to 2315\n",
      "Data columns (total 13 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   Online Post ID          2316 non-null   int64  \n",
      " 1   Twitter ID              1677 non-null   float64\n",
      " 2   Related Online Post ID  448 non-null    float64\n",
      " 3   Source ID               2316 non-null   object \n",
      " 4   Online Post Text        639 non-null    object \n",
      " 5   Subjectivity            2316 non-null   int64  \n",
      " 6   Sentiment Polarity      2316 non-null   object \n",
      " 7   Emotion                 2316 non-null   object \n",
      " 8   Sarcasm                 2316 non-null   int64  \n",
      " 9   Irony                   2316 non-null   int64  \n",
      " 10  Negation                2316 non-null   int64  \n",
      " 11  Off-topic               2316 non-null   int64  \n",
      " 12  Language                2316 non-null   int64  \n",
      "dtypes: float64(2), int64(7), object(4)\n",
      "memory usage: 235.3+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1929 entries, 0 to 1928\n",
      "Data columns (total 13 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   Online Post ID          1929 non-null   int64  \n",
      " 1   Twitter ID              1314 non-null   float64\n",
      " 2   Related Online Post ID  396 non-null    float64\n",
      " 3   Source ID               1929 non-null   object \n",
      " 4   Online Post Text        615 non-null    object \n",
      " 5   Subjectivity            1929 non-null   int64  \n",
      " 6   Sentiment Polarity      1929 non-null   object \n",
      " 7   Emotion                 1929 non-null   object \n",
      " 8   Sarcasm                 1929 non-null   int64  \n",
      " 9   Irony                   1929 non-null   int64  \n",
      " 10  Negation                1929 non-null   int64  \n",
      " 11  Off-topic               1929 non-null   int64  \n",
      " 12  Language                1929 non-null   int64  \n",
      "dtypes: float64(2), int64(7), object(4)\n",
      "memory usage: 196.0+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6387 entries, 0 to 6386\n",
      "Data columns (total 13 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   Online Post ID          6387 non-null   int64  \n",
      " 1   Twitter ID              4664 non-null   float64\n",
      " 2   Related Online Post ID  1177 non-null   float64\n",
      " 3   Source ID               6387 non-null   object \n",
      " 4   Online Post Text        1723 non-null   object \n",
      " 5   Subjectivity            6387 non-null   int64  \n",
      " 6   Sentiment Polarity      6387 non-null   object \n",
      " 7   Emotion                 6387 non-null   object \n",
      " 8   Sarcasm                 6387 non-null   int64  \n",
      " 9   Irony                   6387 non-null   int64  \n",
      " 10  Negation                6387 non-null   int64  \n",
      " 11  Off-topic               6387 non-null   int64  \n",
      " 12  Language                6387 non-null   int64  \n",
      "dtypes: float64(2), int64(7), object(4)\n",
      "memory usage: 648.8+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>Sentiment Polarity</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Sarcasm</th>\n",
       "      <th>Irony</th>\n",
       "      <th>Negation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20180001</td>\n",
       "      <td>Great BUDGET . Even cigarettes were not touche...</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>trust</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20180002</td>\n",
       "      <td>I haven't exactly scanned the budget throughou...</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>disgust</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20180003</td>\n",
       "      <td>There’s already smoking cessation programs for...</td>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20180004</td>\n",
       "      <td>So should alcohol and fuel for private vehicle...</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20180005</td>\n",
       "      <td>Practical? You should say that in a third worl...</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>anger</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID                                               Text  Subjectivity  \\\n",
       "0  20180001  Great BUDGET . Even cigarettes were not touche...             1   \n",
       "1  20180002  I haven't exactly scanned the budget throughou...             1   \n",
       "2  20180003  There’s already smoking cessation programs for...             1   \n",
       "3  20180004  So should alcohol and fuel for private vehicle...             1   \n",
       "4  20180005  Practical? You should say that in a third worl...             1   \n",
       "\n",
       "  Sentiment Polarity       Emotion  Sarcasm  Irony  Negation  \n",
       "0           positive         trust        0      0         1  \n",
       "1           negative       disgust        0      0         1  \n",
       "2            neutral  anticipation        0      0         0  \n",
       "3           negative       sadness        0      0         0  \n",
       "4           negative         anger        0      0         1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "malta_loc_18 = '../data/Malta-Budget-2018-dataset-v1.csv'\n",
    "malta_loc_19 = '../data/Malta-Budget-2019-dataset-v1.csv'\n",
    "malta_loc_20 = '../data/Malta-Budget-2020-dataset-v1.csv'\n",
    "\n",
    "malta_data_18 = pd.read_csv(malta_loc_18)\n",
    "malta_data_19 = pd.read_csv(malta_loc_19)\n",
    "malta_data_20 = pd.read_csv(malta_loc_20)\n",
    "\n",
    "print(malta_data_18.info())\n",
    "print(malta_data_19.info())\n",
    "print(malta_data_20.info())\n",
    "\n",
    "malta_data_19 = malta_data_19.rename(columns={'Off-topic ':'Off-topic'})\n",
    "combined_data = pd.concat([malta_data_18, malta_data_19, malta_data_20], ignore_index=True)\n",
    "combined_data.info()\n",
    "\n",
    "clean_data = combined_data.dropna(subset=['Online Post Text'])\n",
    "clean_data = clean_data.drop(['Twitter ID', 'Related Online Post ID', 'Source ID','Off-topic'], axis=1)\n",
    "clean_data = clean_data[clean_data['Language'] == 0] # get all data that is in english \n",
    "clean_data = clean_data.drop(['Language'], axis=1)\n",
    "clean_data = clean_data.rename(columns={'Online Post ID':'ID','Online Post Text':'Text'})\n",
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/alex/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest record is : 171 words\n",
      "The longest record is : 171 tokens\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>Sentiment Polarity</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Sarcasm</th>\n",
       "      <th>Irony</th>\n",
       "      <th>Negation</th>\n",
       "      <th>tokens</th>\n",
       "      <th>padded</th>\n",
       "      <th>masks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20180001</td>\n",
       "      <td>great budget even cigarette touched great work...</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>trust</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[great, budget, even, cigarette, touched, grea...</td>\n",
       "      <td>[101, 2307, 5166, 2130, 9907, 5028, 2307, 2147...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20180002</td>\n",
       "      <td>exactly scanned budget throughout earth make i...</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>disgust</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[exactly, scanned, budget, throughout, earth, ...</td>\n",
       "      <td>[101, 3599, 11728, 5166, 2802, 3011, 2191, 166...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20180003</td>\n",
       "      <td>already smoking cessation program people want ...</td>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[already, smoking, ce, ##ssa, ##tion, program,...</td>\n",
       "      <td>[101, 2525, 9422, 8292, 11488, 3508, 2565, 211...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20180004</td>\n",
       "      <td>alcohol fuel private vehicle raising tax cigar...</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[alcohol, fuel, private, vehicle, raising, tax...</td>\n",
       "      <td>[101, 6544, 4762, 2797, 4316, 6274, 4171, 9907...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20180005</td>\n",
       "      <td>practical say third world country supposed eur...</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>anger</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[practical, say, third, world, country, suppos...</td>\n",
       "      <td>[101, 6742, 2360, 2353, 2088, 2406, 4011, 2647...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID                                               Text  Subjectivity  \\\n",
       "0  20180001  great budget even cigarette touched great work...             1   \n",
       "1  20180002  exactly scanned budget throughout earth make i...             1   \n",
       "2  20180003  already smoking cessation program people want ...             1   \n",
       "3  20180004  alcohol fuel private vehicle raising tax cigar...             1   \n",
       "4  20180005  practical say third world country supposed eur...             1   \n",
       "\n",
       "  Sentiment Polarity       Emotion  Sarcasm  Irony  Negation  \\\n",
       "0           positive         trust        0      0         1   \n",
       "1           negative       disgust        0      0         1   \n",
       "2            neutral  anticipation        0      0         0   \n",
       "3           negative       sadness        0      0         0   \n",
       "4           negative         anger        0      0         1   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [great, budget, even, cigarette, touched, grea...   \n",
       "1  [exactly, scanned, budget, throughout, earth, ...   \n",
       "2  [already, smoking, ce, ##ssa, ##tion, program,...   \n",
       "3  [alcohol, fuel, private, vehicle, raising, tax...   \n",
       "4  [practical, say, third, world, country, suppos...   \n",
       "\n",
       "                                              padded  \\\n",
       "0  [101, 2307, 5166, 2130, 9907, 5028, 2307, 2147...   \n",
       "1  [101, 3599, 11728, 5166, 2802, 3011, 2191, 166...   \n",
       "2  [101, 2525, 9422, 8292, 11488, 3508, 2565, 211...   \n",
       "3  [101, 6544, 4762, 2797, 4316, 6274, 4171, 9907...   \n",
       "4  [101, 6742, 2360, 2353, 2088, 2406, 4011, 2647...   \n",
       "\n",
       "                                               masks  \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import contractions\n",
    "from transformers import BertTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "processed_data = clean_data.copy(deep=True)\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    pattern = re.compile(r'[^a-zA-Z\\s]')\n",
    "    return pattern.sub('', text)\n",
    "\n",
    "# Remove URLs and HTML tags\n",
    "processed_data['Text'] = processed_data['Text'].str.replace(r'http\\S+|www\\S+|https\\S+', '', regex=True)\n",
    "processed_data['Text'] = processed_data['Text'].str.replace(r'<.*?>', '', regex=True)\n",
    "\n",
    "# Expand contractions\n",
    "processed_data['Text'] = processed_data['Text'].apply(lambda x: contractions.fix(x))\n",
    "\n",
    "# Convert to lowercase\n",
    "processed_data['Text'] = processed_data['Text'].str.lower()\n",
    "\n",
    "# Remove punctuation\n",
    "processed_data['Text'] = processed_data['Text'].str.replace(f\"[{string.punctuation}]\", \" \", regex=True)\n",
    "\n",
    "# Remove numbers\n",
    "processed_data['Text'] = processed_data['Text'].str.replace(r'\\d+', '', regex=True)\n",
    "\n",
    "# Remove special characters\n",
    "processed_data['Text'] = processed_data['Text'].apply(remove_special_characters)\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "processed_data['Text'] = processed_data['Text'].apply(lambda x: ' '.join(word for word in x.split() if word not in stop_words))\n",
    "\n",
    "# Remove extra whitespace\n",
    "processed_data['Text'] = processed_data['Text'].str.strip()\n",
    "processed_data['Text'] = processed_data['Text'].str.replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "# Lemmatize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "processed_data['Text'] = processed_data['Text'].apply(lambda x: ' '.join(lemmatizer.lemmatize(word) for word in x.split()))\n",
    "\n",
    "# Tokenize\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\") #96574\n",
    "# tokenizer_features = 110000\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') #30522 \n",
    "tokenizer_features = 30522\n",
    "processed_data['tokens'] = processed_data['Text'].apply(lambda x: tokenizer.tokenize(x)) \n",
    "\n",
    "max_words = processed_data['Text'].apply(lambda x: len(x.split())).max()\n",
    "print(f\"The longest record is : {max_words} words\")\n",
    "\n",
    "max_tokens = processed_data['tokens'].apply(lambda x: len(x)).max()\n",
    "print(f\"The longest record is : {max_words} tokens\")\n",
    "\n",
    "def encode_texts(texts, tokenizer, max_len): \n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for text in texts:\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=False,\n",
    "            truncation=True\n",
    "        )\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "    \n",
    "    return input_ids, attention_masks\n",
    "\n",
    "processed_data['padded'], processed_data['masks'] = encode_texts(processed_data['Text'].tolist(), tokenizer, 100)\n",
    "\n",
    "# Encode\n",
    "# processed_data['encoded'] = processed_data['tokens'].apply(lambda x: tokenizer.convert_tokens_to_ids(x))\n",
    "\n",
    "processed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional, LSTM, Embedding, Concatenate\n",
    "from keras.layers import Layer\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='attention_weight', shape=(input_shape[-1], 1),\n",
    "                                 initializer='random_normal', trainable=True)\n",
    "        self.b = self.add_weight(name='attention_bias', shape=(input_shape[1], 1),\n",
    "                                 initializer='zeros', trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        # Alignment scores. Pass them through tanh function\n",
    "        e = K.tanh(K.dot(x,self.W)+self.b)\n",
    "        # Remove dimension of size 1\n",
    "        e = K.squeeze(e, axis=-1)   \n",
    "        # Compute the weights\n",
    "        alpha = K.softmax(e)\n",
    "        # Reshape to tensorFlow format\n",
    "        alpha = K.expand_dims(alpha, axis=-1)\n",
    "        # Compute the context vector\n",
    "        context = x * alpha\n",
    "        context = K.sum(context, axis=1)\n",
    "        return context\n",
    "    \n",
    "    \n",
    "def build_and_train_model(df, max_features, maximum_length=100, embedding_dim=256, dropout_rate=0.5, lstm_units=60, dense=None, batch_size=16, epochs=20, verbose=0):\n",
    "    # Compute class weights\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(df['Emotion']), y=df['Emotion'])\n",
    "    class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "    # Preprocess features\n",
    "    X_text = np.asarray(df['padded'].to_list())\n",
    "    X_sarcasm = df['Sarcasm'].values.reshape(-1, 1)\n",
    "    X_irony = df['Irony'].values.reshape(-1, 1)\n",
    "    X_negation = df['Negation'].values.reshape(-1, 1)\n",
    "    X_sentiment_polarity = OneHotEncoder().fit_transform(df[['Sentiment Polarity']]).toarray()\n",
    "    # X_sentiment_polarity = OneHotEncoder().fit_transform(df[['Sentiment Polarity']])\n",
    "    y = pd.get_dummies(df['Emotion']).values\n",
    "\n",
    "    # Split data\n",
    "    X_text_train, X_text_val, X_sarcasm_train, X_sarcasm_val, X_irony_train, X_irony_val, X_negation_train, X_negation_val, X_sentiment_polarity_train, X_sentiment_polarity_val, y_train, y_val = train_test_split(\n",
    "        X_text, X_sarcasm, X_irony, X_negation, X_sentiment_polarity, y, test_size=0.1, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # Define model inputs\n",
    "    text_input = Input(shape=(maximum_length,), dtype='int32', name='text_input')\n",
    "    sarcasm_input = Input(shape=(1,), dtype='int32', name='sarcasm_input')\n",
    "    irony_input = Input(shape=(1,), dtype='int32', name='irony_input')\n",
    "    negation_input = Input(shape=(1,), dtype='int32', name='negation_input')\n",
    "    sentiment_polarity_input = Input(shape=(X_sentiment_polarity_train.shape[1],), dtype='int32', name='sentiment_polarity_input')\n",
    "    inputs = [text_input, sarcasm_input, irony_input, negation_input, sentiment_polarity_input]\n",
    "\n",
    "    # Text processing\n",
    "    x = Embedding(input_dim=max_features, output_dim=embedding_dim)(text_input)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = Bidirectional(LSTM(lstm_units, return_sequences=True))(x)\n",
    "    x = Attention()(x)\n",
    "\n",
    "    # Concatenate additional features\n",
    "    x = Concatenate()([x, sarcasm_input, irony_input, negation_input, sentiment_polarity_input])\n",
    "    if dense != None:\n",
    "        x = Dense(dense, activation='relu')(x)\n",
    "    outputs = Dense(y.shape[1], activation='softmax')(x)\n",
    "\n",
    "    # Create and compile the model\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    if verbose != 0:\n",
    "        model.summary()\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        [X_text_train, X_sarcasm_train, X_irony_train, X_negation_train, X_sentiment_polarity_train], y_train,\n",
    "        batch_size=batch_size, epochs=epochs,\n",
    "        validation_data=([X_text_val, X_sarcasm_val, X_irony_val, X_negation_val, X_sentiment_polarity_val], y_val),\n",
    "        class_weight=class_weights,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=15, dense_layer=None, batch_size=32\n",
      "Value accuracy: 0.1785714328289032.  Top accuracy: 0.2767857015132904 at epoch: 3\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=15, dense_layer=None, batch_size=64\n",
      "Value accuracy: 0.2053571492433548.  Top accuracy: 0.3214285671710968 at epoch: 3\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=15, dense_layer=None, batch_size=128\n",
      "Value accuracy: 0.2857142984867096.  Top accuracy: 0.3035714328289032 at epoch: 15\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=15, dense_layer=None, batch_size=256\n",
      "Value accuracy: 0.0892857164144516.  Top accuracy: 0.3214285671710968 at epoch: 11\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=15, dense_layer=8, batch_size=32\n",
      "Value accuracy: 0.2142857164144516.  Top accuracy: 0.3125 at epoch: 3\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=15, dense_layer=8, batch_size=64\n",
      "Value accuracy: 0.2678571343421936.  Top accuracy: 0.3035714328289032 at epoch: 16\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=15, dense_layer=8, batch_size=128\n",
      "Value accuracy: 0.3125.  Top accuracy: 0.3571428656578064 at epoch: 16\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=15, dense_layer=8, batch_size=256\n",
      "Value accuracy: 0.1785714328289032.  Top accuracy: 0.1875 at epoch: 2\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=15, dense_layer=16, batch_size=32\n",
      "Value accuracy: 0.2321428507566452.  Top accuracy: 0.3303571343421936 at epoch: 11\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=15, dense_layer=16, batch_size=64\n",
      "Value accuracy: 0.2142857164144516.  Top accuracy: 0.2767857015132904 at epoch: 18\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=15, dense_layer=16, batch_size=128\n",
      "Value accuracy: 0.2857142984867096.  Top accuracy: 0.3482142984867096 at epoch: 8\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=15, dense_layer=16, batch_size=256\n",
      "Value accuracy: 0.3571428656578064.  Top accuracy: 0.3839285671710968 at epoch: 11\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=15, dense_layer=32, batch_size=32\n",
      "Value accuracy: 0.2767857015132904.  Top accuracy: 0.4017857015132904 at epoch: 4\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=15, dense_layer=32, batch_size=64\n",
      "Value accuracy: 0.3303571343421936.  Top accuracy: 0.3660714328289032 at epoch: 15\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=15, dense_layer=32, batch_size=128\n",
      "Value accuracy: 0.2410714328289032.  Top accuracy: 0.3125 at epoch: 9\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=15, dense_layer=32, batch_size=256\n",
      "Value accuracy: 0.3303571343421936.  Top accuracy: 0.3303571343421936 at epoch: 16\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=15, dense_layer=64, batch_size=32\n",
      "Value accuracy: 0.2142857164144516.  Top accuracy: 0.3482142984867096 at epoch: 6\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=15, dense_layer=64, batch_size=64\n",
      "Value accuracy: 0.3303571343421936.  Top accuracy: 0.3928571343421936 at epoch: 3\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=15, dense_layer=64, batch_size=128\n",
      "Value accuracy: 0.3571428656578064.  Top accuracy: 0.4017857015132904 at epoch: 8\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=15, dense_layer=64, batch_size=256\n",
      "Value accuracy: 0.2321428507566452.  Top accuracy: 0.3125 at epoch: 9\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=15, dense_layer=128, batch_size=32\n",
      "Value accuracy: 0.2589285671710968.  Top accuracy: 0.4285714328289032 at epoch: 7\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=15, dense_layer=128, batch_size=64\n",
      "Value accuracy: 0.3214285671710968.  Top accuracy: 0.3660714328289032 at epoch: 7\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=15, dense_layer=128, batch_size=128\n",
      "Value accuracy: 0.3392857015132904.  Top accuracy: 0.4017857015132904 at epoch: 7\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=15, dense_layer=128, batch_size=256\n",
      "Value accuracy: 0.3660714328289032.  Top accuracy: 0.3928571343421936 at epoch: 10\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=15, dense_layer=256, batch_size=32\n",
      "Value accuracy: 0.3214285671710968.  Top accuracy: 0.4017857015132904 at epoch: 3\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=15, dense_layer=256, batch_size=64\n",
      "Value accuracy: 0.3125.  Top accuracy: 0.3839285671710968 at epoch: 2\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=15, dense_layer=256, batch_size=128\n",
      "Value accuracy: 0.3214285671710968.  Top accuracy: 0.4285714328289032 at epoch: 4\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=15, dense_layer=256, batch_size=256\n",
      "Value accuracy: 0.3303571343421936.  Top accuracy: 0.3660714328289032 at epoch: 17\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=30, dense_layer=None, batch_size=32\n",
      "Value accuracy: 0.2410714328289032.  Top accuracy: 0.2946428656578064 at epoch: 15\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=30, dense_layer=None, batch_size=64\n",
      "Value accuracy: 0.25.  Top accuracy: 0.3392857015132904 at epoch: 13\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=30, dense_layer=None, batch_size=128\n",
      "Value accuracy: 0.2232142835855484.  Top accuracy: 0.2678571343421936 at epoch: 2\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=30, dense_layer=None, batch_size=256\n",
      "Value accuracy: 0.0982142835855484.  Top accuracy: 0.2410714328289032 at epoch: 16\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=30, dense_layer=8, batch_size=32\n",
      "Value accuracy: 0.2232142835855484.  Top accuracy: 0.2589285671710968 at epoch: 17\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=30, dense_layer=8, batch_size=64\n",
      "Value accuracy: 0.2410714328289032.  Top accuracy: 0.3214285671710968 at epoch: 14\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=30, dense_layer=8, batch_size=128\n",
      "Value accuracy: 0.2321428507566452.  Top accuracy: 0.3392857015132904 at epoch: 2\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=30, dense_layer=8, batch_size=256\n",
      "Value accuracy: 0.1339285671710968.  Top accuracy: 0.2678571343421936 at epoch: 9\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=30, dense_layer=16, batch_size=32\n",
      "Value accuracy: 0.2589285671710968.  Top accuracy: 0.3214285671710968 at epoch: 17\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=30, dense_layer=16, batch_size=64\n",
      "Value accuracy: 0.25.  Top accuracy: 0.3839285671710968 at epoch: 9\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=30, dense_layer=16, batch_size=128\n",
      "Value accuracy: 0.2053571492433548.  Top accuracy: 0.3392857015132904 at epoch: 12\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=30, dense_layer=16, batch_size=256\n",
      "Value accuracy: 0.1607142835855484.  Top accuracy: 0.3035714328289032 at epoch: 15\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=30, dense_layer=32, batch_size=32\n",
      "Value accuracy: 0.3303571343421936.  Top accuracy: 0.4017857015132904 at epoch: 5\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=30, dense_layer=32, batch_size=64\n",
      "Value accuracy: 0.2857142984867096.  Top accuracy: 0.3839285671710968 at epoch: 7\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=30, dense_layer=32, batch_size=128\n",
      "Value accuracy: 0.2946428656578064.  Top accuracy: 0.3660714328289032 at epoch: 9\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=30, dense_layer=32, batch_size=256\n",
      "Value accuracy: 0.2857142984867096.  Top accuracy: 0.3482142984867096 at epoch: 14\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=30, dense_layer=64, batch_size=32\n",
      "Value accuracy: 0.2946428656578064.  Top accuracy: 0.3660714328289032 at epoch: 8\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=30, dense_layer=64, batch_size=64\n",
      "Value accuracy: 0.3303571343421936.  Top accuracy: 0.4017857015132904 at epoch: 0\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=30, dense_layer=64, batch_size=128\n",
      "Value accuracy: 0.2589285671710968.  Top accuracy: 0.375 at epoch: 7\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=30, dense_layer=64, batch_size=256\n",
      "Value accuracy: 0.3392857015132904.  Top accuracy: 0.375 at epoch: 15\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=30, dense_layer=128, batch_size=32\n",
      "Value accuracy: 0.2142857164144516.  Top accuracy: 0.4196428656578064 at epoch: 6\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=30, dense_layer=128, batch_size=64\n",
      "Value accuracy: 0.3125.  Top accuracy: 0.4107142984867096 at epoch: 4\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=30, dense_layer=128, batch_size=128\n",
      "Value accuracy: 0.4196428656578064.  Top accuracy: 0.4285714328289032 at epoch: 3\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=30, dense_layer=128, batch_size=256\n",
      "Value accuracy: 0.3571428656578064.  Top accuracy: 0.4285714328289032 at epoch: 6\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=30, dense_layer=256, batch_size=32\n",
      "Value accuracy: 0.2857142984867096.  Top accuracy: 0.4017857015132904 at epoch: 0\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=30, dense_layer=256, batch_size=64\n",
      "Value accuracy: 0.3214285671710968.  Top accuracy: 0.3660714328289032 at epoch: 5\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=30, dense_layer=256, batch_size=128\n",
      "Value accuracy: 0.3035714328289032.  Top accuracy: 0.3928571343421936 at epoch: 7\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=30, dense_layer=256, batch_size=256\n",
      "Value accuracy: 0.375.  Top accuracy: 0.4285714328289032 at epoch: 4\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=60, dense_layer=None, batch_size=32\n",
      "Value accuracy: 0.1785714328289032.  Top accuracy: 0.3035714328289032 at epoch: 12\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=60, dense_layer=None, batch_size=64\n",
      "Value accuracy: 0.2232142835855484.  Top accuracy: 0.2857142984867096 at epoch: 11\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=60, dense_layer=None, batch_size=128\n",
      "Value accuracy: 0.3035714328289032.  Top accuracy: 0.3035714328289032 at epoch: 19\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=60, dense_layer=None, batch_size=256\n",
      "Value accuracy: 0.1339285671710968.  Top accuracy: 0.1339285671710968 at epoch: 19\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=60, dense_layer=8, batch_size=32\n",
      "Value accuracy: 0.2321428507566452.  Top accuracy: 0.2946428656578064 at epoch: 14\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=60, dense_layer=8, batch_size=64\n",
      "Value accuracy: 0.2589285671710968.  Top accuracy: 0.2946428656578064 at epoch: 18\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=60, dense_layer=8, batch_size=128\n",
      "Value accuracy: 0.2232142835855484.  Top accuracy: 0.3035714328289032 at epoch: 10\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=60, dense_layer=8, batch_size=256\n",
      "Value accuracy: 0.125.  Top accuracy: 0.3035714328289032 at epoch: 9\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=60, dense_layer=16, batch_size=32\n",
      "Value accuracy: 0.3125.  Top accuracy: 0.3303571343421936 at epoch: 4\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=60, dense_layer=16, batch_size=64\n",
      "Value accuracy: 0.2410714328289032.  Top accuracy: 0.3125 at epoch: 9\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=60, dense_layer=16, batch_size=128\n",
      "Value accuracy: 0.2142857164144516.  Top accuracy: 0.3303571343421936 at epoch: 10\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=60, dense_layer=16, batch_size=256\n",
      "Value accuracy: 0.1785714328289032.  Top accuracy: 0.2857142984867096 at epoch: 18\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=60, dense_layer=32, batch_size=32\n",
      "Value accuracy: 0.2142857164144516.  Top accuracy: 0.3571428656578064 at epoch: 4\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=60, dense_layer=32, batch_size=64\n",
      "Value accuracy: 0.25.  Top accuracy: 0.3571428656578064 at epoch: 10\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=60, dense_layer=32, batch_size=128\n",
      "Value accuracy: 0.2946428656578064.  Top accuracy: 0.3214285671710968 at epoch: 5\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=60, dense_layer=32, batch_size=256\n",
      "Value accuracy: 0.3303571343421936.  Top accuracy: 0.3303571343421936 at epoch: 19\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=60, dense_layer=64, batch_size=32\n",
      "Value accuracy: 0.3660714328289032.  Top accuracy: 0.375 at epoch: 2\n",
      "Training with embedding_dim=64, dropout_rate=0.2, lstm_units=60, dense_layer=64, batch_size=64\n"
     ]
    }
   ],
   "source": [
    "embeddings = [64,128,256]\n",
    "dropouts = [0.2,0.3,0.4,0.5]\n",
    "lstm_units = [15,30,60,120,240]\n",
    "dense = [None,8,16,32,64,128,256]\n",
    "batchs = [32,64,128,256]\n",
    "\n",
    "best_accuracy = 0\n",
    "best_params = {}\n",
    "\n",
    "for e in embeddings:\n",
    "    for d in dropouts:\n",
    "        for l in lstm_units:\n",
    "            for de in dense:\n",
    "                for b in batchs:\n",
    "                    print(f\"Training with embedding_dim={e}, dropout_rate={d}, lstm_units={l}, dense_layer={de}, batch_size={b}\")\n",
    "                    model, history = build_and_train_model(processed_data, tokenizer_features, maximum_length=100, embedding_dim=e, dropout_rate=d, lstm_units=l, dense=de, batch_size=b, epochs=20)\n",
    "                    \n",
    "                    val_accuracy = history.history['val_accuracy'][-1]\n",
    "                    top_val_accuracy = max(history.history['val_accuracy'])\n",
    "                    ind = np.argmax(history.history['val_accuracy'])\n",
    "                    print(f\"Value accuracy: {val_accuracy}.  Top accuracy: {top_val_accuracy} at epoch: {ind}\")\n",
    "                    if top_val_accuracy > best_accuracy:\n",
    "                        best_accuracy = top_val_accuracy\n",
    "                        best_params = {'embedding_dim': e, 'dropout_rate': d, 'lstm_unit': l, 'dense': de, 'batch_size': b, 'epoch': ind}\n",
    "\n",
    "print(f\"Best validation accuracy: {best_accuracy}\")\n",
    "print(f\"Best parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'embedding_dim': 64, 'dropout_rate': 0.2, 'lstm_unit': 15, 'dense': None, 'batch_size': 32, 'epoch': 14}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_183\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_183\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ text_input          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_183       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,953,408</span> │ text_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_183         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_183[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_183   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,600</span> │ dropout_183[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ attention_183       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span> │ bidirectional_18… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Attention</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sarcasm_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ irony_input         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ negation_input      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sentiment_polarity… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_183     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ attention_183[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ sarcasm_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │                   │            │ irony_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ negation_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ sentiment_polari… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_364 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">296</span> │ concatenate_183[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ text_input          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_183       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │  \u001b[38;5;34m1,953,408\u001b[0m │ text_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_183         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ embedding_183[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_183   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m30\u001b[0m)   │      \u001b[38;5;34m9,600\u001b[0m │ dropout_183[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ attention_183       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m)        │        \u001b[38;5;34m130\u001b[0m │ bidirectional_18… │\n",
       "│ (\u001b[38;5;33mAttention\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sarcasm_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ irony_input         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ negation_input      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sentiment_polarity… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_183     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ attention_183[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ sarcasm_input[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │                   │            │ irony_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ negation_input[\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ sentiment_polari… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_364 (\u001b[38;5;33mDense\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)         │        \u001b[38;5;34m296\u001b[0m │ concatenate_183[\u001b[38;5;34m…\u001b[0m │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,963,434</span> (7.49 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,963,434\u001b[0m (7.49 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,963,434</span> (7.49 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,963,434\u001b[0m (7.49 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.1209 - loss: 2.1300 - val_accuracy: 0.1518 - val_loss: 2.1087\n",
      "Epoch 2/14\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.0878 - loss: 2.1391 - val_accuracy: 0.1518 - val_loss: 2.0872\n",
      "Epoch 3/14\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.1394 - loss: 2.0627 - val_accuracy: 0.1875 - val_loss: 2.0552\n",
      "Epoch 4/14\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.1407 - loss: 2.0499 - val_accuracy: 0.1429 - val_loss: 2.0485\n",
      "Epoch 5/14\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.1653 - loss: 2.0447 - val_accuracy: 0.1696 - val_loss: 2.0265\n",
      "Epoch 6/14\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.2409 - loss: 1.9265 - val_accuracy: 0.1786 - val_loss: 2.0161\n",
      "Epoch 7/14\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.2727 - loss: 1.7492 - val_accuracy: 0.1875 - val_loss: 2.0519\n",
      "Epoch 8/14\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.5423 - loss: 1.5497 - val_accuracy: 0.2143 - val_loss: 2.0955\n",
      "Epoch 9/14\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - accuracy: 0.6180 - loss: 1.3424 - val_accuracy: 0.2411 - val_loss: 2.1141\n",
      "Epoch 10/14\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.7091 - loss: 1.1787 - val_accuracy: 0.3125 - val_loss: 2.1614\n",
      "Epoch 11/14\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.7414 - loss: 1.0227 - val_accuracy: 0.2946 - val_loss: 2.1253\n",
      "Epoch 12/14\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.7072 - loss: 1.0527 - val_accuracy: 0.2589 - val_loss: 2.1657\n",
      "Epoch 13/14\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.7644 - loss: 0.9151 - val_accuracy: 0.1964 - val_loss: 2.3195\n",
      "Epoch 14/14\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.8364 - loss: 0.7516 - val_accuracy: 0.2232 - val_loss: 2.3891\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best parameters: {best_params}\")\n",
    "# best_params = {'embedding_dim': 128, 'dropout_rate': 0.4, 'lstm_unit': 60, 'batch_size': 128, 'epoch': 20}\n",
    "model, history = build_and_train_model(processed_data, tokenizer_features, maximum_length=100, embedding_dim=best_params['embedding_dim'], dropout_rate=best_params['dropout_rate'], lstm_units=best_params['lstm_unit'], dense=best_params['dense'], batch_size=best_params['batch_size'], epochs=best_params['epoch'], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with embedding_dim=64, dropout_rate=0.1, batch_size=8\n",
    "Value accuracy: 0.2410714328289032\n",
    "Training with embedding_dim=64, dropout_rate=0.1, batch_size=16\n",
    "Value accuracy: 0.3571428656578064\n",
    "Training with embedding_dim=64, dropout_rate=0.1, batch_size=32\n",
    "Value accuracy: 0.3571428656578064\n",
    "Training with embedding_dim=64, dropout_rate=0.1, batch_size=64\n",
    "Value accuracy: 0.3928571343421936\n",
    "Training with embedding_dim=64, dropout_rate=0.1, batch_size=128\n",
    "Value accuracy: 0.2678571343421936\n",
    "Training with embedding_dim=64, dropout_rate=0.2, batch_size=8\n",
    "Value accuracy: 0.2142857164144516\n",
    "Training with embedding_dim=64, dropout_rate=0.2, batch_size=16\n",
    "Value accuracy: 0.2232142835855484\n",
    "Training with embedding_dim=64, dropout_rate=0.2, batch_size=32\n",
    "Value accuracy: 0.3839285671710968\n",
    "Training with embedding_dim=64, dropout_rate=0.2, batch_size=64\n",
    "Value accuracy: 0.2857142984867096\n",
    "Training with embedding_dim=64, dropout_rate=0.2, batch_size=128\n",
    "Value accuracy: 0.2857142984867096\n",
    "Training with embedding_dim=64, dropout_rate=0.3, batch_size=8\n",
    "Value accuracy: 0.2946428656578064\n",
    "Training with embedding_dim=64, dropout_rate=0.3, batch_size=16\n",
    "Value accuracy: 0.2946428656578064\n",
    "Training with embedding_dim=64, dropout_rate=0.3, batch_size=32\n",
    "Value accuracy: 0.375\n",
    "Training with embedding_dim=64, dropout_rate=0.3, batch_size=64\n",
    "Value accuracy: 0.3125\n",
    "Training with embedding_dim=64, dropout_rate=0.3, batch_size=128\n",
    "Value accuracy: 0.3660714328289032\n",
    "Training with embedding_dim=64, dropout_rate=0.4, batch_size=8\n",
    "Value accuracy: 0.3035714328289032\n",
    "Training with embedding_dim=64, dropout_rate=0.4, batch_size=16\n",
    "Value accuracy: 0.3125\n",
    "Training with embedding_dim=64, dropout_rate=0.4, batch_size=32\n",
    "Value accuracy: 0.2857142984867096\n",
    "Training with embedding_dim=64, dropout_rate=0.4, batch_size=64\n",
    "Value accuracy: 0.3214285671710968\n",
    "Training with embedding_dim=64, dropout_rate=0.4, batch_size=128\n",
    "Value accuracy: 0.3482142984867096\n",
    "Training with embedding_dim=64, dropout_rate=0.5, batch_size=8\n",
    "Value accuracy: 0.3303571343421936\n",
    "Training with embedding_dim=64, dropout_rate=0.5, batch_size=16\n",
    "Value accuracy: 0.3571428656578064\n",
    "Training with embedding_dim=64, dropout_rate=0.5, batch_size=32\n",
    "Value accuracy: 0.2589285671710968\n",
    "Training with embedding_dim=64, dropout_rate=0.5, batch_size=64\n",
    "Value accuracy: 0.3839285671710968\n",
    "Training with embedding_dim=64, dropout_rate=0.5, batch_size=128\n",
    "Value accuracy: 0.3660714328289032\n",
    "Training with embedding_dim=128, dropout_rate=0.1, batch_size=8\n",
    "Value accuracy: 0.2678571343421936\n",
    "Training with embedding_dim=128, dropout_rate=0.1, batch_size=16\n",
    "Value accuracy: 0.2767857015132904\n",
    "Training with embedding_dim=128, dropout_rate=0.1, batch_size=32\n",
    "Value accuracy: 0.2946428656578064\n",
    "Training with embedding_dim=128, dropout_rate=0.1, batch_size=64\n",
    "Value accuracy: 0.3839285671710968\n",
    "Training with embedding_dim=128, dropout_rate=0.1, batch_size=128\n",
    "Value accuracy: 0.3571428656578064\n",
    "Training with embedding_dim=128, dropout_rate=0.2, batch_size=8\n",
    "Value accuracy: 0.2589285671710968\n",
    "Training with embedding_dim=128, dropout_rate=0.2, batch_size=16\n",
    "Value accuracy: 0.2767857015132904\n",
    "Training with embedding_dim=128, dropout_rate=0.2, batch_size=32\n",
    "Value accuracy: 0.2678571343421936\n",
    "Training with embedding_dim=128, dropout_rate=0.2, batch_size=64\n",
    "Value accuracy: 0.2857142984867096\n",
    "Training with embedding_dim=128, dropout_rate=0.2, batch_size=128\n",
    "Value accuracy: 0.3392857015132904\n",
    "Training with embedding_dim=128, dropout_rate=0.3, batch_size=8\n",
    "Value accuracy: 0.3125\n",
    "Training with embedding_dim=128, dropout_rate=0.3, batch_size=16\n",
    "Value accuracy: 0.3571428656578064\n",
    "Training with embedding_dim=128, dropout_rate=0.3, batch_size=32\n",
    "Value accuracy: 0.3571428656578064\n",
    "Training with embedding_dim=128, dropout_rate=0.3, batch_size=64\n",
    "Value accuracy: 0.3303571343421936\n",
    "Training with embedding_dim=128, dropout_rate=0.3, batch_size=128\n",
    "Value accuracy: 0.3482142984867096\n",
    "Training with embedding_dim=128, dropout_rate=0.4, batch_size=8\n",
    "Value accuracy: 0.2589285671710968\n",
    "Training with embedding_dim=128, dropout_rate=0.4, batch_size=16\n",
    "Value accuracy: 0.2142857164144516\n",
    "Training with embedding_dim=128, dropout_rate=0.4, batch_size=32\n",
    "Value accuracy: 0.2767857015132904\n",
    "Training with embedding_dim=128, dropout_rate=0.4, batch_size=64\n",
    "Value accuracy: 0.3571428656578064\n",
    "Training with embedding_dim=128, dropout_rate=0.4, batch_size=128\n",
    "Value accuracy: 0.4107142984867096\n",
    "Training with embedding_dim=128, dropout_rate=0.5, batch_size=8\n",
    "Value accuracy: 0.3214285671710968\n",
    "Training with embedding_dim=128, dropout_rate=0.5, batch_size=16\n",
    "Value accuracy: 0.3125\n",
    "Training with embedding_dim=128, dropout_rate=0.5, batch_size=32\n",
    "Value accuracy: 0.3482142984867096\n",
    "Training with embedding_dim=128, dropout_rate=0.5, batch_size=64\n",
    "Value accuracy: 0.3571428656578064\n",
    "Training with embedding_dim=128, dropout_rate=0.5, batch_size=128\n",
    "Value accuracy: 0.2857142984867096\n",
    "Training with embedding_dim=256, dropout_rate=0.1, batch_size=8\n",
    "Value accuracy: 0.2589285671710968\n",
    "Training with embedding_dim=256, dropout_rate=0.1, batch_size=16\n",
    "Value accuracy: 0.2767857015132904\n",
    "Training with embedding_dim=256, dropout_rate=0.1, batch_size=32\n",
    "Value accuracy: 0.2321428507566452\n",
    "Training with embedding_dim=256, dropout_rate=0.1, batch_size=64\n",
    "Value accuracy: 0.2678571343421936\n",
    "Training with embedding_dim=256, dropout_rate=0.1, batch_size=128\n",
    "Value accuracy: 0.3482142984867096\n",
    "Training with embedding_dim=256, dropout_rate=0.2, batch_size=8\n",
    "Value accuracy: 0.2767857015132904\n",
    "Training with embedding_dim=256, dropout_rate=0.2, batch_size=16\n",
    "Value accuracy: 0.3482142984867096\n",
    "Training with embedding_dim=256, dropout_rate=0.2, batch_size=32\n",
    "Value accuracy: 0.2410714328289032\n",
    "Training with embedding_dim=256, dropout_rate=0.2, batch_size=64\n",
    "Value accuracy: 0.3214285671710968\n",
    "Training with embedding_dim=256, dropout_rate=0.2, batch_size=128\n",
    "Value accuracy: 0.2678571343421936\n",
    "Training with embedding_dim=256, dropout_rate=0.3, batch_size=8\n",
    "Value accuracy: 0.2857142984867096\n",
    "Training with embedding_dim=256, dropout_rate=0.3, batch_size=16\n",
    "Value accuracy: 0.25\n",
    "Training with embedding_dim=256, dropout_rate=0.3, batch_size=32\n",
    "Value accuracy: 0.2946428656578064\n",
    "Training with embedding_dim=256, dropout_rate=0.3, batch_size=64\n",
    "Value accuracy: 0.3214285671710968\n",
    "Training with embedding_dim=256, dropout_rate=0.3, batch_size=128\n",
    "Value accuracy: 0.3482142984867096\n",
    "Training with embedding_dim=256, dropout_rate=0.4, batch_size=8\n",
    "Value accuracy: 0.2232142835855484\n",
    "Training with embedding_dim=256, dropout_rate=0.4, batch_size=16\n",
    "Value accuracy: 0.2321428507566452\n",
    "Training with embedding_dim=256, dropout_rate=0.4, batch_size=32\n",
    "Value accuracy: 0.3571428656578064\n",
    "Training with embedding_dim=256, dropout_rate=0.4, batch_size=64\n",
    "Value accuracy: 0.3125\n",
    "Training with embedding_dim=256, dropout_rate=0.4, batch_size=128\n",
    "Value accuracy: 0.3303571343421936\n",
    "Training with embedding_dim=256, dropout_rate=0.5, batch_size=8\n",
    "Value accuracy: 0.2589285671710968\n",
    "Training with embedding_dim=256, dropout_rate=0.5, batch_size=16\n",
    "Value accuracy: 0.2589285671710968\n",
    "Training with embedding_dim=256, dropout_rate=0.5, batch_size=32\n",
    "Value accuracy: 0.3482142984867096\n",
    "Training with embedding_dim=256, dropout_rate=0.5, batch_size=64\n",
    "Value accuracy: 0.2857142984867096\n",
    "Training with embedding_dim=256, dropout_rate=0.5, batch_size=128\n",
    "Value accuracy: 0.3214285671710968\n",
    "Training with embedding_dim=512, dropout_rate=0.1, batch_size=8\n",
    "Value accuracy: 0.2857142984867096\n",
    "Training with embedding_dim=512, dropout_rate=0.1, batch_size=16\n",
    "Value accuracy: 0.3035714328289032\n",
    "Training with embedding_dim=512, dropout_rate=0.1, batch_size=32\n",
    "Value accuracy: 0.3035714328289032\n",
    "Training with embedding_dim=512, dropout_rate=0.1, batch_size=64\n",
    "Value accuracy: 0.2321428507566452\n",
    "Training with embedding_dim=512, dropout_rate=0.1, batch_size=128\n",
    "Value accuracy: 0.2589285671710968\n",
    "Training with embedding_dim=512, dropout_rate=0.2, batch_size=8\n",
    "Value accuracy: 0.2410714328289032\n",
    "Training with embedding_dim=512, dropout_rate=0.2, batch_size=16\n",
    "Value accuracy: 0.2767857015132904\n",
    "Training with embedding_dim=512, dropout_rate=0.2, batch_size=32\n",
    "Value accuracy: 0.2767857015132904\n",
    "Training with embedding_dim=512, dropout_rate=0.2, batch_size=64\n",
    "Value accuracy: 0.2946428656578064\n",
    "Training with embedding_dim=512, dropout_rate=0.2, batch_size=128\n",
    "Value accuracy: 0.2767857015132904\n",
    "Training with embedding_dim=512, dropout_rate=0.3, batch_size=8\n",
    "Value accuracy: 0.2053571492433548\n",
    "Training with embedding_dim=512, dropout_rate=0.3, batch_size=16\n",
    "Value accuracy: 0.1875\n",
    "Training with embedding_dim=512, dropout_rate=0.3, batch_size=32\n",
    "Value accuracy: 0.2857142984867096\n",
    "Training with embedding_dim=512, dropout_rate=0.3, batch_size=64\n",
    "Value accuracy: 0.2678571343421936\n",
    "Training with embedding_dim=512, dropout_rate=0.3, batch_size=128\n",
    "Value accuracy: 0.3571428656578064\n",
    "Training with embedding_dim=512, dropout_rate=0.4, batch_size=8\n",
    "Value accuracy: 0.2767857015132904\n",
    "Training with embedding_dim=512, dropout_rate=0.4, batch_size=16\n",
    "Value accuracy: 0.25\n",
    "Training with embedding_dim=512, dropout_rate=0.4, batch_size=32\n",
    "Value accuracy: 0.2678571343421936\n",
    "Training with embedding_dim=512, dropout_rate=0.4, batch_size=64\n",
    "Value accuracy: 0.3035714328289032\n",
    "Training with embedding_dim=512, dropout_rate=0.4, batch_size=128\n",
    "Value accuracy: 0.3214285671710968\n",
    "Training with embedding_dim=512, dropout_rate=0.5, batch_size=8\n",
    "Value accuracy: 0.2767857015132904\n",
    "Training with embedding_dim=512, dropout_rate=0.5, batch_size=16\n",
    "Value accuracy: 0.2946428656578064\n",
    "Training with embedding_dim=512, dropout_rate=0.5, batch_size=32\n",
    "Value accuracy: 0.25\n",
    "Training with embedding_dim=512, dropout_rate=0.5, batch_size=64\n",
    "Value accuracy: 0.25\n",
    "Training with embedding_dim=512, dropout_rate=0.5, batch_size=128\n",
    "Value accuracy: 0.3035714328289032\n",
    "Training with embedding_dim=1024, dropout_rate=0.1, batch_size=8\n",
    "Value accuracy: 0.2410714328289032\n",
    "Training with embedding_dim=1024, dropout_rate=0.1, batch_size=16\n",
    "Value accuracy: 0.2946428656578064\n",
    "Training with embedding_dim=1024, dropout_rate=0.1, batch_size=32\n",
    "Value accuracy: 0.2767857015132904\n",
    "Training with embedding_dim=1024, dropout_rate=0.1, batch_size=64\n",
    "Value accuracy: 0.2946428656578064\n",
    "Training with embedding_dim=1024, dropout_rate=0.1, batch_size=128\n",
    "Value accuracy: 0.3214285671710968\n",
    "Training with embedding_dim=1024, dropout_rate=0.2, batch_size=8\n",
    "Value accuracy: 0.2410714328289032\n",
    "Training with embedding_dim=1024, dropout_rate=0.2, batch_size=16\n",
    "Value accuracy: 0.2321428507566452\n",
    "Training with embedding_dim=1024, dropout_rate=0.2, batch_size=32\n",
    "Value accuracy: 0.2321428507566452\n",
    "Training with embedding_dim=1024, dropout_rate=0.2, batch_size=64\n",
    "Value accuracy: 0.2410714328289032\n",
    "Training with embedding_dim=1024, dropout_rate=0.2, batch_size=128\n",
    "Value accuracy: 0.2321428507566452\n",
    "Training with embedding_dim=1024, dropout_rate=0.3, batch_size=8\n",
    "Value accuracy: 0.2589285671710968\n",
    "Training with embedding_dim=1024, dropout_rate=0.3, batch_size=16\n",
    "Value accuracy: 0.2678571343421936\n",
    "Training with embedding_dim=1024, dropout_rate=0.3, batch_size=32\n",
    "Value accuracy: 0.2410714328289032\n",
    "Training with embedding_dim=1024, dropout_rate=0.3, batch_size=64\n",
    "Value accuracy: 0.2410714328289032\n",
    "Training with embedding_dim=1024, dropout_rate=0.3, batch_size=128\n",
    "Value accuracy: 0.3303571343421936\n",
    "Training with embedding_dim=1024, dropout_rate=0.4, batch_size=8\n",
    "Value accuracy: 0.25\n",
    "Training with embedding_dim=1024, dropout_rate=0.4, batch_size=16\n",
    "Value accuracy: 0.2410714328289032\n",
    "Training with embedding_dim=1024, dropout_rate=0.4, batch_size=32\n",
    "Value accuracy: 0.2946428656578064\n",
    "Training with embedding_dim=1024, dropout_rate=0.4, batch_size=64\n",
    "Value accuracy: 0.2232142835855484\n",
    "Training with embedding_dim=1024, dropout_rate=0.4, batch_size=128\n",
    "Value accuracy: 0.3571428656578064\n",
    "Training with embedding_dim=1024, dropout_rate=0.5, batch_size=8\n",
    "Value accuracy: 0.2767857015132904\n",
    "Training with embedding_dim=1024, dropout_rate=0.5, batch_size=16\n",
    "Value accuracy: 0.2678571343421936\n",
    "Training with embedding_dim=1024, dropout_rate=0.5, batch_size=32\n",
    "Value accuracy: 0.25\n",
    "Training with embedding_dim=1024, dropout_rate=0.5, batch_size=64\n",
    "Value accuracy: 0.2589285671710968\n",
    "Training with embedding_dim=1024, dropout_rate=0.5, batch_size=128\n",
    "Value accuracy: 0.2857142984867096\n",
    "Best validation accuracy: 0.4107142984867096\n",
    "Best parameters: {'embedding_dim': 128, 'dropout_rate': 0.4, 'batch_size': 128}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_loss = history.history[ 'loss']\n",
    "val_loss = history.history['val_loss']\n",
    "tr_acc = history.history[ 'accuracy']\n",
    "val_acc = history.history[ 'val_accuracy']\n",
    "epochs = range(len(tr_loss)) # get the epochs as a list to plot along the x-axis\n",
    "\n",
    "plt.figure(figsize=(14,10))\n",
    "# Loss curves\n",
    "ax = plt.subplot(2,2,1)\n",
    "plt.plot(epochs, tr_loss, label='Training Loss') \n",
    "plt.plot(epochs, val_loss, label='Validation Loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Loss Curves')\n",
    "plt.legend()\n",
    "# Accuracy curves\n",
    "ax = plt.subplot(2,2,2)\n",
    "plt.plot(epochs, tr_acc, label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, label='Validation Accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Accuracy Curves')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = processed_data[\"Emotion\"].unique().tolist()\n",
    "\n",
    "y_pred = model.predict([X_text_val, X_sarcasm_val, X_irony_val, X_negation_val, X_sentiment_polarity_val])\n",
    "\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "Y_test = np.argmax(y_val, axis=1)\n",
    "\n",
    "print(classification_report(Y_test, y_pred, target_names=labels))\n",
    "\n",
    "cm = confusion_matrix(Y_test, y_pred)\n",
    "# Plot the confusion matrix\n",
    "sns.heatmap(cm, annot=True,fmt='d', cmap='YlGnBu', xticklabels=labels, yticklabels=labels)\n",
    "plt.ylabel('Prediction',fontsize=12)\n",
    "plt.xlabel('Actual',fontsize=12)\n",
    "plt.title('Confusion Matrix',fontsize=16)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
