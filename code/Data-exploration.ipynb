{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "## MAKE A CNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2142 entries, 0 to 2141\n",
      "Data columns (total 13 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   Online Post ID          2142 non-null   int64  \n",
      " 1   Twitter ID              1673 non-null   float64\n",
      " 2   Related Online Post ID  333 non-null    float64\n",
      " 3   Source ID               2142 non-null   object \n",
      " 4   Online Post Text        469 non-null    object \n",
      " 5   Subjectivity            2142 non-null   int64  \n",
      " 6   Sentiment Polarity      2142 non-null   object \n",
      " 7   Emotion                 2142 non-null   object \n",
      " 8   Sarcasm                 2142 non-null   int64  \n",
      " 9   Irony                   2142 non-null   int64  \n",
      " 10  Negation                2142 non-null   int64  \n",
      " 11  Off-topic               2142 non-null   int64  \n",
      " 12  Language                2142 non-null   int64  \n",
      "dtypes: float64(2), int64(7), object(4)\n",
      "memory usage: 217.7+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2316 entries, 0 to 2315\n",
      "Data columns (total 13 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   Online Post ID          2316 non-null   int64  \n",
      " 1   Twitter ID              1677 non-null   float64\n",
      " 2   Related Online Post ID  448 non-null    float64\n",
      " 3   Source ID               2316 non-null   object \n",
      " 4   Online Post Text        639 non-null    object \n",
      " 5   Subjectivity            2316 non-null   int64  \n",
      " 6   Sentiment Polarity      2316 non-null   object \n",
      " 7   Emotion                 2316 non-null   object \n",
      " 8   Sarcasm                 2316 non-null   int64  \n",
      " 9   Irony                   2316 non-null   int64  \n",
      " 10  Negation                2316 non-null   int64  \n",
      " 11  Off-topic               2316 non-null   int64  \n",
      " 12  Language                2316 non-null   int64  \n",
      "dtypes: float64(2), int64(7), object(4)\n",
      "memory usage: 235.3+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1929 entries, 0 to 1928\n",
      "Data columns (total 13 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   Online Post ID          1929 non-null   int64  \n",
      " 1   Twitter ID              1314 non-null   float64\n",
      " 2   Related Online Post ID  396 non-null    float64\n",
      " 3   Source ID               1929 non-null   object \n",
      " 4   Online Post Text        615 non-null    object \n",
      " 5   Subjectivity            1929 non-null   int64  \n",
      " 6   Sentiment Polarity      1929 non-null   object \n",
      " 7   Emotion                 1929 non-null   object \n",
      " 8   Sarcasm                 1929 non-null   int64  \n",
      " 9   Irony                   1929 non-null   int64  \n",
      " 10  Negation                1929 non-null   int64  \n",
      " 11  Off-topic               1929 non-null   int64  \n",
      " 12  Language                1929 non-null   int64  \n",
      "dtypes: float64(2), int64(7), object(4)\n",
      "memory usage: 196.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "malta_loc_18 = '../data/Malta-Budget-2018-dataset-v1.csv'\n",
    "malta_loc_19 = '../data/Malta-Budget-2019-dataset-v1.csv'\n",
    "malta_loc_20 = '../data/Malta-Budget-2020-dataset-v1.csv'\n",
    "\n",
    "malta_data_18 = pd.read_csv(malta_loc_18)\n",
    "malta_data_19 = pd.read_csv(malta_loc_19)\n",
    "malta_data_20 = pd.read_csv(malta_loc_20)\n",
    "\n",
    "print(malta_data_18.info())\n",
    "print(malta_data_19.info())\n",
    "print(malta_data_20.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge data files into single DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6387 entries, 0 to 6386\n",
      "Data columns (total 13 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   Online Post ID          6387 non-null   int64  \n",
      " 1   Twitter ID              4664 non-null   float64\n",
      " 2   Related Online Post ID  1177 non-null   float64\n",
      " 3   Source ID               6387 non-null   object \n",
      " 4   Online Post Text        1723 non-null   object \n",
      " 5   Subjectivity            6387 non-null   int64  \n",
      " 6   Sentiment Polarity      6387 non-null   object \n",
      " 7   Emotion                 6387 non-null   object \n",
      " 8   Sarcasm                 6387 non-null   int64  \n",
      " 9   Irony                   6387 non-null   int64  \n",
      " 10  Negation                6387 non-null   int64  \n",
      " 11  Off-topic               6387 non-null   int64  \n",
      " 12  Language                6387 non-null   int64  \n",
      "dtypes: float64(2), int64(7), object(4)\n",
      "memory usage: 648.8+ KB\n"
     ]
    }
   ],
   "source": [
    "malta_data_19 = malta_data_19.rename(columns={'Off-topic ':'Off-topic'})\n",
    "combined_data = pd.concat([malta_data_18, malta_data_19, malta_data_20], ignore_index=True)\n",
    "combined_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up of data columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>Sentiment Polarity</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Sarcasm</th>\n",
       "      <th>Irony</th>\n",
       "      <th>Negation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20180001</td>\n",
       "      <td>Great BUDGET . Even cigarettes were not touche...</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>trust</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20180002</td>\n",
       "      <td>I haven't exactly scanned the budget throughou...</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>disgust</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20180003</td>\n",
       "      <td>There’s already smoking cessation programs for...</td>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20180004</td>\n",
       "      <td>So should alcohol and fuel for private vehicle...</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20180005</td>\n",
       "      <td>Practical? You should say that in a third worl...</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>anger</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID                                               Text  Subjectivity  \\\n",
       "0  20180001  Great BUDGET . Even cigarettes were not touche...             1   \n",
       "1  20180002  I haven't exactly scanned the budget throughou...             1   \n",
       "2  20180003  There’s already smoking cessation programs for...             1   \n",
       "3  20180004  So should alcohol and fuel for private vehicle...             1   \n",
       "4  20180005  Practical? You should say that in a third worl...             1   \n",
       "\n",
       "  Sentiment Polarity       Emotion  Sarcasm  Irony  Negation  \n",
       "0           positive         trust        0      0         1  \n",
       "1           negative       disgust        0      0         1  \n",
       "2            neutral  anticipation        0      0         0  \n",
       "3           negative       sadness        0      0         0  \n",
       "4           negative         anger        0      0         1  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data = combined_data.dropna(subset=['Online Post Text'])\n",
    "clean_data = clean_data.drop(['Twitter ID', 'Related Online Post ID', 'Source ID','Off-topic'], axis=1)\n",
    "clean_data = clean_data[clean_data['Language'] == 0] # get all data that is in english \n",
    "clean_data = clean_data.drop(['Language'], axis=1)\n",
    "clean_data = clean_data.rename(columns={'Online Post ID':'ID','Online Post Text':'Text'})\n",
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1114 entries, 0 to 5072\n",
      "Data columns (total 8 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   ID                  1114 non-null   int64 \n",
      " 1   Text                1114 non-null   object\n",
      " 2   Subjectivity        1114 non-null   int64 \n",
      " 3   Sentiment Polarity  1114 non-null   object\n",
      " 4   Emotion             1114 non-null   object\n",
      " 5   Sarcasm             1114 non-null   int64 \n",
      " 6   Irony               1114 non-null   int64 \n",
      " 7   Negation            1114 non-null   int64 \n",
      "dtypes: int64(5), object(3)\n",
      "memory usage: 78.3+ KB\n"
     ]
    }
   ],
   "source": [
    "clean_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Emotion\n",
       "disgust         253\n",
       "anger           225\n",
       "anticipation    199\n",
       "sadness         138\n",
       "joy              95\n",
       "surprise         92\n",
       "trust            67\n",
       "fear             45\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data[\"Emotion\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "value_counts = clean_data['Emotion'].value_counts()\n",
    "\n",
    "# Plotting the frequency of string values in the 'Category' column\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=value_counts.index, y=value_counts.values, palette='viridis', hue=value_counts.index, legend=False)\n",
    "plt.xlabel('Emotion')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Frequency of Emotional Categories In English Comments about The Maltese Budget 2018-2020')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Processing of Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/alex/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Subjectivity</th>\n",
       "      <th>Sentiment Polarity</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Sarcasm</th>\n",
       "      <th>Irony</th>\n",
       "      <th>Negation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20180001</td>\n",
       "      <td>great budget even cigarette touched great work...</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>trust</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20180002</td>\n",
       "      <td>exactly scanned budget throughout earth make i...</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>disgust</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20180003</td>\n",
       "      <td>already smoking cessation program people want ...</td>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20180004</td>\n",
       "      <td>alcohol fuel private vehicle raising tax cigar...</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20180005</td>\n",
       "      <td>practical say third world country supposed eur...</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>anger</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID                                               Text  Subjectivity  \\\n",
       "0  20180001  great budget even cigarette touched great work...             1   \n",
       "1  20180002  exactly scanned budget throughout earth make i...             1   \n",
       "2  20180003  already smoking cessation program people want ...             1   \n",
       "3  20180004  alcohol fuel private vehicle raising tax cigar...             1   \n",
       "4  20180005  practical say third world country supposed eur...             1   \n",
       "\n",
       "  Sentiment Polarity       Emotion  Sarcasm  Irony  Negation  \n",
       "0           positive         trust        0      0         1  \n",
       "1           negative       disgust        0      0         1  \n",
       "2            neutral  anticipation        0      0         0  \n",
       "3           negative       sadness        0      0         0  \n",
       "4           negative         anger        0      0         1  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import contractions\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    pattern = re.compile(r'[^a-zA-Z\\s]')\n",
    "    return pattern.sub('', text)\n",
    "\n",
    "# Remove URLs and HTML tags\n",
    "clean_data['Text'] = clean_data['Text'].str.replace(r'http\\S+|www\\S+|https\\S+', '', regex=True)\n",
    "clean_data['Text'] = clean_data['Text'].str.replace(r'<.*?>', '', regex=True)\n",
    "\n",
    "# Expand contractions\n",
    "clean_data['Text'] = clean_data['Text'].apply(lambda x: contractions.fix(x))\n",
    "\n",
    "# Convert to lowercase\n",
    "clean_data['Text'] = clean_data['Text'].str.lower()\n",
    "\n",
    "# Remove punctuation\n",
    "clean_data['Text'] = clean_data['Text'].str.replace(f\"[{string.punctuation}]\", \" \", regex=True)\n",
    "\n",
    "# Remove numbers\n",
    "clean_data['Text'] = clean_data['Text'].str.replace(r'\\d+', '', regex=True)\n",
    "\n",
    "# Remove special characters\n",
    "clean_data['Text'] = clean_data['Text'].apply(remove_special_characters)\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "clean_data['Text'] = clean_data['Text'].apply(lambda x: ' '.join(word for word in x.split() if word not in stop_words))\n",
    "\n",
    "# Remove extra whitespace\n",
    "clean_data['Text'] = clean_data['Text'].str.strip()\n",
    "clean_data['Text'] = clean_data['Text'].str.replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "# Lemmatize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "clean_data['Text'] = clean_data['Text'].apply(lambda x: ' '.join(lemmatizer.lemmatize(word) for word in x.split()))\n",
    "\n",
    "# Tokenize\n",
    "# clean_data['tokens'] = clean_data['Text'].apply(word_tokenize)\n",
    " \n",
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20180001</td>\n",
       "      <td>great budget even cigarette touched great work...</td>\n",
       "      <td>trust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20180002</td>\n",
       "      <td>exactly scanned budget throughout earth make i...</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20180003</td>\n",
       "      <td>already smoking cessation program people want ...</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20180004</td>\n",
       "      <td>alcohol fuel private vehicle raising tax cigar...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20180005</td>\n",
       "      <td>practical say third world country supposed eur...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID                                               Text       Emotion\n",
       "0  20180001  great budget even cigarette touched great work...         trust\n",
       "1  20180002  exactly scanned budget throughout earth make i...       disgust\n",
       "2  20180003  already smoking cessation program people want ...  anticipation\n",
       "3  20180004  alcohol fuel private vehicle raising tax cigar...       sadness\n",
       "4  20180005  practical say third world country supposed eur...         anger"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stripped_data = clean_data.drop(columns=['Subjectivity','Sentiment Polarity', 'Sarcasm','Irony','Negation'])\n",
    "stripped_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.28699551569506726\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.19      0.31      0.24        45\n",
      "anticipation       0.43      0.45      0.44        40\n",
      "     disgust       0.32      0.51      0.39        51\n",
      "        fear       0.50      0.11      0.18         9\n",
      "         joy       0.00      0.00      0.00        19\n",
      "     sadness       0.29      0.18      0.22        28\n",
      "    surprise       0.00      0.00      0.00        18\n",
      "       trust       0.00      0.00      0.00        13\n",
      "\n",
      "    accuracy                           0.29       223\n",
      "   macro avg       0.22      0.20      0.18       223\n",
      "weighted avg       0.25      0.29      0.25       223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "train_df, test_df = train_test_split(stripped_data, test_size=0.2, stratify=stripped_data['Emotion'], random_state=42)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_df['Text'])\n",
    "X_test = vectorizer.transform(test_df['Text'])\n",
    "\n",
    "# Labels\n",
    "y_train = train_df['Emotion']\n",
    "y_test = test_df['Emotion']\n",
    "\n",
    "# Training Naive Bayes classifier\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data: \n",
      "(891, 100)\n",
      "(891, 8)\n",
      "Shape of test data: \n",
      "(223, 100)\n",
      "(223, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_17\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_17\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d_14         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_17 (\u001b[38;5;33mEmbedding\u001b[0m)        │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_24 (\u001b[38;5;33mConv1D\u001b[0m)              │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d_14         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_28 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_19 (\u001b[38;5;33mDropout\u001b[0m)            │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_29 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.1653 - loss: 2.0289 - val_accuracy: 0.2287 - val_loss: 1.9030\n",
      "Epoch 2/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.2514 - loss: 1.8765 - val_accuracy: 0.1614 - val_loss: 1.8990\n",
      "Epoch 3/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.3165 - loss: 1.7833 - val_accuracy: 0.2377 - val_loss: 1.8667\n",
      "Epoch 4/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5786 - loss: 1.4613 - val_accuracy: 0.1794 - val_loss: 1.8679\n",
      "Epoch 5/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.7309 - loss: 0.9369 - val_accuracy: 0.2422 - val_loss: 1.9366\n",
      "Epoch 6/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.8989 - loss: 0.5338 - val_accuracy: 0.2511 - val_loss: 2.1165\n",
      "Epoch 7/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9488 - loss: 0.2675 - val_accuracy: 0.2242 - val_loss: 2.3059\n",
      "Epoch 8/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9780 - loss: 0.1467 - val_accuracy: 0.2242 - val_loss: 2.4687\n",
      "Epoch 9/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.9875 - loss: 0.0912 - val_accuracy: 0.2422 - val_loss: 2.6327\n",
      "Epoch 10/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9886 - loss: 0.0609 - val_accuracy: 0.2377 - val_loss: 2.8403\n",
      "Epoch 11/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9915 - loss: 0.0423 - val_accuracy: 0.2377 - val_loss: 2.9157\n",
      "Epoch 12/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9946 - loss: 0.0307 - val_accuracy: 0.2287 - val_loss: 2.9863\n",
      "Epoch 13/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9936 - loss: 0.0273 - val_accuracy: 0.2377 - val_loss: 3.0519\n",
      "Epoch 14/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9934 - loss: 0.0340 - val_accuracy: 0.2197 - val_loss: 3.1585\n",
      "Epoch 15/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9938 - loss: 0.0279 - val_accuracy: 0.2466 - val_loss: 3.3216\n",
      "Epoch 16/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9947 - loss: 0.0192 - val_accuracy: 0.2242 - val_loss: 3.3941\n",
      "Epoch 17/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9982 - loss: 0.0170 - val_accuracy: 0.2466 - val_loss: 3.4286\n",
      "Epoch 18/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9956 - loss: 0.0164 - val_accuracy: 0.2197 - val_loss: 3.5217\n",
      "Epoch 19/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9972 - loss: 0.0107 - val_accuracy: 0.2242 - val_loss: 3.5590\n",
      "Epoch 20/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9959 - loss: 0.0130 - val_accuracy: 0.2466 - val_loss: 3.6168\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2163 - loss: 3.8724 \n",
      "Test Loss: 3.874948740005493\n",
      "Test Accuracy: 0.2466367781162262\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer, one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, MaxPooling2D, Conv2D\n",
    "\n",
    "# Tokenization and Padding\n",
    "max_features = 10000  # Max number of words in tokenizer\n",
    "max_len = 100  # Max length of each sequence (pad/truncate to this length)\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(stripped_data['Text'])\n",
    "sequences = tokenizer.texts_to_sequences(stripped_data['Text'])\n",
    "x_data = pad_sequences(sequences, maxlen=max_len)\n",
    "y_data = pd.get_dummies(stripped_data['Emotion']).values  # Convert categorical labels to one-hot encoded vectors\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, stratify=stripped_data['Emotion'], random_state=42)\n",
    "print('Shape of training data: ')\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print('Shape of test data: ')\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# CNN Model Architecture\n",
    "embedding_dim = 100\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_features, output_dim=embedding_dim))  # Specify input_dim instead of input_length\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y_data.shape[1], activation='softmax'))  # Output layer with softmax activation for multi-class classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Training the model\n",
    "batch_size = 20\n",
    "epochs = 20\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with One Hot Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data: \n",
      "(891, 100)\n",
      "(891, 8)\n",
      "Shape of test data: \n",
      "(223, 100)\n",
      "(223, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_18\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_18\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d_15         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_18 (\u001b[38;5;33mEmbedding\u001b[0m)        │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_25 (\u001b[38;5;33mConv1D\u001b[0m)              │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d_15         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_30 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_20 (\u001b[38;5;33mDropout\u001b[0m)            │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_31 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.1772 - loss: 2.0203 - val_accuracy: 0.2287 - val_loss: 1.9102\n",
      "Epoch 2/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.3021 - loss: 1.8859 - val_accuracy: 0.1794 - val_loss: 1.9107\n",
      "Epoch 3/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.4457 - loss: 1.7594 - val_accuracy: 0.1928 - val_loss: 1.8895\n",
      "Epoch 4/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.5618 - loss: 1.4322 - val_accuracy: 0.2108 - val_loss: 1.8702\n",
      "Epoch 5/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.7172 - loss: 0.9618 - val_accuracy: 0.2825 - val_loss: 1.9189\n",
      "Epoch 6/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8856 - loss: 0.5447 - val_accuracy: 0.2825 - val_loss: 2.0389\n",
      "Epoch 7/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9606 - loss: 0.2514 - val_accuracy: 0.2242 - val_loss: 2.2766\n",
      "Epoch 8/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9820 - loss: 0.1316 - val_accuracy: 0.2063 - val_loss: 2.4359\n",
      "Epoch 9/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9848 - loss: 0.0881 - val_accuracy: 0.2780 - val_loss: 2.4381\n",
      "Epoch 10/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9873 - loss: 0.0562 - val_accuracy: 0.3049 - val_loss: 2.5371\n",
      "Epoch 11/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9944 - loss: 0.0418 - val_accuracy: 0.2915 - val_loss: 2.6489\n",
      "Epoch 12/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9991 - loss: 0.0271 - val_accuracy: 0.2915 - val_loss: 2.7652\n",
      "Epoch 13/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9947 - loss: 0.0303 - val_accuracy: 0.2735 - val_loss: 2.7938\n",
      "Epoch 14/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9981 - loss: 0.0216 - val_accuracy: 0.2601 - val_loss: 2.8073\n",
      "Epoch 15/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9865 - loss: 0.0279 - val_accuracy: 0.2691 - val_loss: 2.8682\n",
      "Epoch 16/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9963 - loss: 0.0240 - val_accuracy: 0.2780 - val_loss: 3.0069\n",
      "Epoch 17/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9979 - loss: 0.0160 - val_accuracy: 0.2780 - val_loss: 2.9866\n",
      "Epoch 18/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9986 - loss: 0.0100 - val_accuracy: 0.2915 - val_loss: 3.0717\n",
      "Epoch 19/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9894 - loss: 0.0195 - val_accuracy: 0.2735 - val_loss: 3.1139\n",
      "Epoch 20/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9901 - loss: 0.0274 - val_accuracy: 0.2287 - val_loss: 3.2556\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1919 - loss: 3.4428 \n",
      "Test Loss: 3.3796417713165283\n",
      "Test Accuracy: 0.22869955003261566\n"
     ]
    }
   ],
   "source": [
    "# Tokenization and Padding using one_hot\n",
    "max_features = 10000  # Max number of words in tokenizer\n",
    "max_len = 100  # Max length of each sequence (pad/truncate to this length)\n",
    "encoded_docs = [one_hot(d, max_features) for d in stripped_data['Text']]\n",
    "x_data = pad_sequences(encoded_docs, maxlen=max_len)\n",
    "y_data = pd.get_dummies(stripped_data['Emotion']).values  # Convert categorical labels to one-hot encoded vectors\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, stratify=stripped_data['Emotion'], random_state=42)\n",
    "print('Shape of training data: ')\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print('Shape of test data: ')\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# CNN Model Architecture\n",
    "embedding_dim = 100\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_features, output_dim=embedding_dim))  # Specify input_dim instead of input_length\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y_data.shape[1], activation='softmax'))  # Output layer with softmax activation for multi-class classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Training the model\n",
    "batch_size = 20\n",
    "epochs = 20\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Mark II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data: \n",
      "(891, 100)\n",
      "(891, 8)\n",
      "Shape of test data: \n",
      "(223, 100)\n",
      "(223, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_19\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_19\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,100</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">97</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">30,100</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">40,100</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d_16         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,100</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">808</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_19 (\u001b[38;5;33mEmbedding\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │     \u001b[38;5;34m1,000,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_26 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m99\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │        \u001b[38;5;34m20,100\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_27 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m97\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │        \u001b[38;5;34m30,100\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_28 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │        \u001b[38;5;34m40,100\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_10 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_11 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d_16         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_21 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_32 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m10,100\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_22 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_33 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │           \u001b[38;5;34m808\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,101,208</span> (4.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,101,208\u001b[0m (4.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,101,208</span> (4.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,101,208\u001b[0m (4.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.1669 - loss: 2.0340 - val_accuracy: 0.2287 - val_loss: 1.9101\n",
      "Epoch 2/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.2304 - loss: 1.9629 - val_accuracy: 0.1570 - val_loss: 1.9051\n",
      "Epoch 3/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.2347 - loss: 1.9061 - val_accuracy: 0.2332 - val_loss: 1.8958\n",
      "Epoch 4/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.2995 - loss: 1.7757 - val_accuracy: 0.2287 - val_loss: 1.9401\n",
      "Epoch 5/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.4514 - loss: 1.4344 - val_accuracy: 0.2332 - val_loss: 2.2247\n",
      "Epoch 6/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.5480 - loss: 1.1780 - val_accuracy: 0.1928 - val_loss: 2.7669\n",
      "Epoch 7/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.6196 - loss: 0.9783 - val_accuracy: 0.1749 - val_loss: 3.3798\n",
      "Epoch 8/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.6796 - loss: 0.8216 - val_accuracy: 0.1704 - val_loss: 3.8527\n",
      "Epoch 9/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.7193 - loss: 0.7348 - val_accuracy: 0.1749 - val_loss: 4.3492\n",
      "Epoch 10/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.7845 - loss: 0.5945 - val_accuracy: 0.1570 - val_loss: 4.9084\n",
      "Epoch 11/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.8055 - loss: 0.4856 - val_accuracy: 0.1839 - val_loss: 5.2256\n",
      "Epoch 12/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.8181 - loss: 0.4761 - val_accuracy: 0.1704 - val_loss: 5.4903\n",
      "Epoch 13/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.8406 - loss: 0.4345 - val_accuracy: 0.1435 - val_loss: 5.7081\n",
      "Epoch 14/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.8696 - loss: 0.3471 - val_accuracy: 0.1659 - val_loss: 6.1494\n",
      "Epoch 15/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.8709 - loss: 0.3313 - val_accuracy: 0.2108 - val_loss: 6.3081\n",
      "Epoch 16/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.8974 - loss: 0.2487 - val_accuracy: 0.2108 - val_loss: 6.8424\n",
      "Epoch 17/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9053 - loss: 0.2654 - val_accuracy: 0.1839 - val_loss: 7.1072\n",
      "Epoch 18/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.8986 - loss: 0.2520 - val_accuracy: 0.1839 - val_loss: 7.2217\n",
      "Epoch 19/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9198 - loss: 0.2242 - val_accuracy: 0.2063 - val_loss: 7.3870\n",
      "Epoch 20/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.8994 - loss: 0.2578 - val_accuracy: 0.2152 - val_loss: 7.1512\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2052 - loss: 7.6637 \n",
      "Test Loss: 7.306800842285156\n",
      "Test Accuracy: 0.21524663269519806\n"
     ]
    }
   ],
   "source": [
    "# Tokenization and Padding using one_hot\n",
    "max_features = 10000  # Max number of words in tokenizer\n",
    "max_len = 100  # Max length of each sequence (pad/truncate to this length)\n",
    "encoded_docs = [one_hot(d, max_features) for d in stripped_data['Text']]\n",
    "x_data = pad_sequences(encoded_docs, maxlen=max_len)\n",
    "y_data = pd.get_dummies(stripped_data['Emotion']).values  # Convert categorical labels to one-hot encoded vectors\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, stratify=stripped_data['Emotion'], random_state=42)\n",
    "print('Shape of training data: ')\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print('Shape of test data: ')\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# CNN Model Architecture\n",
    "embedding_dim = 100\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(max_len,), dtype='int32'))\n",
    "model.add(Embedding(input_dim=max_features, output_dim=embedding_dim))  # Specify input_dim instead of input_length\n",
    "model.add(Conv1D(100, 2, activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Conv1D(100, 3, activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Conv1D(100, 4, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y_data.shape[1], activation='softmax')) \n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Training the model\n",
    "batch_size = 20\n",
    "epochs = 20\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with SMOTE (for undistributed data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data: \n",
      "(891, 100)\n",
      "(891, 8)\n",
      "Shape of test data: \n",
      "(223, 100)\n",
      "(223, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_20\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_20\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d_17         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_20 (\u001b[38;5;33mEmbedding\u001b[0m)        │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_29 (\u001b[38;5;33mConv1D\u001b[0m)              │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d_17         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_34 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_23 (\u001b[38;5;33mDropout\u001b[0m)            │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_35 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.1349 - loss: 2.0789 - val_accuracy: 0.0942 - val_loss: 2.0851\n",
      "Epoch 2/20\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.3062 - loss: 1.9835 - val_accuracy: 0.1345 - val_loss: 2.0463\n",
      "Epoch 3/20\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.6105 - loss: 1.5777 - val_accuracy: 0.2242 - val_loss: 1.9634\n",
      "Epoch 4/20\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8194 - loss: 0.8121 - val_accuracy: 0.2287 - val_loss: 2.0867\n",
      "Epoch 5/20\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9487 - loss: 0.3090 - val_accuracy: 0.2511 - val_loss: 2.3690\n",
      "Epoch 6/20\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9733 - loss: 0.1369 - val_accuracy: 0.2646 - val_loss: 2.5188\n",
      "Epoch 7/20\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9898 - loss: 0.0701 - val_accuracy: 0.2825 - val_loss: 2.6400\n",
      "Epoch 8/20\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9936 - loss: 0.0383 - val_accuracy: 0.2646 - val_loss: 2.7855\n",
      "Epoch 9/20\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9947 - loss: 0.0284 - val_accuracy: 0.2466 - val_loss: 2.9215\n",
      "Epoch 10/20\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9965 - loss: 0.0219 - val_accuracy: 0.2377 - val_loss: 3.0376\n",
      "Epoch 11/20\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9985 - loss: 0.0140 - val_accuracy: 0.2377 - val_loss: 3.1038\n",
      "Epoch 12/20\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9985 - loss: 0.0161 - val_accuracy: 0.2511 - val_loss: 3.1701\n",
      "Epoch 13/20\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9939 - loss: 0.0169 - val_accuracy: 0.2466 - val_loss: 3.2370\n",
      "Epoch 14/20\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9973 - loss: 0.0112 - val_accuracy: 0.2556 - val_loss: 3.3592\n",
      "Epoch 15/20\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9989 - loss: 0.0086 - val_accuracy: 0.2556 - val_loss: 3.4353\n",
      "Epoch 16/20\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9989 - loss: 0.0074 - val_accuracy: 0.2780 - val_loss: 3.4825\n",
      "Epoch 17/20\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9987 - loss: 0.0117 - val_accuracy: 0.2601 - val_loss: 3.5261\n",
      "Epoch 18/20\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9991 - loss: 0.0059 - val_accuracy: 0.2466 - val_loss: 3.6004\n",
      "Epoch 19/20\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9973 - loss: 0.0069 - val_accuracy: 0.2466 - val_loss: 3.6081\n",
      "Epoch 20/20\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9991 - loss: 0.0067 - val_accuracy: 0.2691 - val_loss: 3.6303\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2498 - loss: 3.7071 \n",
      "Test Loss: 3.775036573410034\n",
      "Test Accuracy: 0.2690582871437073\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Tokenization and Padding using one_hot\n",
    "max_features = 10000  # Max number of words in tokenizer\n",
    "max_len = 100  # Max length of each sequence (pad/truncate to this length)\n",
    "encoded_docs = [one_hot(d, max_features) for d in stripped_data['Text']]\n",
    "x_data = pad_sequences(encoded_docs, maxlen=max_len)\n",
    "y_data = pd.get_dummies(stripped_data['Emotion']).values  # Convert categorical labels to one-hot encoded vectors\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, stratify=stripped_data['Emotion'], random_state=42)\n",
    "print('Shape of training data: ')\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print('Shape of test data: ')\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "smote = SMOTE()\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# CNN Model Architecture\n",
    "embedding_dim = 100\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_features, output_dim=embedding_dim))  # Specify input_dim instead of input_length\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y_data.shape[1], activation='softmax'))  # Output layer with softmax activation for multi-class classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Training the model\n",
    "batch_size = 20#30\n",
    "epochs = 20\n",
    "model.fit(X_train_resampled, y_train_resampled, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data: \n",
      "(891, 100)\n",
      "(891, 8)\n",
      "Shape of test data: \n",
      "(223, 100)\n",
      "(223, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_21\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_21\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d_18         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_21 (\u001b[38;5;33mEmbedding\u001b[0m)        │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_30 (\u001b[38;5;33mConv1D\u001b[0m)              │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d_18         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_36 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_24 (\u001b[38;5;33mDropout\u001b[0m)            │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_37 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.1739 - loss: 2.0832 - val_accuracy: 0.1031 - val_loss: 2.0702\n",
      "Epoch 2/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.3333 - loss: 1.9949 - val_accuracy: 0.1435 - val_loss: 2.0757\n",
      "Epoch 3/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5517 - loss: 1.8354 - val_accuracy: 0.1076 - val_loss: 2.0628\n",
      "Epoch 4/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.7168 - loss: 1.5758 - val_accuracy: 0.1928 - val_loss: 1.9732\n",
      "Epoch 5/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8834 - loss: 0.9808 - val_accuracy: 0.1973 - val_loss: 1.9095\n",
      "Epoch 6/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9454 - loss: 0.4049 - val_accuracy: 0.2377 - val_loss: 1.9755\n",
      "Epoch 7/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9752 - loss: 0.1785 - val_accuracy: 0.2466 - val_loss: 2.0488\n",
      "Epoch 8/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9920 - loss: 0.0895 - val_accuracy: 0.2377 - val_loss: 2.1484\n",
      "Epoch 9/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9920 - loss: 0.0551 - val_accuracy: 0.2422 - val_loss: 2.1944\n",
      "Epoch 10/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9892 - loss: 0.0376 - val_accuracy: 0.2332 - val_loss: 2.2718\n",
      "Epoch 11/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9947 - loss: 0.0289 - val_accuracy: 0.2691 - val_loss: 2.3280\n",
      "Epoch 12/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9913 - loss: 0.0371 - val_accuracy: 0.2287 - val_loss: 2.4128\n",
      "Epoch 13/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9976 - loss: 0.0254 - val_accuracy: 0.2511 - val_loss: 2.4551\n",
      "Epoch 14/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9963 - loss: 0.0181 - val_accuracy: 0.2332 - val_loss: 2.4952\n",
      "Epoch 15/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9930 - loss: 0.0211 - val_accuracy: 0.2511 - val_loss: 2.5087\n",
      "Epoch 16/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9863 - loss: 0.0264 - val_accuracy: 0.2646 - val_loss: 2.5779\n",
      "Epoch 17/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9981 - loss: 0.0105 - val_accuracy: 0.2601 - val_loss: 2.5837\n",
      "Epoch 18/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9988 - loss: 0.0100 - val_accuracy: 0.2511 - val_loss: 2.6006\n",
      "Epoch 19/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9958 - loss: 0.0132 - val_accuracy: 0.2152 - val_loss: 2.6821\n",
      "Epoch 20/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9968 - loss: 0.0163 - val_accuracy: 0.2556 - val_loss: 2.7152\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2494 - loss: 2.8303 \n",
      "Test Loss: 2.8647234439849854\n",
      "Test Accuracy: 0.2556053698062897\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Tokenization and Padding using one_hot\n",
    "max_features = 10000  # Max number of words in tokenizer\n",
    "max_len = 100  # Max length of each sequence (pad/truncate to this length)\n",
    "encoded_docs = [one_hot(d, max_features) for d in stripped_data['Text']]\n",
    "x_data = pad_sequences(encoded_docs, maxlen=max_len)\n",
    "y_data = pd.get_dummies(stripped_data['Emotion']).values  # Convert categorical labels to one-hot encoded vectors\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, stratify=stripped_data['Emotion'], random_state=42)\n",
    "print('Shape of training data: ')\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print('Shape of test data: ')\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(stripped_data['Emotion']), y=stripped_data['Emotion'])\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "# CNN Model Architecture\n",
    "embedding_dim = 100\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_features, output_dim=embedding_dim))  # Specify input_dim instead of input_length\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y_data.shape[1], activation='softmax'))  # Output layer with softmax activation for multi-class classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Training the model\n",
    "batch_size = 20\n",
    "epochs = 20\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test), class_weight=class_weights)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with Word2Vec embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data: \n",
      "(891, 100)\n",
      "(891, 8)\n",
      "Shape of test data: \n",
      "(223, 100)\n",
      "(223, 8)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unrecognized keyword arguments passed to Embedding: {'weights': [array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n         0.        ,  0.        ],\n       [-0.0377471 ,  0.04051033,  0.01418809, ..., -0.05840995,\n         0.00959121, -0.00311465],\n       [-0.04275942,  0.04129154,  0.01375338, ..., -0.04937395,\n        -0.00240904, -0.00474263],\n       ...,\n       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n         0.        ,  0.        ],\n       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n         0.        ,  0.        ],\n       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n         0.        ,  0.        ]])]}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential()\n\u001b[1;32m     42\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Input(shape\u001b[38;5;241m=\u001b[39m(max_len,), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint32\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m---> 43\u001b[0m model\u001b[38;5;241m.\u001b[39madd(\u001b[43mEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43membedding_matrix\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# Pre-trained Word2Vec embeddings\u001b[39;00m\n\u001b[1;32m     44\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Conv1D(\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m5\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     45\u001b[0m model\u001b[38;5;241m.\u001b[39madd(MaxPooling1D(pool_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:81\u001b[0m, in \u001b[0;36mEmbedding.__init__\u001b[0;34m(self, input_dim, output_dim, embeddings_initializer, embeddings_regularizer, embeddings_constraint, mask_zero, lora_rank, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     72\u001b[0m     input_dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     80\u001b[0m ):\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_dim \u001b[38;5;241m=\u001b[39m input_dim\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dim \u001b[38;5;241m=\u001b[39m output_dim\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/keras/src/layers/layer.py:265\u001b[0m, in \u001b[0;36mLayer.__init__\u001b[0;34m(self, activity_regularizer, trainable, dtype, autocast, name, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_shape_arg \u001b[38;5;241m=\u001b[39m input_shape_arg\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[0;32m--> 265\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized keyword arguments \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    267\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassed to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    268\u001b[0m     )\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype_policy \u001b[38;5;241m=\u001b[39m dtype_policies\u001b[38;5;241m.\u001b[39mget(dtype)\n",
      "\u001b[0;31mValueError\u001b[0m: Unrecognized keyword arguments passed to Embedding: {'weights': [array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n         0.        ,  0.        ],\n       [-0.0377471 ,  0.04051033,  0.01418809, ..., -0.05840995,\n         0.00959121, -0.00311465],\n       [-0.04275942,  0.04129154,  0.01375338, ..., -0.04937395,\n        -0.00240904, -0.00474263],\n       ...,\n       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n         0.        ,  0.        ],\n       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n         0.        ,  0.        ],\n       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n         0.        ,  0.        ]])]}"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Tokenization and Padding using one_hot\n",
    "max_features = 10000  # Max number of words in tokenizer\n",
    "max_len = 100  # Max length of each sequence (pad/truncate to this length)\n",
    "encoded_docs = [one_hot(d, max_features) for d in stripped_data['Text']]\n",
    "x_data = pad_sequences(encoded_docs, maxlen=max_len)\n",
    "y_data = pd.get_dummies(stripped_data['Emotion']).values  # Convert categorical labels to one-hot encoded vectors\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, stratify=stripped_data['Emotion'], random_state=42)\n",
    "print('Shape of training data: ')\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print('Shape of test data: ')\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(stripped_data['Emotion']), y=stripped_data['Emotion'])\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "# Train Word2Vec model\n",
    "texts = stripped_data['Text'].apply(lambda x: x.split())\n",
    "word2vec_model = Word2Vec(sentences=texts, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Create embedding matrix\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((max_features, embedding_dim))\n",
    "word_index = tokenizer.word_index\n",
    "for word, i in word_index.items():\n",
    "    if i < max_features:\n",
    "        try:\n",
    "            embedding_vector = word2vec_model.wv[word]\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "# Build CNN Model with Word2Vec embeddings\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(max_len,), dtype='int32'))\n",
    "model.add(Embedding(input_dim=max_features, output_dim=embedding_dim, weights=[embedding_matrix], trainable=False))  # Pre-trained Word2Vec embeddings\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(y_data.shape[1], activation='softmax'))  # Output layer with softmax activation for multi-class classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Training the model\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test), class_weight=class_weights)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN GloVe embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping line: . . . -0.1573 -0.29517 0.30453 -0.54773 0.098293 -0.1776 0.21662 0.19261 -0.21101 0.53788 -0.047755 0.40675 0.023592 -0.32814 0.046858 0.19367 0.25565 -0.021019 -0.15957 -0.1023 0.20303 -0.043333 0.11618 -0.18486 0.0011948 -0.052301 0.34587 0.052335 0.16774 -0.21384 0.055947 0.24934 -0.12179 0.16749 0.28922 -0.033739 0.3015 -0.13241 0.092635 0.37155 -0.2884 -0.0052731 -0.001005 -0.51153 -0.28476 -0.20139 0.11837 -0.0055891 0.43604 0.16796 -0.2701 0.063957 -0.093253 -0.22079 0.36501 0.06545 0.23941 -0.19292 0.098293 0.12172 -0.1168 -0.027436 0.20507 -0.39139 -0.23111 0.46239 0.22888 -0.028415 -0.1798 0.23817 0.28093 -0.47935 0.23177 -0.35587 0.14246 0.11861 0.011018 0.091986 0.0054809 -0.39955 -0.40183 -0.10629 -0.30851 0.12383 -0.16737 -0.43569 0.4211 -0.57416 -0.19964 0.51312 0.090747 -0.21657 0.043519 0.24288 0.081134 0.49104 -0.33342 -0.31056 -0.3136 0.26931 -0.14402 0.33185 -0.21662 -0.072985 0.080603 -0.7266 -0.098385 -0.36233 -0.25346 0.1154 0.25738 0.15802 -0.15633 -0.024581 0.35673 0.31153 0.33475 -0.081155 -0.3061 0.019077 -0.049047 -0.11232 -0.07417 0.35596 -0.2642 0.012781 -0.20715 0.020223 0.054534 -0.28803 0.42863 -0.10312 0.24771 0.013196 0.19768 -0.013528 -0.15134 0.20307 -0.028973 -0.022706 -0.29199 -0.082062 0.19048 0.0053574 0.14067 -0.28675 0.21343 0.42428 -0.28186 -0.11801 -0.45227 -0.0067998 0.044784 -0.0062886 0.25087 0.34481 -0.64459 -0.20467 0.35007 0.1468 -0.14007 -0.0050219 -0.24053 0.41426 -0.40902 0.21141 0.25726 -0.4883 0.027066 0.56367 -0.39594 -0.035206 0.63079 0.14343 0.038315 0.32527 -0.080335 -0.20065 -0.30848 -0.0031591 0.15296 -0.21014 0.42143 -0.20944 -0.069285 0.13555 -0.020401 -0.22555 0.33491 0.16035 0.17739 -0.023627 0.097575 -0.19395 -0.018754 -0.119 -0.0067027 -0.4178 0.29027 0.13034 -0.30212 0.61173 -0.39918 -0.020191 -0.34531 -0.092082 0.46818 0.36671 0.21021 -0.053162 -0.37872 -0.14271 -0.13604 0.31715 -0.17227 -0.091266 0.16417 0.15069 0.53556 -0.29678 0.13965 -0.29788 0.1282 0.1971 -0.045515 -0.41355 -0.050333 -0.39015 -0.29579 -0.096145 -0.03151 0.053714 -0.37309 -0.36523 -0.17235 0.39251 -0.065909 -0.25267 -0.34448 -0.11503 0.43665 0.18832 0.20631 0.27801 -0.046077 0.13397 -0.091953 -0.098542 0.15811 0.2752 0.081383 0.32077 -0.10028 0.1088 -0.24836 0.10477 0.15243 -0.071302 0.12861 0.23061 0.0074864 0.090918 -0.12269 -0.14831 0.010586 0.35745 -0.23412 -0.23746 -0.22646 -0.27641 -0.1634 0.071909 -0.093884 0.21331 -0.20627 0.44406 0.34691 0.019064 0.034657 0.36789 0.32276 -0.31099 -0.023443 -0.77048 -0.26001 0.033961 -0.13874 0.051973 -0.0090509 0.27427 0.046548 -0.48214 -0.1437 -0.1975 -0.038126 -0.16555 0.071697 0.049449 0.15386 -0.81663\n",
      "\n",
      "Skipping line: at name@domain.com 0.0061218 0.39595 -0.22079 0.78149 0.38759 0.28888 0.18495 -0.37328 -0.60018 0.19625 0.42975 0.17942 0.06375 -0.44127 0.72035 0.50539 0.17985 -0.71305 0.11122 0.19733 0.063884 0.023288 0.017074 0.04756 -0.083167 0.14506 -0.21856 -0.07979 -0.058909 -0.79864 0.65868 -0.45031 0.41921 -0.043908 -0.059099 0.21384 -0.05214 0.31267 0.20417 -0.66489 -0.17885 -0.13729 -0.13825 -0.59669 -0.09297 0.31121 -0.027161 0.17923 0.43076 0.1012 0.32516 -0.34159 0.21662 -0.31267 -0.065984 -0.35734 -0.52104 -0.16432 -0.31183 0.33793 -0.3952 0.11288 0.10796 0.16632 -0.077719 0.48287 -0.50849 0.37261 0.27273 0.36141 -0.027295 -0.44898 0.48865 0.13509 0.0082683 0.44873 0.46155 -0.28687 0.28691 -0.16943 -0.4255 -0.13788 -0.84983 0.38833 -0.092206 0.09504 0.57431 -0.55836 0.37006 -0.23685 0.31727 -0.29539 0.40986 -0.30945 -0.4857 0.52769 -0.94486 -0.22039 0.1913 0.014544 0.37755 -0.36721 -0.07249 -0.17701 0.32814 0.2158 0.43346 0.14774 -0.59317 -0.23463 -0.064771 -0.42996 0.59558 0.62189 0.62726 -0.97772 -0.15548 -0.25757 0.57236 0.7448 0.33936 0.088019 0.12554 0.44163 0.48089 0.14379 0.44052 0.0033798 0.17334 0.62277 0.13859 -0.46311 -0.3785 -0.09034 -0.31398 0.43096 0.14501 0.10217 -0.30989 -0.20551 0.95437 0.36931 -1.0046 -0.20179 0.2239 0.093057 0.4248 -0.14321 -1.0822 0.49819 0.42587 0.48591 0.22223 0.01698 -0.082398 0.69111 0.3967 -0.50514 0.12603 0.2714 0.7082 -0.079401 -0.10775 -0.48217 0.85182 -0.0047614 0.06633 0.065622 -0.31762 0.14979 -0.48519 0.31868 0.66676 -0.53971 0.26185 -0.025039 -0.042315 0.096601 -0.80947 0.11489 0.22729 0.15132 0.48739 -0.17873 0.86134 0.20061 0.018419 -0.57752 0.31828 0.46305 0.31762 -0.43985 0.56173 0.14558 -0.23908 0.94529 0.35095 0.09231 0.38298 0.18845 0.12514 0.11943 -0.11206 0.17589 0.2973 0.31892 -0.015596 0.32167 0.066616 -0.04847 0.31435 0.12557 0.036991 0.17287 -0.055305 0.87528 0.11223 0.28194 -0.08516 0.58705 0.18693 -0.32145 0.46188 0.51299 -0.88555 -0.46913 -0.26764 -0.51375 -0.38804 -0.78149 -0.14336 -0.25571 0.12985 0.33621 -0.22846 0.48309 -0.47053 -0.23177 0.17509 0.060625 0.19792 -0.10821 -0.090677 -0.18252 0.73394 -0.17721 0.28438 0.75051 0.37784 -0.26525 0.15274 -0.4612 -0.60663 -0.042408 0.093703 -0.59664 0.097915 -0.46943 -0.18268 -0.26516 -0.14962 0.05476 -0.078979 -0.50688 0.39585 0.69243 0.039802 0.13965 0.21709 0.023105 0.19744 0.45611 0.26455 -0.17963 -0.2695 0.16681 0.27557 -0.13141 -0.12941 -0.40996 0.23409 0.40783 0.28148 -0.15247 0.17776 0.077139 0.30951 0.21843 -0.075161 0.33983 0.30904 0.1404 0.5608 -0.020466 0.29512 -0.43178 -0.31083 -0.28874 -0.12015 0.38455\n",
      "\n",
      "Skipping line: . . . . . -0.23773 -0.82788 0.82326 -0.91878 0.35868 0.1309 0.26195 -0.30068 0.42963 -1.5335 0.50492 0.59069 0.17763 -0.010302 -0.63371 -0.040832 0.69336 -0.85472 0.693 -0.44824 0.25476 0.17091 0.96056 -0.26516 0.17454 -0.27371 0.20591 -0.60435 -0.50923 0.36049 0.75756 -0.79493 0.12379 0.40019 0.56716 0.19451 0.15996 -0.07882 0.9756 0.54761 -0.25123 -0.70006 -0.78286 -0.95702 0.23045 -0.3643 0.80267 -0.30315 0.35283 -0.19961 0.71134 -0.94116 -0.086543 0.17827 0.40124 -0.071006 0.82202 -0.51055 0.47492 -0.0582 -0.2687 0.94119 0.0066621 -0.53636 -0.68859 0.27149 0.33482 0.53987 -0.72912 -0.53719 0.17591 -0.52643 0.42509 0.12433 -0.17178 0.53168 0.69058 0.703 0.17845 -0.39243 -0.50617 0.39575 0.18461 -0.26065 -0.61119 0.013174 0.32934 -0.4861 -0.46474 0.69677 0.47846 -0.43841 0.051551 0.4195 -0.36847 0.57844 0.053724 -0.026579 -0.5901 0.73221 -0.53987 -0.3428 -0.12343 -0.51213 -0.59237 -0.14589 0.094055 -0.48848 -0.09944 0.092527 0.76764 -0.21207 -0.2395 0.27517 0.27508 0.29604 0.01908 -0.64152 0.51208 0.30948 -0.47126 0.86562 -0.68993 0.19003 -0.78941 -0.10668 -0.64497 0.078153 -0.29598 -0.2383 0.40626 0.066945 0.021937 -0.28291 -0.34962 0.098551 -0.068353 0.06824 -0.17466 -0.21469 1.4612 0.30606 -0.28516 0.078682 0.54627 -0.28879 0.11363 0.32769 -0.38409 -0.038104 -0.70277 -0.47623 -0.29545 0.1906 1.2011 0.38105 0.24595 -0.15847 0.051696 0.71491 0.040783 -0.24121 -0.69558 0.8648 -0.18283 -0.11746 -0.058645 0.033855 -0.32785 -0.23554 -0.12762 0.088837 1.1886 0.6515 -0.32243 0.20434 -0.28283 -0.076 -0.5461 0.66366 -0.19004 0.41687 0.16786 -0.57624 0.0021248 0.038208 0.46581 0.17952 0.15208 0.77239 0.56825 1.3564 0.36707 -0.90959 -0.20369 -0.30854 0.43101 -0.62625 0.62072 -0.48968 -0.4026 0.83121 0.27788 -0.63801 -0.90269 -0.26409 0.55212 -0.21899 0.57153 0.10422 -0.23276 0.32775 0.15975 0.52786 -0.18071 -0.66116 -0.28231 -0.95566 0.32314 -0.10176 0.49961 -0.59512 0.30664 0.17566 -0.0082404 -1.2304 -0.18822 -0.094328 -0.25801 0.07948 0.59872 -0.029741 -0.14526 -0.044699 0.23121 0.20907 -0.442 0.40599 0.16151 -0.49981 0.13384 0.35293 0.034782 0.513 -0.39248 0.39123 0.28351 0.62023 0.6796 0.83006 0.48423 0.70625 -0.25091 -0.23268 0.15136 -0.35641 -0.45169 -0.071751 0.61275 0.12885 -0.62407 0.38831 -0.38358 -0.40596 0.022178 0.44976 -0.11437 -0.43881 -0.72876 -0.76978 -0.42424 0.26061 -0.83878 0.7724 -0.050794 0.0083812 0.18012 -0.77772 -0.22227 0.79349 0.3748 -0.96256 -0.97854 -0.92611 -0.65978 0.080195 -0.25126 1.1857 -0.72262 1.0632 0.50418 0.07075 -0.25384 -0.57426 -0.19791 -0.68991 0.061501 -0.17089 0.18609 -0.78265\n",
      "\n",
      "Skipping line: to name@domain.com 0.33865 0.12698 -0.16885 0.55476 0.48296 0.45018 0.0094233 -0.36575 -0.87561 -0.35802 0.2379 0.31284 -0.081367 0.061482 0.81921 0.77488 0.68518 -0.48005 -0.012098 0.53366 0.038321 0.26857 0.56736 0.20427 0.2847 0.68113 -0.26921 0.10099 -0.33252 -0.22999 0.66003 0.21833 -0.086523 -0.30044 -0.42253 0.47525 -0.2165 0.3268 0.63515 -0.15657 -0.25835 -0.11663 -0.41092 -0.73779 -0.0015122 0.14481 0.13287 0.26097 0.58175 0.29285 0.27168 -0.0058512 0.27731 -0.40565 0.05047 0.059203 -0.39081 -0.098029 -0.13969 0.42714 -0.20103 -0.019703 1.1957 0.36278 -0.6468 0.096856 -0.54383 0.29666 -0.0098302 0.5042 -0.3419 -0.25067 0.72375 0.81957 0.57959 -0.28619 0.91511 -0.49127 0.42129 0.11429 -0.32411 -0.092257 -1.0342 -0.25774 -0.19044 -0.34201 1.2339 -0.65392 0.83096 -0.3133 -0.10256 -0.075641 0.88435 -0.1656 0.041536 0.23504 -0.59116 -0.12573 0.48002 0.23561 -0.16167 -0.1404 -0.45722 -0.13186 0.9173 -0.78831 -0.027372 0.036396 -0.49337 -0.2556 -0.15139 -0.53403 0.90196 0.60318 0.43602 -0.075461 -0.09196 0.14248 0.14335 0.67287 0.52725 -0.21312 -0.31636 0.63391 0.33626 -0.037607 0.60729 0.46028 0.0010946 0.22783 0.38501 -0.85127 -0.76092 0.20159 -0.13641 -0.15588 0.060973 0.13442 -0.25715 0.054989 0.82028 0.32086 -0.94738 -0.55948 -0.072619 -0.31286 0.53855 -0.27876 -0.45292 1.0417 0.59105 0.10445 -0.0042194 -0.11733 0.38836 0.43267 0.43228 -0.3419 0.091655 0.47851 0.62238 0.091656 -0.11656 -0.6118 0.076486 -0.27552 -0.33407 0.30166 -0.089347 -0.4676 0.14198 0.2924 0.69944 -0.53839 0.14589 0.36273 -0.18743 -0.32278 -0.46289 -0.0079593 0.025591 0.47468 0.79237 -0.72242 0.92688 0.49471 -0.39816 -0.099986 0.15334 -0.050664 0.74726 -0.047998 0.43063 -0.1613 -0.020673 0.98916 -0.27938 0.10797 0.55202 0.37686 0.17607 -0.27058 -0.1362 -0.024761 0.63829 -0.0054866 0.12085 0.56029 0.28192 0.29992 0.23869 0.070296 0.34272 -0.11676 -0.24377 0.90668 0.1251 -0.12149 -0.057172 0.36016 0.057005 -0.33869 0.14762 0.91449 -0.20497 -0.64492 -0.47259 -0.43348 -0.47966 -0.84446 -0.35207 0.0050338 -0.33834 0.15072 -0.9816 0.13519 -0.33266 0.11602 0.032961 -0.58063 0.039186 -0.34515 -0.088268 -0.49642 0.56537 -0.13342 0.29861 0.4289 0.72562 -0.61233 -0.41913 0.19969 -0.64272 -0.62544 0.58792 -0.33489 -0.066748 -0.42025 -0.25105 -0.52392 -0.17985 -0.059297 -0.28532 -0.76441 0.41743 0.83472 0.46515 0.3741 0.35699 -0.11646 0.10322 -0.16215 -0.30452 -0.0082364 -0.49549 0.47146 -0.054368 -0.18286 0.083092 -0.2659 0.449 0.3066 0.19909 0.21744 0.77927 -0.09052 -0.28405 0.02556 -0.27124 -0.26324 0.45414 0.19379 -0.21259 -0.38467 0.18494 -0.26692 -0.18104 0.051336 -0.25989 -0.15024\n",
      "\n",
      "Skipping line: . . 0.035974 -0.024421 0.71402 -0.61127 0.012771 -0.11201 0.16847 -0.14069 -0.053491 -0.87539 -0.13959 0.29731 0.072308 -0.084514 -0.1879 0.12358 0.37639 -0.39238 -0.01111 -0.04924 0.63649 0.058814 0.19076 -0.20828 -0.11036 0.14934 0.24667 -0.39438 0.22853 -0.11201 0.33539 -0.32929 -0.049727 -0.090764 0.29095 0.27504 0.22802 -0.15616 0.37302 0.3752 -0.3677 0.1518 -0.27551 -0.63281 -0.31298 -0.22441 -0.15435 -0.64802 0.28404 0.12356 0.0034255 0.03094 0.35345 -0.46781 0.59203 -0.17966 0.27702 -0.46738 0.19438 0.21939 -0.36743 -0.084781 0.03253 -0.51323 -0.55466 0.49585 0.066985 0.47906 -0.25118 0.011123 0.15605 -1.0761 0.60875 -0.15764 0.066122 0.12779 -0.089209 0.4311 0.045732 -0.29364 -0.19994 -0.065952 0.26236 0.34039 -0.4956 -0.41187 0.055566 -0.69902 -0.057696 0.76519 0.2018 -0.34497 -0.22707 0.34316 -0.16098 0.42469 0.0080257 -0.33017 -0.43485 0.23581 -0.71085 0.27985 -0.31261 -0.012817 0.48305 -0.75151 -0.02347 -0.39653 -0.86857 0.2877 0.26678 0.22291 -0.1736 -0.12782 0.35032 0.27365 0.28287 0.093409 -0.18104 -0.088499 0.1189 0.026563 -0.027942 0.17254 -0.032427 -0.10745 -0.30334 -0.096047 -0.45369 -0.12113 0.06388 -0.31841 0.0059388 0.17693 -0.21071 -0.6171 0.0018674 0.27296 0.18762 -0.060409 0.5949 0.13994 -0.25773 0.20023 0.4918 0.010659 0.046456 0.4339 -0.10386 0.021517 -0.42845 -0.25458 -0.1307 0.28307 -0.27127 0.080445 -0.31285 -0.12807 0.71382 -0.086474 -0.35553 0.65281 -0.33706 0.38617 -0.12551 0.0056478 0.10091 -0.3638 -0.033486 0.32146 -0.28719 -0.24936 0.65761 0.24376 -0.068703 0.35459 0.080304 -0.30996 -0.20199 0.31482 0.15092 -0.25125 0.76149 -0.33679 -0.079472 -0.04 -0.024693 -0.55248 0.38834 0.14696 0.50003 -0.1377 0.20994 -0.53022 -0.23712 0.24392 0.29524 -0.20951 0.11347 0.16736 -0.057263 0.20945 -0.49785 0.22321 -0.24234 -0.076378 0.42953 0.71138 0.12599 -0.1331 -0.13823 -0.22315 -0.17269 0.43008 -0.34042 -0.23127 0.66599 0.15312 0.47323 -0.28108 0.097872 -0.33014 -0.068678 0.57197 0.099838 -0.6237 -0.22572 -0.59751 -0.30157 -0.1239 0.0057373 -0.058747 -0.030736 -0.10812 0.17601 0.26234 0.15636 -0.19436 -0.097775 0.15462 -0.083865 -0.15106 0.27862 0.28175 -0.27084 0.029867 0.082898 0.020298 0.35015 -0.027691 -0.010642 0.21173 0.090988 0.59747 0.12784 -0.31951 0.26881 -0.41771 -0.2073 -0.077332 -0.32069 -0.020763 -0.24735 -0.23254 -0.005691 -0.088722 -0.13886 -0.16886 -0.13215 -0.17242 -0.1223 -0.24484 0.1374 0.24458 -0.1688 0.64932 0.051973 -0.30896 0.1567 0.351 0.081668 -0.60955 -0.54647 -0.61302 -0.48092 0.22664 -0.27639 0.12523 -0.1358 0.3322 0.54997 -0.068345 0.07199 -0.11543 0.19326 -0.085861 -0.1098 0.034066 -0.072258 -0.58648\n",
      "\n",
      "Skipping line: . . . . 0.033459 -0.085658 0.27155 -0.56132 0.60419 -0.027276 -0.093992 0.068236 -0.3961 -0.83028 0.17456 0.46373 0.13719 0.25598 -0.33885 0.18365 0.44451 -0.8193 -0.081032 -0.070653 0.11253 -0.0087314 0.45494 -0.13481 -0.19651 -0.0098954 0.34683 -0.010663 -0.10555 -0.027425 0.46831 -0.29624 0.0027633 -0.18621 0.32282 0.20276 0.5976 -0.15331 0.52121 0.59813 -0.28581 -0.23798 -0.078883 -0.64561 0.03057 0.28304 0.15654 -0.18034 0.48116 0.39754 0.2106 -0.039421 -0.31166 0.38636 0.64125 -0.52607 0.064417 -0.23567 0.37243 -0.089502 -0.39855 -0.12211 0.37331 -0.45538 -0.40342 0.65258 0.14624 0.3247 -0.41644 0.15981 0.092788 -0.47863 0.64507 -0.013909 0.21356 0.39679 0.52347 0.16871 -0.017134 -0.57287 -0.47366 0.30996 -0.32248 -0.11949 -0.48315 -0.20478 0.45759 -1.0443 -0.58684 0.58544 -0.081284 -0.21224 -0.25302 0.90371 -0.20399 0.65895 -0.11742 -0.13352 -0.091149 0.11375 -0.18618 0.098569 -0.2067 -0.22156 -0.1557 -0.7965 -0.23144 -0.31047 -0.46223 0.26712 0.31644 -0.066401 -0.40895 0.11665 0.50156 0.47769 -0.075403 -0.482 0.033416 0.17506 -0.063225 0.19088 0.039387 0.2396 -0.35331 -0.09274 -0.33705 0.085312 0.080227 -0.020173 0.507 -0.072361 0.13175 -0.45573 0.20334 -0.056897 -0.11733 0.047485 0.016908 -0.24814 0.71598 -0.055958 0.37822 0.3392 0.17434 -0.37932 0.03775 0.019235 -0.29447 -0.18964 -0.45798 -0.16 -0.056206 0.042038 0.64842 0.47592 -0.67641 -0.57816 0.769 -0.096332 -0.064562 0.042617 -0.23167 0.51994 0.10079 0.15415 -0.14515 -0.21889 0.23307 -0.043176 -0.41485 0.1427 0.84153 -0.078821 0.060877 0.12004 -0.26696 -0.53933 -0.36731 -0.37563 -0.35086 -0.24055 0.49705 -0.34699 0.021831 0.53459 0.2906 -0.065185 -0.19822 0.33217 0.28218 0.34624 0.36723 -0.66741 0.082508 -0.10312 0.27671 -0.55848 0.67853 0.040049 -0.0057962 0.86106 -0.40337 -0.45425 -0.48904 -0.11567 0.71084 -0.00071633 -0.075139 0.29584 -0.38594 0.17853 0.033168 0.19186 -0.10717 -0.51614 0.19278 -0.43339 0.51961 0.10363 0.28009 -0.41613 0.34869 0.052315 -0.026509 -0.64101 -0.047879 -0.42739 -0.018592 0.18577 -0.16994 -0.18489 -0.076386 -0.28981 0.26335 0.21274 0.11926 -0.32697 0.22216 -0.24976 0.36953 0.24742 0.37245 0.37218 -0.18512 -0.23093 0.035941 0.2418 0.058993 0.16378 0.43893 0.19057 -0.15457 0.17481 -0.30859 -0.1335 0.21542 -0.44562 0.48477 0.28185 -0.063253 0.053603 -0.05103 -0.29387 0.17704 0.64601 -0.11012 -0.23288 -0.60988 -0.33054 -0.25336 -0.042175 -0.43281 -0.061033 -0.097728 0.42459 0.051169 -0.37042 0.053402 0.18471 0.37567 -0.66422 -0.2099 -0.92126 -0.098194 0.034713 0.19768 0.35687 -0.03349 0.72838 0.1016 -0.42271 -0.19997 -0.21866 -0.2066 -0.59319 -0.052248 -0.032041 0.068443 -0.92476\n",
      "\n",
      "Skipping line: email name@domain.com 0.33529 0.32949 0.2646 0.64219 0.70701 -0.074487 -0.066128 -0.30804 -0.71712 -0.65856 0.21812 0.20661 0.079468 -0.37277 0.69841 0.40459 0.10838 -0.84972 0.084111 -0.099199 -0.15883 -0.15245 0.10664 0.2866 0.058125 0.10893 -0.042369 -0.10781 -0.10683 -0.61831 0.71169 -0.4143 0.46387 0.11437 0.11797 0.78043 0.15162 0.050014 0.27046 -0.15689 -0.35901 -0.22091 0.2174 -0.53654 -0.26149 0.071581 0.28225 0.61944 0.56737 0.054449 0.63952 -0.25328 0.34196 -0.39231 0.14372 -0.3836 -0.43978 -0.13577 -0.38214 0.58241 -0.49185 0.020045 0.025588 0.13079 -0.43368 0.39428 -0.48619 0.53205 0.231 0.026263 0.20574 -0.5681 0.23058 -0.20142 0.23862 0.34402 0.53577 -0.36288 0.24647 -0.15684 -0.49296 0.063751 -0.35141 -0.10532 0.0045396 -0.047133 0.9011 -0.60606 0.073045 -0.4576 0.052548 -0.4998 0.55807 -0.21151 -0.60018 1.0234 -0.80815 0.15263 0.47984 0.09278 0.35607 -0.28802 -0.14008 -0.12359 0.48392 -0.54092 0.26861 0.18011 -0.5053 0.35732 0.027227 -0.63815 0.65042 0.60165 0.71557 -0.58427 0.0075552 0.071666 0.31442 0.5006 0.14557 0.057359 0.23515 0.09242 0.40623 0.12675 0.40275 0.11623 0.13235 0.048337 0.40902 -0.46324 -0.36735 0.10782 -0.1503 0.20129 0.1842 -0.20326 0.025293 -0.60814 0.94863 0.19966 -1.0322 -0.33815 0.42628 -0.18858 0.70273 -0.10762 -0.57743 0.14396 0.57879 0.55755 -0.10448 -0.08187 -0.0042659 0.41304 0.61282 -0.27785 0.19742 0.62788 0.44321 -0.0018459 0.076699 -0.37352 0.64692 -0.21255 0.12151 0.14871 -0.35841 -0.14698 -0.49289 0.36204 0.66804 -0.49378 0.21669 0.021931 -0.33109 -0.00088334 -0.48386 0.64701 0.10726 0.26803 0.54351 -0.19216 0.82757 0.092724 0.20233 -0.057761 0.14353 0.35351 0.053238 -0.34339 0.50812 0.027739 -0.17723 0.91226 0.24673 -0.16193 0.48266 0.17719 0.37488 -0.13047 -0.071644 -0.050737 0.36963 0.54698 -0.027903 0.010671 0.059521 0.37979 -0.0033416 0.43249 0.092561 -0.10606 0.13377 0.72729 -0.0078999 0.30048 0.12956 0.82322 0.71878 -0.6804 0.24552 0.49127 -0.62899 -0.757 -0.20259 -0.43667 -0.38061 -0.97217 -0.059481 -0.1246 0.23146 0.11842 -0.32492 0.41028 -0.43662 -0.16969 0.29064 0.053265 0.012703 -0.050542 -0.15929 -0.14017 0.46322 -0.25027 0.26577 0.65584 0.32285 -0.3679 0.1908 -0.30116 -0.61803 -0.17922 0.21711 -0.26571 -0.26099 -0.44266 -0.1252 -0.43041 -0.011423 0.30861 0.11921 -0.24908 0.58099 0.73546 -0.06392 0.18363 0.2339 0.042055 0.040574 0.47928 0.25323 0.019253 -0.2219 0.36961 0.021001 -0.0050943 -0.18382 -0.7743 0.24078 0.31495 0.35245 -0.4387 0.21064 0.41219 0.48009 0.074987 -0.11304 0.37291 0.43416 0.15984 0.30369 -0.45189 0.040446 -0.33307 -0.41444 -0.13447 0.09813 0.30967\n",
      "\n",
      "Skipping line: or name@domain.com 0.48374 0.49669 -0.25089 0.90389 0.60307 0.11141 -0.021157 0.10037 -0.15173 -1.0246 0.27841 -0.062042 -0.061043 -0.47401 0.60886 0.29636 -0.077564 -0.81931 -0.023403 0.30916 0.029043 -0.34372 -0.20211 0.17291 -0.49248 0.19652 0.30329 -0.023517 0.062654 -0.56825 0.20126 -0.66776 0.064812 -0.024986 -0.081007 0.38214 -0.0035715 0.40572 0.090373 -0.023844 -0.14541 0.049812 0.054826 -0.53691 -0.14741 -0.097398 0.028146 -0.011163 0.71235 0.36371 0.66517 -0.043227 0.34479 -0.4558 -0.012266 -0.4921 -0.98134 -0.063421 -0.11077 0.46109 -0.82556 -0.25896 0.48532 0.31552 -0.57685 0.63555 -0.40461 0.41238 0.0662 0.17397 -0.1019 -0.65447 0.20334 0.053966 -0.15827 0.27588 0.43867 -0.081736 0.47221 -0.45256 -0.43325 0.14461 -0.14118 0.20053 -0.3692 0.018312 0.8577 -0.61049 -0.15427 -0.4262 0.34196 -0.36692 0.51229 -0.14486 -0.73069 1.002 -1.0086 -0.18131 0.10296 0.10914 0.34994 -0.094706 -0.22878 -0.24424 0.43829 0.063811 0.22713 -0.023387 -0.38057 0.20119 -0.027965 0.027987 0.33908 0.63356 0.64577 -0.73545 -0.064695 -0.072888 0.49183 0.26422 0.2987 0.17597 0.22247 0.082598 0.13588 0.15017 0.25407 0.27976 0.49918 0.4207 0.2508 -0.31198 -0.2434 0.079441 -0.32652 0.34065 0.090272 0.074694 0.030794 -0.69689 1.2249 0.24339 -1.0911 0.17123 0.12347 -0.22785 0.36976 -0.55589 -0.87727 0.33553 0.18196 0.3075 0.054316 -0.14845 -0.30466 0.33621 0.6362 -0.38048 0.15098 0.2354 0.81349 -0.30932 -0.13298 0.11503 0.82272 0.35549 0.059715 0.23187 -0.15168 0.17065 -0.15352 0.43389 0.64511 -0.033239 0.26057 -0.081056 -0.43354 -0.067527 -0.27017 0.3825 0.078534 0.19403 0.92968 0.061512 0.84848 0.29075 0.48908 0.043084 0.19724 0.81964 0.11703 -0.029999 0.85199 0.076113 -0.56209 0.86088 0.50691 0.10746 0.45046 -0.0016683 0.51397 0.14412 -0.45557 0.093795 -0.023058 0.20497 -0.23176 0.19408 0.096653 0.52317 -0.1884 0.04104 0.046953 -0.11417 0.035494 0.5119 0.23042 0.19461 0.36794 0.63884 0.52288 -0.33988 0.60725 0.35293 -0.73891 -0.3898 -0.23657 -0.5773 -0.24254 -0.98802 0.27399 -0.26853 0.23769 0.21178 -0.08188 0.29739 -0.50025 -0.26005 0.43288 -0.22978 0.20331 -0.36308 -0.022572 0.042341 0.43218 -0.016921 0.13097 1.0983 0.29603 -0.29021 0.020473 -0.31833 -0.58473 -0.098 0.049079 -0.52673 0.20277 -0.38167 -0.42331 -0.33024 -0.6353 0.11201 -0.0072223 -0.098756 0.084024 0.38257 -0.17998 -0.10995 -0.17451 0.57884 0.051173 0.33876 0.17066 -0.0044339 -0.48094 0.45427 0.21647 -0.33465 -0.30874 -0.84948 0.23015 0.26173 0.48439 -0.37949 0.16163 0.42102 -0.20904 -0.028854 -0.25135 0.6825 0.50248 -0.045756 0.41991 -0.22363 0.43526 -0.27397 -0.17557 -0.21913 -0.10409 0.64972\n",
      "\n",
      "Skipping line: contact name@domain.com 0.016426 0.13728 0.18781 0.75784 0.44012 0.096794 0.060987 0.31293 -0.15884 -1.2367 0.43769 0.10465 0.048858 -0.23182 0.71125 0.022376 0.63524 -1.4974 0.12243 -0.07386 -0.021514 -0.37652 0.17503 -0.011225 -0.12668 -0.0090601 0.38418 0.11132 0.15851 -0.47498 0.33619 -0.48833 0.23423 0.13258 0.29362 0.13526 -0.05115 -0.0055236 0.27734 -0.23565 0.19571 -0.29095 0.062419 -0.47502 -0.71402 -0.36384 0.53562 0.40136 0.30963 0.16238 -0.11662 -0.16201 0.30672 0.21663 0.086839 -0.38895 -0.19644 -0.52311 -0.33153 0.27012 -0.89654 -0.15193 0.12447 -0.19112 -0.494 -0.011873 -0.41412 0.52585 0.27316 -0.047525 -0.1178 -0.3371 0.61151 -0.012169 0.36935 0.32679 -0.098269 0.038729 0.003551 -0.51871 -0.48189 -0.079238 -0.34291 -0.44045 -0.24479 0.05593 0.83227 -0.55939 -0.29242 -0.19718 0.17693 -0.12205 0.55837 -0.28505 -0.64676 0.57716 -1.4398 0.066288 -0.086048 0.381 -0.25805 -0.11941 -0.25664 -0.057845 1.0033 -1.0863 0.14343 0.17181 -0.81313 0.19286 0.12922 -0.20835 0.98495 0.58797 0.33635 -0.68359 0.5062 0.10678 0.57212 0.59786 0.053268 0.028426 0.42805 0.23711 0.66162 0.2584 0.32478 -0.041477 0.35121 0.19489 0.0080291 -0.36795 -0.38805 0.17224 -0.36105 0.36373 0.011708 -0.056135 0.26371 -0.59047 1.1774 0.36488 -0.66474 -0.5875 0.38414 -0.055643 0.39133 -0.34066 -0.47462 0.58672 0.46918 0.6621 0.085039 -0.26208 -0.27596 0.51976 0.78402 -0.21788 0.41615 0.057734 0.48454 0.032466 -0.57003 -0.24011 0.64941 0.1786 0.16456 0.22892 -0.34876 -0.21609 -0.40112 0.69337 0.85653 -0.14865 0.55925 0.28205 -0.48622 0.041444 -0.52311 0.89091 -0.14692 0.59723 0.45691 -0.34411 0.71453 -0.028327 0.066396 -0.28523 -0.066252 0.52379 0.26523 -0.07658 0.41631 -0.56412 -0.11156 0.55618 0.58285 0.19863 0.28857 0.26299 0.9523 0.15152 -0.021361 0.024232 0.29266 0.47276 -0.061013 -0.34556 -0.065448 0.51832 -0.01764 -0.065723 0.035893 -0.57871 -0.36286 0.53646 -0.30941 0.28428 0.035597 0.74796 0.71145 -0.51704 -0.059401 0.29316 -0.33866 -0.47136 -0.26915 -0.48318 -0.73448 -0.82325 0.18229 -0.19008 -0.17385 0.34227 -0.016458 0.15097 -0.77959 -0.72615 0.38242 -0.25839 0.067803 -0.22942 -0.70517 -0.31424 0.57725 -0.18576 0.3782 0.72404 0.56874 -0.16028 0.13383 -0.54238 -0.47211 0.30398 -0.20181 -0.35449 0.40175 -0.58787 0.2751 -0.0001217 0.1691 0.33633 -0.1821 -0.33645 0.3404 0.60535 -0.17278 0.31449 0.11914 -0.22371 0.043944 0.45005 0.89327 -0.18597 -0.77009 0.79949 0.378 0.026014 -0.39198 -0.80185 0.24173 0.22765 0.75733 -0.37899 0.30622 0.56785 0.43714 0.18369 -0.027663 0.099504 0.33348 0.088492 0.080002 -0.75372 0.26302 -0.14392 -0.37882 0.31386 -0.084045 0.70962\n",
      "\n",
      "Skipping line: Email name@domain.com 0.37344 0.024573 -0.12583 0.36009 0.25605 0.07326 0.3292 -0.0037022 -0.10145 -1.8061 0.17271 0.40612 0.14999 0.077813 0.87133 0.37621 0.2232 -1.5266 0.38978 0.072969 0.4398 0.25206 0.22285 0.35195 -0.15422 -0.13137 -0.033746 -0.16894 0.14939 -0.85071 0.8303 -0.22077 0.32756 -0.46347 0.15704 0.15775 -0.075318 -0.024818 -0.045062 0.076934 0.048395 -0.72626 0.46597 -0.26966 -0.58763 0.23509 0.41876 0.38672 0.38338 0.32791 0.1036 0.31148 0.36017 -0.17754 -0.12346 -0.28095 -0.12792 -0.4066 0.14674 0.66217 -0.48934 0.33352 -0.08094 -0.081471 -0.56518 0.60675 -0.071591 0.55095 0.37616 -0.11914 -0.22842 -0.72311 -0.30394 0.06935 0.11861 0.18844 0.40996 -0.30491 -0.073846 -0.24488 -0.85284 -0.0016392 0.058739 -0.11254 0.21054 -0.12957 0.60001 -0.57975 0.098064 0.25146 0.38879 -0.59721 0.74942 0.13982 -0.64333 0.8068 -0.8529 -0.097729 0.27562 0.024347 0.32667 0.03216 0.28938 0.042973 0.66781 -0.62474 0.050046 0.11425 -0.1425 0.096074 0.45494 -0.089113 0.52999 0.30064 0.51882 0.062207 0.4262 0.30348 0.58217 0.4461 -0.083093 0.23491 0.32839 -0.20729 -0.18414 0.25996 0.75131 0.1707 -0.22728 -0.02473 0.44001 -0.67008 -0.1713 0.045325 -0.19009 -0.053806 0.49594 -0.15684 -0.25893 -0.29879 0.68854 0.052915 -0.55306 -0.12448 0.16334 0.39501 0.45141 -0.051286 -0.23803 0.31448 0.098186 0.5036 0.22789 -0.22292 -0.30905 -0.27327 0.4907 -0.31093 0.034955 0.53066 0.8932 0.19769 0.02168 -0.044989 0.80471 0.13248 -0.004733 0.53816 0.0021613 -0.34276 -0.17542 0.6035 0.51525 -0.2402 0.31781 0.028639 -0.4713 -0.029991 -0.58394 0.31659 0.075969 0.37688 0.47865 -0.62708 0.47169 0.74507 0.15765 -0.17955 0.095543 0.43644 0.056638 -0.46781 0.3761 0.21661 -0.18378 0.34377 0.40629 0.47489 1.0477 0.11735 0.22259 0.38282 0.098808 -0.088356 0.089153 0.099398 -0.090611 -0.54011 0.15283 0.06929 -0.27999 -0.076277 0.18012 -0.53787 -0.34725 0.71109 -0.28219 0.20893 0.099495 0.59917 0.48383 -0.13997 0.5227 -0.19693 -0.24871 -0.63357 -0.09565 -0.69101 -0.25347 -0.85556 -0.15462 0.0071559 0.038633 0.67575 -0.20821 1.0249 -0.50038 0.012226 0.57338 -0.32834 0.78573 -0.087241 -0.7386 -0.24992 0.43287 0.24101 0.73668 0.64582 0.26793 -0.2846 -0.34104 -0.0062504 -0.64824 0.58085 -0.20472 -0.41832 -0.023617 -0.44203 -0.017908 0.14106 -0.33705 -0.65206 0.45318 -0.53319 0.57341 0.5229 0.024291 -0.59827 -0.026417 0.10453 0.12568 0.80541 0.77114 0.069694 -0.62583 0.3634 0.11365 0.076932 0.21719 -0.50624 0.1619 0.66214 0.12204 -0.20929 -0.030636 0.64906 0.1983 0.12721 -0.67107 0.6206 0.71175 -0.45709 -0.35856 -0.51486 0.20406 0.16845 -0.24387 0.024908 0.094458 0.18698\n",
      "\n",
      "Skipping line: on name@domain.com 0.037295 -0.15381 -0.045189 1.0566 0.42898 0.24093 0.34305 -0.090393 -0.79877 -1.2107 0.31958 0.46744 -0.20072 -0.61936 0.69963 0.70189 0.58516 -1.6244 0.45742 -0.10967 -0.068128 0.11185 0.1758 0.36282 0.207 0.085671 -0.024428 0.064891 0.27972 -0.62379 0.11502 -0.48324 0.32067 0.064549 0.13098 0.42373 -0.2884 0.062554 0.28625 -0.40777 -0.15946 -0.3914 -0.18405 -0.69952 -0.41679 -0.27461 0.5992 0.031153 0.57437 0.19658 0.3127 -0.078999 0.03098 -0.53815 0.091315 -0.38143 -0.75908 -0.12581 -0.20777 0.24508 -0.51285 0.020266 0.16943 -0.21203 -0.14944 0.46839 -0.40152 0.24182 0.059784 -0.10328 -0.23935 -0.15119 -0.052004 -0.078992 -0.35178 -0.21634 0.59896 -0.47535 -0.11839 -0.21983 -0.61098 -0.31832 -0.55874 0.052747 0.039198 -0.56885 1.0475 -0.22668 -0.53216 -0.12656 0.29789 -0.12112 0.59625 0.076639 -1.0092 0.66201 -0.51496 -0.076104 0.082936 -0.0094538 0.63184 0.20894 0.15745 0.074381 0.31779 -0.68213 -0.18466 -0.074668 -0.48459 0.06025 0.43421 -0.26723 0.7676 0.5288 0.27123 -0.82053 0.26875 0.25783 0.4725 0.94076 -0.27055 0.11796 0.16388 0.16271 0.71576 -0.84291 0.27383 0.12842 -0.13808 -0.57815 -0.16535 -0.45732 -0.59394 -0.063703 0.1311 0.33161 0.27681 -0.049656 0.035738 -0.29478 1.111 0.086364 -0.63429 -0.28759 0.16863 0.088104 0.33693 -0.24049 -0.40589 0.63117 0.2691 -0.0059601 0.077498 0.0048893 -0.42893 0.19667 0.18104 -0.033527 0.41909 0.56974 0.22464 0.44984 -0.3836 -0.056989 0.70036 -0.062551 -0.076638 0.41616 -0.43017 0.051679 -0.45284 0.59514 0.74749 -0.49283 0.30089 0.020652 -0.28359 -0.15402 -0.60682 -0.044729 0.27258 0.17982 0.69056 0.26935 0.88267 -0.25968 0.018912 -0.06569 0.2459 0.57953 0.38363 -0.38261 0.89573 -0.030606 -0.19293 1.0351 0.47315 -0.029376 0.29365 0.53393 0.23375 0.46295 -0.3838 0.33758 0.40473 0.5511 0.0051471 0.098574 -0.0016179 0.055963 -0.26139 0.075229 -0.16292 0.24037 -0.07256 0.22245 -0.0040634 0.12477 0.16262 0.61958 0.10218 -0.30888 0.41935 0.23097 -0.78383 -0.42864 -0.43192 -0.52814 0.13438 -0.83851 -0.41554 -0.24818 -0.20883 0.75631 -0.25623 0.32551 -0.6399 0.26467 0.18173 -0.32961 0.028943 0.44702 -0.31151 0.34936 0.61233 -0.34992 0.68184 0.64509 -0.098248 -0.45894 0.20604 -0.334 -0.40975 0.17371 -0.15081 -0.93562 0.091008 -0.41959 0.026537 -0.37542 -0.39248 -0.01727 0.1491 -0.42839 1.2822 0.86974 0.4999 -0.09103 -0.44413 0.15755 0.34085 0.51527 0.41727 0.2675 -0.37372 0.35689 0.241 -0.65587 -0.11725 -0.6031 0.1791 0.32321 0.27526 -0.18872 0.12421 0.077509 0.6748 -0.053329 0.071209 0.47614 0.35372 -0.26264 0.067594 -0.063179 0.2563 0.059929 0.30376 0.16772 -0.067342 0.30749\n",
      "\n",
      "Skipping line: At Killerseats.com -0.13854 -0.01706 -0.13651 0.1237 0.15633 -0.16556 0.29374 -0.064174 0.3209 -0.091516 -0.017139 -0.061828 -0.17736 -0.037355 0.0015256 -0.13485 -0.1714 -0.040029 0.12924 0.06956 0.02036 -0.011202 0.0039595 -0.1076 -0.014068 0.091797 0.0064314 -0.14604 0.0039716 -0.045866 -0.16544 -0.13641 0.099001 0.10089 0.018211 0.045112 0.18615 -0.018217 -0.085986 -0.13309 -0.0047582 -0.076491 0.23942 0.061977 -0.11535 -0.02209 0.1325 -0.0012518 0.10991 -0.18539 -0.12344 -0.06419 0.0020279 0.08325 0.23703 -0.20077 0.0068709 -0.24464 -0.019755 -0.11095 0.0046046 -0.20673 -0.049618 -0.067157 0.02305 0.09159 -0.11349 0.030432 -0.034791 0.1984 -0.056183 -0.10847 0.0062699 0.047591 0.0012842 0.039199 -0.086015 0.12928 0.27708 -0.11308 -0.030623 0.10101 -0.26294 0.021336 -0.030286 0.094648 -0.26473 0.20893 0.011739 -0.27749 -0.26562 0.015384 -0.066983 0.091436 -0.01803 -0.10952 0.033702 0.19421 0.018353 0.067715 -0.14521 0.076887 0.20074 0.049053 0.031508 0.18307 0.12594 -0.012344 -0.00075399 0.20056 0.18769 -0.13718 0.23926 0.15341 0.068396 0.10398 0.12115 0.049698 -0.0432 0.11788 0.063636 -0.051266 0.00095202 -0.063047 -0.11646 -0.0499 -0.010252 0.037608 0.027558 -0.047844 0.18169 -0.026459 -0.08777 0.0095411 0.17127 -0.10256 -0.17665 -0.0016558 -0.29702 -0.14523 -0.060911 -0.12564 0.27857 0.016822 0.051903 -0.21652 -0.15691 0.033816 -0.045045 -0.16395 0.12095 -0.036018 -0.12004 0.16239 0.17236 0.0043345 -0.09299 0.12625 -0.16393 -0.16347 0.13903 -0.0024316 -0.059626 0.094548 -0.16164 -0.025202 0.09998 -0.2755 -0.028732 -0.073529 -0.25114 -0.057089 -0.14148 0.011597 -0.25583 -0.040875 -0.0020845 0.24255 0.094112 0.28931 0.20464 0.15538 -0.27549 0.046415 -0.018823 0.036006 -0.1589 -0.1442 -0.11284 0.19729 -0.11681 -0.0027812 0.11 -0.069434 0.16456 -0.19392 0.03563 -4.8304e-06 0.21579 0.096814 0.17293 -0.30057 0.087337 -0.088238 -0.13899 0.16369 -0.061456 0.10775 0.23821 0.08518 0.076787 -0.091381 -0.12392 -0.12457 0.20588 -0.21204 0.064002 0.22108 -0.34458 -0.029845 -0.086207 -0.070035 0.015213 0.0044742 0.054369 0.16562 -0.071822 -0.0196 -0.18231 0.0070107 0.10313 -0.057425 0.09921 -0.17666 0.30342 -0.09457 0.087363 -0.10153 -0.15785 0.074526 -0.055874 -0.24675 -0.059254 0.093743 0.10697 0.11725 0.010904 -0.070051 -0.094899 0.12954 -0.037657 -0.11976 -0.25947 0.10308 0.12485 -0.06518 -0.018488 -0.045376 0.12019 -0.020251 -0.046336 0.056461 -0.041232 -0.0052311 0.02632 0.0015934 0.11822 0.20279 0.011894 0.32837 -0.025418 0.050482 -0.37133 -0.08665 -0.0078504 -0.031485 -0.22887 0.046758 0.10404 -0.086826 -0.31606 0.14347 0.081695 0.0028145 0.13973 0.10326 0.064327 0.06976 -0.03511 0.038678 -0.11595 0.26615 -0.22468 0.26911 0.019649 -0.13941 0.024992 -0.2906 0.32547 0.12586\n",
      "\n",
      "Skipping line: by name@domain.com 0.6882 -0.36436 0.62079 1.1482 -0.055475 -0.37936 0.0064471 -0.33046 -0.43406 -1.3468 0.70312 -0.41314 -0.65868 0.64324 0.13018 0.65846 0.86269 -0.93108 0.3476 0.73912 -0.51405 -0.15113 0.27331 0.51396 -0.74688 0.87989 -0.11887 0.3641 0.37838 0.36177 -0.45182 0.16173 -0.36353 -0.55643 -1.1186 0.70117 -0.48075 0.074095 0.43022 0.4625 0.011133 0.030287 -0.73342 -0.772 0.31058 0.022106 -0.16845 -0.70695 -0.16243 -0.15454 -0.12034 0.018702 0.51626 -0.17255 0.37335 -0.059377 0.013126 -0.30727 0.1581 0.74527 -0.7927 -0.34603 -0.01438 -1.055 -0.95074 -0.81794 0.27925 -0.35405 -0.26783 -0.30391 0.16093 -0.064806 0.69283 -1.1955 0.18414 -0.71183 0.062622 -0.62435 -0.16458 -0.74362 -0.19251 -0.1841 0.99035 -0.20552 -0.46621 0.98506 1.4113 0.024391 -0.14285 0.40063 0.10516 0.065123 -0.4613 0.27429 0.022191 0.55307 0.18442 -0.22378 -0.50433 0.046039 0.12306 -0.11203 -0.30851 -0.13275 -0.36831 -0.63785 -0.99149 -0.55833 0.17128 0.27324 -0.37803 0.4641 0.39427 0.048909 0.46002 0.072136 0.090133 -0.77511 0.31636 1.0484 0.65455 -0.54555 -0.16023 0.51455 0.46977 -0.70197 -0.032668 -0.23516 -0.053423 0.6293 -0.35302 -0.44905 -0.44235 -0.14551 -0.60106 -0.68264 0.71618 0.44723 0.17647 0.60878 1.0692 -1.0488 -0.48178 0.10825 -0.27708 0.50754 0.69316 -0.40861 0.98707 0.045813 0.6237 -0.041425 0.23719 0.50664 -0.14663 0.55561 0.60422 0.089302 -0.18124 0.51483 0.51676 -0.25888 0.64182 0.22324 0.54462 0.39816 -0.97695 0.17496 -0.35361 -0.77967 0.23945 0.21711 0.26793 -0.029782 0.75712 0.083235 0.38329 0.14695 -0.42997 -0.12402 0.078012 0.69183 0.36966 0.75105 0.91858 -0.11291 0.53013 0.35894 0.65682 -0.30278 0.074167 0.064695 0.08892 -0.04165 -0.5437 1.2794 0.21133 0.47272 0.24959 -0.77101 -0.20456 0.54224 0.44168 0.11797 -0.28053 -0.30537 0.39505 -0.46693 0.54209 0.58962 0.17908 0.15278 0.51678 1.0729 -0.21624 -0.63379 0.6209 -0.029061 0.45802 -0.70106 -0.096258 -0.13379 0.99638 0.62168 0.045304 -0.85691 0.15816 -0.77156 0.45802 -0.48487 0.10298 0.12015 -0.61097 -0.51157 -0.49761 0.34814 0.20141 0.33286 1.2239 -0.42395 -0.33754 -0.51882 -0.66346 -0.32048 0.25638 0.06427 0.22788 -0.32311 0.11933 -0.47903 0.049264 -0.35126 -0.79376 -0.073158 0.25249 -0.92067 -0.2175 0.4002 -0.17607 -0.49748 0.028808 -0.66234 -0.15404 -0.51768 0.023225 0.46048 0.19184 0.17417 -0.32702 0.0023848 0.36328 0.40963 0.2658 -0.016626 -0.85864 -0.77511 0.71664 -0.8535 -0.53959 -0.29513 -0.11449 0.25627 0.3023 -0.32311 0.58707 0.61075 -0.68101 -0.26994 -0.50763 0.23818 1.1097 -0.5269 -0.036155 -0.20817 0.2446 -1.0573 -0.11534 0.08409 -0.47809 -0.32664\n",
      "\n",
      "Skipping line: in mylot.com -0.18148 0.47096 0.32916 0.044196 -0.93045 -0.16299 0.31996 0.39017 0.013753 -1.0117 1.4497 0.14411 0.09409 -0.034215 -0.11889 -0.24614 0.88631 -0.62527 0.1653 0.17695 -0.092857 -0.82453 -0.3729 -0.32292 -0.048221 -0.42264 0.11826 0.17933 -0.058891 0.16458 0.09932 0.4913 0.051474 -0.059584 -0.086531 0.060116 -0.24148 0.34348 0.011429 0.046588 0.33665 -0.63822 0.036459 0.65724 0.17596 -0.33444 -0.17779 -0.047878 0.29427 0.10278 0.31098 0.62907 -0.098816 -0.5123 -0.43283 0.47748 -0.13249 -1.0708 0.081945 -0.04147 -0.23569 0.3655 -0.30838 -0.76359 -0.36836 -0.22545 -1.1477 -0.12543 0.087085 0.10207 -0.24626 -0.41621 0.20584 -0.034737 -0.79378 -0.32414 0.32755 0.66629 0.058202 0.13797 0.39361 -0.41695 -0.40613 -0.076039 -0.11123 -0.17525 -0.0088467 -0.044072 -0.69197 -0.74632 0.39905 -0.047611 0.19114 0.83765 0.42194 0.4051 0.18833 0.76741 -0.50475 -0.38181 0.054343 -0.22186 -0.18301 -0.1434 -0.66303 0.54454 -0.19581 -0.37495 0.37518 -0.14736 0.12769 0.066166 -0.66023 -0.13895 0.47418 0.44716 0.14277 -1.1752 0.17875 -0.26377 -0.71254 -0.056002 1.1058 0.48956 -0.0034874 -1.2341 -0.25508 0.44549 -0.47797 0.38046 0.062388 -0.30549 0.018524 -0.73576 0.11699 -0.48136 0.68783 0.34749 0.33697 0.25123 1.6869 0.22458 -0.25622 0.29453 -0.67176 0.77372 0.17065 -0.90475 0.043384 -0.1352 -0.035265 -0.70517 -0.45938 -0.068112 -0.51148 0.47336 -0.10527 -0.4393 -0.38211 -0.27846 0.19689 0.075033 0.32149 -0.24502 0.054691 0.11946 -0.26407 -0.0074057 -0.66552 -0.20797 -0.043512 0.53229 -0.28674 0.74577 0.27161 -0.18583 0.020504 0.032247 0.093833 0.16584 0.0064513 -0.11134 -0.6277 0.66321 1.0995 0.17221 0.39757 0.22174 0.37047 -0.11836 0.046102 -0.21112 0.41396 -0.0033789 -0.41364 0.41285 0.06004 0.067526 -0.21855 -0.48179 0.16688 0.59051 0.21349 0.41751 0.065228 -0.83219 0.28468 0.027128 0.56385 0.5687 0.12164 0.26231 0.10388 -0.051454 0.16501 -0.92392 -0.10778 -0.31637 0.28046 0.080897 -0.020649 0.56557 0.56447 0.48398 -0.85527 -0.17136 0.038894 -0.28895 -0.19452 -0.17616 0.39584 0.43994 0.36613 0.012852 -0.13704 0.11059 -0.39179 -0.03542 0.084348 -0.41242 -0.59482 -0.51298 0.104 -0.083639 -0.84864 0.34791 -0.22998 0.41067 0.20366 -0.56011 -1.0281 0.66185 0.11875 1.1039 0.15888 -0.18237 -0.50039 -0.058217 -0.60943 0.35013 -0.62093 0.098393 0.50862 -0.10342 -0.14692 0.014277 0.63924 0.14633 -0.2731 0.069363 0.78212 0.17801 -0.81651 0.087242 -0.83718 0.037194 0.29325 -0.062079 0.038745 0.12474 -0.40302 0.11265 -0.82604 -0.12445 0.016351 -0.41329 -0.58576 -0.44029 -0.0035195 0.26308 0.071372 0.43961 -0.34098 -0.27518 0.20454 0.32802 -0.39246 0.11756 0.1804 -0.17405\n",
      "\n",
      "Skipping line: emailing name@domain.com 0.39173 -0.39132 -0.4266 0.82429 0.42919 0.17601 0.16663 -0.011601 -0.53551 -1.255 0.35066 0.06361 -0.70235 -0.31163 0.25379 0.2463 0.21129 -1.5845 0.19661 -0.01602 -0.54956 -0.58445 0.03272 0.90671 0.33191 -0.52747 0.32609 0.53668 -0.18302 -0.49699 0.069751 -0.32102 0.034384 0.30422 -0.37326 0.73485 -0.36973 0.89839 0.30623 0.18463 -0.23602 -0.17 0.34307 0.026764 -0.1685 0.4373 0.43021 -0.17562 0.15805 0.19768 0.064745 0.048354 -0.17168 -0.24719 -0.2572 -0.59113 0.1086 -0.20647 0.19999 0.25039 -0.50867 0.013532 -0.1393 0.20503 -0.94526 0.76842 -0.25137 0.062295 -0.0779 -0.43982 0.17724 -0.5318 0.15458 -0.26637 -0.34106 -0.26619 0.39768 -0.0027818 0.17052 -0.29347 -1.3207 -0.43398 -0.22671 0.026272 0.12023 0.12684 0.92681 -0.62325 -0.086858 -0.17383 0.38999 0.31858 -0.32013 0.32169 -1.4042 0.774 -1.0086 0.076515 0.22546 -0.34501 0.64108 -0.62987 -0.33897 0.69475 0.50582 -0.79349 -0.41876 0.66703 -0.081664 -0.079977 -0.099099 -0.41525 0.9304 -0.13361 0.17219 -0.32933 -0.29329 -0.1672 0.12365 0.75392 0.49888 0.51591 0.5768 0.19417 0.49986 -0.60501 0.5596 0.16308 0.23527 0.34854 -0.4135 -0.4168 -0.34117 0.41811 -0.22984 0.045453 1.021 0.15702 -0.16201 0.34939 0.74524 0.64977 -1.0165 -0.24076 0.024844 0.44116 0.79557 -0.72798 -0.046033 0.28843 0.41386 0.38713 -0.29446 -0.1907 -0.67133 0.57932 -0.1499 -0.049878 -0.2862 1.0487 0.38315 0.5175 0.17042 -1.03 0.6565 0.075288 0.21532 0.2055 -0.10369 -0.24518 -0.79469 0.54382 0.56461 0.12539 0.23951 -0.17072 0.05467 -0.31934 -0.699 0.40972 0.00091743 0.32084 0.050919 -0.19141 0.90119 0.39184 0.97954 0.036333 0.66042 0.94628 0.74539 -0.36498 1.0807 -0.050241 0.16434 -0.014925 0.9391 0.51309 0.4418 0.42688 0.37485 0.021399 0.3364 0.061034 0.87373 0.20534 0.51232 0.44053 0.35507 0.17071 -0.46624 0.21338 -0.28831 0.14749 0.50913 0.52774 0.3225 0.18207 0.25947 0.53903 0.78083 -0.47849 -0.37548 -0.2368 -0.26527 -0.31599 -0.72522 -0.2193 0.14468 -0.51742 -0.29822 0.52975 0.19253 0.02847 -0.12754 0.31759 0.073123 0.23208 -0.32065 -0.64801 -0.49646 0.34046 -0.16553 -0.60268 0.44127 -0.53358 0.53215 0.20003 0.36597 -0.064675 0.06453 -0.18848 -0.58001 -0.13561 0.12283 -0.53895 0.12008 -0.52819 -0.15895 0.23558 -0.32919 0.15223 -0.15 -0.8083 0.25604 0.81459 0.18576 0.048995 0.48536 0.012105 0.43842 0.26448 0.52854 0.16257 -0.20932 0.089357 -0.014704 -0.15414 0.13277 -0.43273 -0.43306 0.0024893 0.15383 -0.27752 0.67561 0.68944 1.0019 0.52244 0.14167 0.51255 0.84324 0.19646 -0.055988 -0.65196 -0.023645 0.26897 -0.13204 0.34183 -1.1074 -0.21701\n",
      "\n",
      "Skipping line: Contact name@domain.com 0.14933 -0.28605 0.3444 0.29015 -0.22999 0.1271 0.35722 0.35118 0.28224 -1.3679 0.44651 -0.017463 -0.12998 0.39657 0.71315 0.11213 0.70139 -1.5873 0.7609 0.072771 0.66317 -0.33039 0.31256 -0.020889 -0.47135 0.3383 0.53767 0.057899 -0.023796 -0.31841 0.23784 -0.28834 0.17782 -0.5244 0.29458 0.44297 0.53541 -0.10591 1.0599 0.18864 0.55033 -0.50063 0.022688 -0.17678 -0.70947 -0.8091 0.27532 -0.048975 0.32952 -0.26311 0.54794 0.50121 0.61235 0.039506 -0.39968 -0.84155 -0.52122 -0.6594 -0.43807 0.54237 -0.79301 0.034652 -0.12363 -0.35739 -0.46396 0.92434 0.29489 0.26151 -0.25829 -0.55722 -0.14739 -0.21945 0.46516 -0.44058 0.41805 -0.011851 0.5441 -0.21327 0.043397 -0.10785 -1.3427 -0.37442 -0.40596 0.044713 0.071265 0.22554 1.0166 -0.18678 -0.080248 -0.070555 0.38612 -0.034025 0.33291 0.16814 -0.33787 0.48429 -0.9275 -0.37455 0.28879 0.011307 -0.034424 -0.074561 0.33677 0.086636 0.58377 -0.97006 0.21602 -0.10354 -0.18782 -0.18874 -0.0075329 0.5045 0.41263 0.63932 0.46135 0.043514 0.037813 0.48608 0.45065 0.094473 -0.51254 -0.2553 0.60168 -0.22751 0.14411 -0.015375 0.26335 0.12941 0.55472 -0.35128 0.14395 -0.37141 -0.47784 0.54191 -0.30975 -0.12484 -0.10024 -0.080352 -0.79665 -0.65575 0.99039 0.092417 -1.0277 0.10834 -0.24512 -0.16528 0.034586 0.35529 -0.27859 0.28722 0.30695 0.31876 -0.4585 0.35226 -0.03028 0.047087 1.488 -0.11154 0.40292 0.092156 0.89874 0.30825 0.068088 0.072786 0.77015 0.43306 -0.089741 0.34737 -0.60596 -0.10934 0.45442 0.38238 0.90917 0.40277 0.037961 -0.71513 -0.28856 -0.21618 0.098854 0.65154 0.36816 0.12634 -0.07398 -0.81584 0.79696 0.65227 0.62773 -0.19997 0.58956 1.2545 0.16172 -0.26292 0.48653 0.12693 -0.20123 0.12498 0.43937 0.64485 0.5004 -0.11575 -0.010742 -0.094577 0.52967 0.14039 0.30212 0.58504 -0.074609 -0.35289 0.032048 0.23581 -0.47016 -0.041431 0.15496 -0.25254 -0.18551 0.34496 -0.016185 0.1799 0.76838 0.5616 -0.0043105 -0.083277 0.86002 -0.2186 -0.78777 -0.31371 -0.408 -0.30427 -0.48196 -0.69419 0.00093017 0.20153 0.46771 0.055064 0.16938 0.028213 -1.1183 0.22422 0.32095 -0.21798 0.78578 -0.4124 -0.90433 -0.23226 0.80892 0.36045 0.34569 0.7428 0.57411 0.013348 -0.24344 -0.5165 -0.12468 0.21977 -0.46848 -0.44024 0.57263 -0.39965 0.00536 0.15398 -0.57058 -0.12496 0.17647 -0.36044 -0.077987 0.2501 -0.18364 -0.106 -0.079021 -0.11581 -0.026927 0.7054 0.39769 0.30528 -0.12727 0.49794 0.52739 -0.037058 -0.61461 -0.74724 0.57124 0.0069423 -0.022656 -0.86076 0.33951 0.8744 -0.00072818 0.31948 -0.47734 0.29885 0.72535 -0.79345 -0.031487 -0.58767 0.3152 0.0075835 -0.27275 -0.017409 0.43731 -0.05677\n",
      "\n",
      "Skipping line: at  name@domain.com 0.44321 -0.40005 -0.20065 1.1209 0.34041 0.086082 -0.067128 0.0022702 -0.94649 -1.4669 0.61248 0.34827 -0.20983 -0.61434 0.41102 0.57759 0.69071 -1.9301 0.75265 -0.13238 0.22003 0.28856 0.35234 0.45989 -0.21944 0.1931 -0.11664 0.14996 0.70354 -0.039238 0.55298 -0.53503 -0.3221 -0.28595 -0.1246 0.054544 -0.45937 0.1447 0.8203 -0.33182 0.10864 -0.56552 0.39898 -0.65012 -0.20285 0.11557 0.35711 -0.23958 -0.30281 0.51593 0.71883 -0.30403 0.59458 -0.3217 -0.23967 -0.2576 -0.50224 -0.36055 -0.71763 0.4981 -0.69945 -0.0072578 0.37327 -0.029839 -0.42705 0.93128 -0.046928 0.045162 -0.44879 0.16579 -0.26272 -0.35286 0.17395 -0.24436 -0.1439 -0.39857 0.25342 -0.44737 0.37618 -0.80252 -0.87776 -0.19282 -0.48746 0.065159 -0.24349 -0.77669 0.81629 -0.043888 -0.68276 -0.15709 -0.46533 -0.066009 0.063028 0.090332 -0.81297 0.88979 -0.6391 0.17351 0.3328 -0.30808 0.46158 -0.11289 -0.0261 -0.089243 0.37318 -0.73511 0.19798 -0.060219 -0.12113 -0.2146 0.62061 0.34296 0.89595 0.22495 0.16079 -0.84005 -0.6749 0.4053 0.17549 0.70242 -0.22907 0.068957 0.092359 0.034122 0.24296 -0.47643 0.27613 0.04291 -0.055271 0.35486 0.32692 -0.66761 -0.77199 -0.075454 -0.36238 0.052461 0.34678 0.070744 -0.5845 0.14793 0.81643 0.45906 -0.80253 0.18628 -0.039264 0.21197 0.45927 -0.274 -0.81792 0.73455 0.30696 0.10415 0.035539 0.012445 0.088135 0.67792 0.45663 0.2726 -0.50234 0.62198 0.38648 0.01011 -0.32851 -0.068547 0.85187 0.14533 0.28169 0.757 -0.61313 0.46784 -0.53806 0.22123 0.59195 -0.52707 0.342 -0.32091 -0.0031903 -0.063289 -0.59452 0.22196 -0.2044 0.26864 0.14327 0.077514 0.75915 0.17929 0.27785 -0.24118 0.57407 0.94331 0.61097 -0.36571 0.37771 0.30822 -0.42249 1.1221 0.67737 0.29251 0.42105 0.1204 -0.19767 0.70793 -0.19204 0.27399 0.21617 0.58442 0.020547 -0.004382 0.2435 -0.30135 -0.32904 0.2376 -0.42115 -0.2809 -0.12963 0.23212 0.28883 0.047856 0.019705 0.57196 0.56619 0.012819 0.24007 0.22556 -0.47738 -0.15644 -0.64105 -0.57604 -0.36252 -0.20212 -0.3813 0.27112 0.37976 0.20358 -0.28793 0.36938 -0.42238 0.14111 -0.19996 -0.34411 0.10342 -0.49926 -0.32485 0.42403 0.35958 -0.35653 0.27493 0.43134 0.45197 -0.17992 0.36502 -0.1422 -0.20199 -0.35023 0.16243 -1.1251 0.044546 -0.59278 0.03059 -0.44425 -0.82482 -0.21953 -0.39308 -0.84476 0.20993 0.99719 0.1983 -0.28091 -0.36525 0.52296 0.65851 0.46585 0.52257 0.14744 -0.74425 -0.12268 0.81749 -0.2461 0.018109 -1.0154 0.29857 0.32427 0.57664 -0.41371 -0.099047 0.24261 0.12247 0.31508 -0.076621 0.6407 0.25333 -0.2548 0.022574 -0.037949 0.0059522 0.068205 -0.11948 -0.44629 -0.45647 -0.15084\n",
      "\n",
      "Skipping line: • name@domain.com -0.13288 -0.31383 -0.032356 0.52036 -0.26985 0.43339 0.32587 -0.51581 -0.9806 -1.5879 0.73555 0.35252 -0.73296 0.6446 0.37158 -0.26666 0.21589 -0.91457 0.18992 0.81333 0.8067 0.59507 -0.5518 0.48099 0.12726 0.03783 0.24796 0.50997 0.075809 -0.75845 0.32875 -0.29168 0.057426 -0.16102 -0.68347 0.86165 0.2599 -0.56354 -0.040419 -0.28022 -0.20049 0.0064199 -0.45262 -0.10725 0.31171 0.35856 0.032082 1.203 -0.3693 -0.027401 1.1 -0.20264 0.18898 -0.45642 0.048134 0.5478 -0.14036 0.073562 1.0508 -0.12821 0.33468 0.2014 0.028187 -0.37055 0.48804 0.042724 0.030665 -0.025691 0.10987 -0.31731 -1.3087 -0.24197 -0.16837 -0.69608 0.086061 -0.17898 0.46507 0.33104 0.55842 -0.60105 -0.063918 -0.24297 0.48818 0.14094 -0.59654 0.79936 0.58047 0.24874 -0.98018 0.24882 0.25235 -0.72269 -0.2084 0.12267 0.011462 0.16495 -0.33902 -0.52693 -0.42821 0.42544 1.0436 0.31029 -0.021538 -1.0413 -0.80963 -0.098143 -0.1954 0.36169 -1.1466 -0.44647 0.35587 0.76049 0.50332 -0.52995 0.84432 0.77632 -0.18343 -0.89036 0.14031 -0.14793 -0.55824 0.16072 -0.15132 0.35349 -0.0015194 -0.1344 0.75585 1.4173 0.34559 0.84169 0.23758 -0.55682 -0.4233 0.34256 0.21639 0.23196 -0.058196 -0.34022 0.30834 0.085223 1.0005 -0.75103 -0.19934 1.1943 -0.75858 0.47305 0.2842 -0.23272 -0.31152 0.016731 0.26253 -0.11474 -0.97903 -0.0092514 0.068853 0.98873 -0.24826 0.081491 0.48154 0.83283 0.70266 0.42554 0.57001 0.98106 0.18615 0.48742 0.57736 0.55659 -0.68907 0.33097 -1.2181 -0.01721 -0.80811 -0.41657 -0.03589 0.019744 0.38763 0.75493 0.9686 -0.81071 0.28657 -0.41282 0.46566 -0.3717 1.0972 0.36665 0.90244 -0.82436 0.56426 -0.35644 0.36488 -0.47052 0.98485 -0.68457 -0.84468 -0.34603 0.71688 0.63703 -0.49426 0.11989 0.018898 1.1553 0.026494 -0.38328 -0.83349 -0.99136 -0.092262 0.39492 0.3145 -0.55455 0.12677 0.18484 0.070504 -0.16828 -0.13925 0.21572 0.29662 -0.24226 -0.19738 0.36044 -0.57074 0.2877 0.539 -0.62249 -0.61904 -0.030457 0.52519 -0.29111 -0.037765 -0.016212 0.29459 0.21433 -0.39144 -0.76351 -0.33047 0.32052 -0.066346 0.29668 0.33397 -1.3425 -0.16393 -0.15417 -0.1092 -0.48709 -0.28508 0.83707 -0.44983 0.99084 0.70429 -0.30875 -1.1595 -0.25862 0.099996 -0.20872 -0.31094 -1.0475 -0.36245 0.39012 -0.3529 0.70455 -0.33983 -0.74342 -0.12024 -0.8151 -0.40635 -0.52105 0.61012 -0.25333 -0.9383 -0.048584 -0.14662 0.48444 0.84921 1.1434 -0.34354 0.61107 -1.0268 0.41495 0.29176 -1.3315 -0.10941 0.41862 -0.14078 -0.41347 0.71217 0.5683 -0.72551 0.29458 -0.88888 0.2785 0.8716 0.42549 -1.2827 0.60796 0.10976 -0.033875 0.056314 0.86177 0.080508 0.17186\n",
      "\n",
      "Skipping line: at Amazon.com -0.5275 -0.73685 0.10968 0.22214 -0.30063 -0.63201 -0.053204 -0.16241 -0.33811 -1.4072 1.1837 0.45468 -0.84209 -0.011505 0.896 0.03319 0.26241 -1.2644 0.57994 -0.23699 -0.49705 0.16729 -0.18469 0.12163 0.23774 -0.0997 -0.3207 0.56191 -0.017743 -0.32547 0.014204 -0.57266 0.42053 -0.020948 -0.29288 -0.086615 0.46141 0.28811 0.30401 -0.10123 -0.90849 0.36862 0.40785 -0.80729 0.23232 -0.14189 0.3161 0.049262 -0.33411 -0.25296 -0.019395 -0.24234 -0.59846 0.76997 0.87265 -0.24425 0.51755 -0.20878 -0.56027 -0.21439 -0.42029 0.13491 -0.028368 -0.56969 -0.1266 0.38287 -0.16767 -0.58677 -0.32237 0.15846 0.59556 -1.0552 0.79441 0.17987 -0.62691 0.38177 -0.066748 0.16996 -0.10173 0.066299 -0.55561 -0.38713 0.33772 0.33874 0.053886 -0.10488 0.56415 -0.13275 0.1262 0.29306 -0.31095 -0.17854 0.35285 0.00013075 0.51898 1.2236 -0.2643 0.037547 0.51511 0.22394 -0.25357 -1.0723 0.49631 -0.27033 0.11704 -0.67869 -0.22536 0.13763 0.1407 0.44025 0.043805 -0.11425 -0.17916 -0.10361 0.14973 0.81742 -0.0013219 -0.14155 0.19542 0.62475 -0.58424 0.31022 0.66416 -0.33286 -0.16716 0.072762 -0.49764 -0.10817 0.22071 0.14323 -0.23053 -1.1992 -0.28862 0.21024 -0.62443 0.076061 0.18231 0.087195 -0.34459 0.02946 0.67898 -0.30063 -0.174 -0.89263 -0.042857 -0.30505 0.66966 -0.013918 0.29442 0.059769 0.24655 -0.29502 -0.024101 -0.43562 -0.042243 0.48316 -0.24061 -0.55936 -0.25671 -0.16496 0.41056 -0.011025 0.3995 -0.10955 0.57864 -0.095326 -0.14995 0.10487 -0.50071 -0.022227 -0.3401 -0.12177 0.21538 0.44572 0.73496 -0.1897 0.0044095 -0.84324 -1.1227 -0.13945 -0.18635 0.047539 -0.02576 0.94764 0.22948 -0.15774 -0.25206 -0.38703 0.6866 0.20996 -0.15875 0.0058876 0.5742 -0.67418 -0.16063 -0.075739 0.42837 0.20913 0.16235 -0.65112 0.33446 0.1024 0.2308 0.26889 -0.53514 0.51401 -0.077549 0.23051 0.27226 -0.096375 0.0030776 0.58233 -0.46236 -0.25057 -0.074138 -0.32048 0.62223 0.54458 0.6217 1.1464 0.50221 -0.90171 0.31349 -1.1352 -0.19198 -0.31481 -0.12779 -0.29484 -0.22873 -0.31661 0.53138 0.24974 0.070768 -0.65404 -0.37577 0.61155 0.49496 -0.052352 0.53938 0.005889 0.21959 0.10324 -1.1018 -0.14499 0.25521 0.24265 0.31371 0.33447 -0.37324 -0.52313 0.20971 -0.13097 -0.16648 -0.049781 0.2748 -0.92401 0.75744 0.48051 -0.44793 -0.42594 0.34599 -0.53192 -0.13063 0.096985 -0.042562 0.52247 0.74385 -0.68267 -0.19708 -0.0070701 -0.02074 -0.58448 -0.065581 1.3816 -0.9249 0.033565 -0.16956 0.83749 -0.40574 -0.50804 -1.0029 -0.14006 -0.1387 -0.58868 -0.2428 -0.022855 -0.34926 0.053938 -0.60598 -0.031057 0.7892 -0.75688 0.12333 -0.094554 0.042073 -0.30965 0.24605 0.12449 -0.48643 0.23043\n",
      "\n",
      "Skipping line: is name@domain.com -0.1197 0.10706 -0.10519 -0.12412 0.4096 -0.0287 0.34704 0.3549 -0.24818 -1.2408 0.080013 0.72535 0.86863 -0.044256 0.91981 0.7825 1.1884 -1.7801 0.95549 0.18023 -0.47942 0.6672 0.06455 -0.89756 0.50649 0.45807 -0.4137 0.29551 -0.72757 0.35583 0.21081 0.69184 0.17948 -0.38435 -0.82156 0.34316 -0.2292 -0.1174 0.93835 -0.24956 -0.21212 -1.2493 0.34025 -0.60752 0.15362 -0.38889 0.27859 -1.0553 -0.10272 0.71688 0.46476 -0.24386 0.60731 -0.14626 0.49906 -0.031597 -1.3152 -0.37701 -0.15905 0.81663 -0.28852 -0.028202 0.2644 0.36998 -0.54607 -0.26863 -0.10398 -0.16196 -0.71419 0.61126 -0.81195 -0.15081 0.49439 0.31647 0.74124 -0.37209 0.68943 0.19744 0.1258 -0.22018 -0.96408 -0.46702 -0.13894 0.30874 0.50665 0.17943 0.71153 -0.00026381 -0.47585 -0.86687 -0.63838 0.3674 -0.027812 -0.50548 -0.0050586 1.1909 -0.2514 0.422 0.0074521 0.78066 0.13396 0.0063949 0.54671 0.21331 -0.063979 -0.71203 -0.34413 -0.10592 -1.664 0.059908 0.035519 0.74303 -0.71156 -0.083368 0.046354 -0.27944 -0.22258 -0.24948 0.76799 0.012705 -0.55539 0.064894 0.14826 0.41538 0.27372 -0.059421 -0.076653 0.06399 0.083734 0.20121 0.17862 -0.30495 0.2237 0.51722 -0.091633 0.17208 0.12375 0.67001 -0.24705 -0.022074 0.60573 -0.093634 0.18453 0.31808 0.088848 0.40591 0.13941 -1.114 -0.73248 1.0375 -0.36915 1.116 0.5766 0.16222 0.34327 0.46721 0.30673 -0.413 0.041562 0.090004 0.93973 0.18829 0.395 0.10011 1.1547 -0.057363 0.40281 0.77703 -0.4892 -0.73961 0.019956 0.66545 0.28972 -0.023978 0.45869 0.43236 0.64363 0.18573 -0.78063 0.050466 -0.091639 0.40486 0.12737 0.83952 1.0197 0.13825 0.6778 -0.45252 -0.43476 0.69962 0.43987 0.044541 1.1151 0.29316 0.55698 0.29285 0.51472 0.12164 -0.17263 0.68153 -0.15713 -0.0082957 0.052173 0.22946 -0.40682 -0.057423 0.34515 -0.2588 -0.71888 0.27499 0.56278 0.59652 -0.82314 0.53207 -0.56464 0.35201 -0.81956 -0.45959 -0.11181 0.002767 0.20839 -0.11993 -0.16981 0.34309 -0.53543 0.12176 0.15777 -0.22412 0.10459 -0.55587 0.36915 0.014192 -0.61869 -0.34567 -0.60151 -0.1234 -0.59543 -0.62935 -0.35452 0.29317 0.092327 -0.044436 -0.48798 0.9649 0.45625 -0.0059057 -0.43416 -0.71245 0.22928 -0.1943 -0.058306 0.016294 -0.67552 0.49185 0.72063 -0.58111 0.43908 -0.052694 -0.27258 -0.14604 -0.70854 -0.26019 -0.077697 -1.2106 0.23728 0.42626 -1.0457 0.17442 -0.62781 0.80538 0.079948 0.20073 0.2751 0.292 -0.07767 0.12779 0.37107 -0.0064965 0.037248 -0.39539 0.050184 0.20656 0.38481 -0.87068 0.56231 -0.33836 0.14319 0.25696 0.36331 -0.38799 0.58948 -0.33161 -0.071877 0.40965 0.13535 -0.030141 1.0225 0.082464 -0.95272 0.0059679\n",
      "\n",
      "Shape of training data: \n",
      "(891, 100)\n",
      "(891, 8)\n",
      "Shape of test data: \n",
      "(223, 100)\n",
      "(223, 8)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unrecognized keyword arguments passed to Embedding: {'weights': [array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n         0.        ,  0.        ],\n       [-0.082752  ,  0.67203999, -0.14986999, ..., -0.1918    ,\n        -0.37845999, -0.06589   ],\n       [ 0.012001  ,  0.20750999, -0.12578   , ...,  0.13871001,\n        -0.36048999, -0.035     ],\n       ...,\n       [-0.17158   ,  0.40489   , -0.29480001, ...,  0.1188    ,\n         0.24481   ,  0.093008  ],\n       [ 0.02762   , -0.057683  , -0.41659999, ..., -0.18632001,\n         0.30022001,  0.27223   ],\n       [-0.084909  , -0.063605  , -0.21768001, ...,  0.0749    ,\n         0.33508   ,  0.15679   ]])]}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 55\u001b[0m\n\u001b[1;32m     53\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential()\n\u001b[1;32m     54\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Input(shape\u001b[38;5;241m=\u001b[39m(max_len,), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint32\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m---> 55\u001b[0m model\u001b[38;5;241m.\u001b[39madd(\u001b[43mEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43membedding_matrix\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# Pre-trained GloVe embeddings\u001b[39;00m\n\u001b[1;32m     56\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Conv1D(\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m5\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     57\u001b[0m model\u001b[38;5;241m.\u001b[39madd(MaxPooling1D(pool_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:81\u001b[0m, in \u001b[0;36mEmbedding.__init__\u001b[0;34m(self, input_dim, output_dim, embeddings_initializer, embeddings_regularizer, embeddings_constraint, mask_zero, lora_rank, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     72\u001b[0m     input_dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     80\u001b[0m ):\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_dim \u001b[38;5;241m=\u001b[39m input_dim\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_dim \u001b[38;5;241m=\u001b[39m output_dim\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/keras/src/layers/layer.py:265\u001b[0m, in \u001b[0;36mLayer.__init__\u001b[0;34m(self, activity_regularizer, trainable, dtype, autocast, name, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_shape_arg \u001b[38;5;241m=\u001b[39m input_shape_arg\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[0;32m--> 265\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized keyword arguments \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    267\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassed to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    268\u001b[0m     )\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype_policy \u001b[38;5;241m=\u001b[39m dtype_policies\u001b[38;5;241m.\u001b[39mget(dtype)\n",
      "\u001b[0;31mValueError\u001b[0m: Unrecognized keyword arguments passed to Embedding: {'weights': [array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n         0.        ,  0.        ],\n       [-0.082752  ,  0.67203999, -0.14986999, ..., -0.1918    ,\n        -0.37845999, -0.06589   ],\n       [ 0.012001  ,  0.20750999, -0.12578   , ...,  0.13871001,\n        -0.36048999, -0.035     ],\n       ...,\n       [-0.17158   ,  0.40489   , -0.29480001, ...,  0.1188    ,\n         0.24481   ,  0.093008  ],\n       [ 0.02762   , -0.057683  , -0.41659999, ..., -0.18632001,\n         0.30022001,  0.27223   ],\n       [-0.084909  , -0.063605  , -0.21768001, ...,  0.0749    ,\n         0.33508   ,  0.15679   ]])]}"
     ]
    }
   ],
   "source": [
    "# Load GloVe embeddings\n",
    "embedding_dim = 100\n",
    "gloVe_loc = \"../data/glove.840B.300d.txt\"  # Path to GloVe file\n",
    "embeddings_index = {}\n",
    "\n",
    "# Determine embedding dimension automatically\n",
    "with open(gloVe_loc, 'r', encoding='utf-8') as f:\n",
    "    first_line = f.readline()\n",
    "    embedding_dim = len(first_line.split()) - 1  # The number of dimensions is one less than the number of columns\n",
    "\n",
    "# Reload the GloVe file to build the embeddings_index\n",
    "with open(gloVe_loc, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        try:\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "        except ValueError:\n",
    "            print(f\"Skipping line: {line}\")\n",
    "            \n",
    "# Tokenization and Padding using one_hot\n",
    "max_features = 10000  # Max number of words in tokenizer\n",
    "max_len = 100  # Max length of each sequence (pad/truncate to this length)\n",
    "encoded_docs = [one_hot(d, max_features) for d in stripped_data['Text']]\n",
    "x_data = pad_sequences(encoded_docs, maxlen=max_len)\n",
    "y_data = pd.get_dummies(stripped_data['Emotion']).values  # Convert categorical labels to one-hot encoded vectors\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, stratify=stripped_data['Emotion'], random_state=42)\n",
    "print('Shape of training data: ')\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print('Shape of test data: ')\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(stripped_data['Emotion']), y=stripped_data['Emotion'])\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "# Create embedding matrix\n",
    "embedding_matrix = np.zeros((max_features, embedding_dim))\n",
    "word_index = {word: i for i, word in enumerate(embeddings_index.keys(), 1)}  # Build word index\n",
    "for word, i in word_index.items():\n",
    "    if i < max_features:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# Build CNN Model with GloVe embeddings\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(max_len,), dtype='int32'))\n",
    "model.add(Embedding(input_dim=max_features, output_dim=embedding_dim, weights=[embedding_matrix], trainable=False))  # Pre-trained GloVe embeddings\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(y_data.shape[1], activation='softmax'))  # Output layer with softmax activation for multi-class classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Training the model\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test), class_weight=class_weights)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
