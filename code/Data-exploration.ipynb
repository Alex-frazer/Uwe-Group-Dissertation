{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "## MAKE A CNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "malta_loc_18 = '../data/Malta-Budget-2018-dataset-v1.csv'\n",
    "malta_loc_19 = '../data/Malta-Budget-2019-dataset-v1.csv'\n",
    "malta_loc_20 = '../data/Malta-Budget-2020-dataset-v1.csv'\n",
    "\n",
    "malta_data_18 = pd.read_csv(malta_loc_18)\n",
    "malta_data_19 = pd.read_csv(malta_loc_19)\n",
    "malta_data_20 = pd.read_csv(malta_loc_20)\n",
    "\n",
    "print(malta_data_18.info())\n",
    "print(malta_data_19.info())\n",
    "print(malta_data_20.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge data files into single DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "malta_data_19 = malta_data_19.rename(columns={'Off-topic ':'Off-topic'})\n",
    "combined_data = pd.concat([malta_data_18, malta_data_19, malta_data_20], ignore_index=True)\n",
    "combined_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up of data columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = combined_data.dropna(subset=['Online Post Text'])\n",
    "clean_data = clean_data.drop(['Twitter ID', 'Related Online Post ID', 'Source ID','Off-topic'], axis=1)\n",
    "clean_data = clean_data[clean_data['Language'] == 0] # get all data that is in english \n",
    "clean_data = clean_data.drop(['Language'], axis=1)\n",
    "clean_data = clean_data.rename(columns={'Online Post ID':'ID','Online Post Text':'Text'})\n",
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data[\"Emotion\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "value_counts = clean_data['Emotion'].value_counts()\n",
    "\n",
    "# Plotting the frequency of string values in the 'Category' column\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=value_counts.index, y=value_counts.values, palette='viridis', hue=value_counts.index, legend=False)\n",
    "plt.xlabel('Emotion')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Frequency of Emotional Categories In English Comments about The Maltese Budget 2018-2020')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Processing of Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import contractions\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    pattern = re.compile(r'[^a-zA-Z\\s]')\n",
    "    return pattern.sub('', text)\n",
    "\n",
    "# Remove URLs and HTML tags\n",
    "clean_data['Text'] = clean_data['Text'].str.replace(r'http\\S+|www\\S+|https\\S+', '', regex=True)\n",
    "clean_data['Text'] = clean_data['Text'].str.replace(r'<.*?>', '', regex=True)\n",
    "\n",
    "# Expand contractions\n",
    "clean_data['Text'] = clean_data['Text'].apply(lambda x: contractions.fix(x))\n",
    "\n",
    "# Convert to lowercase\n",
    "clean_data['Text'] = clean_data['Text'].str.lower()\n",
    "\n",
    "# Remove punctuation\n",
    "clean_data['Text'] = clean_data['Text'].str.replace(f\"[{string.punctuation}]\", \" \", regex=True)\n",
    "\n",
    "# Remove numbers\n",
    "clean_data['Text'] = clean_data['Text'].str.replace(r'\\d+', '', regex=True)\n",
    "\n",
    "# Remove special characters\n",
    "clean_data['Text'] = clean_data['Text'].apply(remove_special_characters)\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "clean_data['Text'] = clean_data['Text'].apply(lambda x: ' '.join(word for word in x.split() if word not in stop_words))\n",
    "\n",
    "# Remove extra whitespace\n",
    "clean_data['Text'] = clean_data['Text'].str.strip()\n",
    "clean_data['Text'] = clean_data['Text'].str.replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "# Lemmatize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "clean_data['Text'] = clean_data['Text'].apply(lambda x: ' '.join(lemmatizer.lemmatize(word) for word in x.split()))\n",
    "\n",
    "# Tokenize\n",
    "# clean_data['tokens'] = clean_data['Text'].apply(word_tokenize)\n",
    " \n",
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stripped_data = clean_data.drop(columns=['Subjectivity','Sentiment Polarity', 'Sarcasm','Irony','Negation'])\n",
    "stripped_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.28699551569506726\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.19      0.31      0.24        45\n",
      "anticipation       0.43      0.45      0.44        40\n",
      "     disgust       0.32      0.51      0.39        51\n",
      "        fear       0.50      0.11      0.18         9\n",
      "         joy       0.00      0.00      0.00        19\n",
      "     sadness       0.29      0.18      0.22        28\n",
      "    surprise       0.00      0.00      0.00        18\n",
      "       trust       0.00      0.00      0.00        13\n",
      "\n",
      "    accuracy                           0.29       223\n",
      "   macro avg       0.22      0.20      0.18       223\n",
      "weighted avg       0.25      0.29      0.25       223\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "train_df, test_df = train_test_split(stripped_data, test_size=0.2, stratify=stripped_data['Emotion'], random_state=42)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_df['Text'])\n",
    "X_test = vectorizer.transform(test_df['Text'])\n",
    "\n",
    "# Labels\n",
    "y_train = train_df['Emotion']\n",
    "y_test = test_df['Emotion']\n",
    "\n",
    "# Training Naive Bayes classifier\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer, one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, MaxPooling2D, Conv2D\n",
    "\n",
    "# Tokenization and Padding\n",
    "max_features = 10000  # Max number of words in tokenizer\n",
    "max_len = 100  # Max length of each sequence (pad/truncate to this length)\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(stripped_data['Text'])\n",
    "sequences = tokenizer.texts_to_sequences(stripped_data['Text'])\n",
    "x_data = pad_sequences(sequences, maxlen=max_len)\n",
    "y_data = pd.get_dummies(stripped_data['Emotion']).values  # Convert categorical labels to one-hot encoded vectors\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, stratify=stripped_data['Emotion'], random_state=42)\n",
    "print('Shape of training data: ')\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print('Shape of test data: ')\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# CNN Model Architecture\n",
    "embedding_dim = 100\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_features, output_dim=embedding_dim))  # Specify input_dim instead of input_length\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y_data.shape[1], activation='softmax'))  # Output layer with softmax activation for multi-class classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Training the model\n",
    "batch_size = 20\n",
    "epochs = 20\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with One Hot Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and Padding using one_hot\n",
    "max_features = 10000  # Max number of words in tokenizer\n",
    "max_len = 100  # Max length of each sequence (pad/truncate to this length)\n",
    "encoded_docs = [one_hot(d, max_features) for d in stripped_data['Text']]\n",
    "x_data = pad_sequences(encoded_docs, maxlen=max_len)\n",
    "y_data = pd.get_dummies(stripped_data['Emotion']).values  # Convert categorical labels to one-hot encoded vectors\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, stratify=stripped_data['Emotion'], random_state=42)\n",
    "print('Shape of training data: ')\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print('Shape of test data: ')\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# CNN Model Architecture\n",
    "embedding_dim = 100\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_features, output_dim=embedding_dim))  # Specify input_dim instead of input_length\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y_data.shape[1], activation='softmax'))  # Output layer with softmax activation for multi-class classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Training the model\n",
    "batch_size = 20\n",
    "epochs = 20\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with Oridinal encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with VAE endcoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with BagOfWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Bag-of-Words Vectorization\n",
    "max_features = 10000  # Max number of words in tokenizer\n",
    "max_len = 100  # Max length of each sequence (pad/truncate to this length)\n",
    "\n",
    "count_vectorizer = CountVectorizer(max_features=max_features)\n",
    "x_data = count_vectorizer.fit_transform(stripped_data['Text']).toarray()\n",
    "\n",
    "# Convert labels to one-hot encoded vectors\n",
    "y_data = pd.get_dummies(stripped_data['Emotion']).values\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, stratify=stripped_data['Emotion'], random_state=42)\n",
    "\n",
    "print('Shape of training data:')\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print('Shape of test data:')\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# CNN Model Architecture\n",
    "embedding_dim = 100\n",
    "model = Sequential()\n",
    "model.add(Dense(embedding_dim, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "model.add(Reshape((embedding_dim, 1)))  # Reshape to match Conv1D input shape\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y_data.shape[1], activation='softmax'))  # Output layer with softmax activation for multi-class classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Training the model\n",
    "batch_size = 20\n",
    "epochs = 20\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.layers import Reshape\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "max_features = 10000  # Max number of words in tokenizer\n",
    "max_len = 100  # Max length of each sequence (pad/truncate to this length)\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=max_features)\n",
    "x_data = tfidf.fit_transform(stripped_data['Text']).toarray()\n",
    "\n",
    "# Convert labels to one-hot encoded vectors\n",
    "y_data = pd.get_dummies(stripped_data['Emotion']).values\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, stratify=stripped_data['Emotion'], random_state=42)\n",
    "\n",
    "print('Shape of training data:')\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print('Shape of test data:')\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# CNN Model Architecture\n",
    "embedding_dim = 100\n",
    "model = Sequential()\n",
    "model.add(Dense(embedding_dim, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "model.add(Reshape((embedding_dim, 1)))  # Reshape to match Conv1D input shape\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y_data.shape[1], activation='softmax'))  # Output layer with softmax activation for multi-class classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Training the model\n",
    "batch_size = 20\n",
    "epochs = 20\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and Padding using Tokenizer with n-grams\n",
    "max_features = 10000  # Max number of words in tokenizer\n",
    "max_len = 100  # Max length of each sequence (pad/truncate to this length)\n",
    "n_gram_range = 2  # Unigram and bigram\n",
    "\n",
    "# Function to add n-grams\n",
    "def create_ngram_set(input_list, ngram_value=2):\n",
    "    return set(zip(*[input_list[i:] for i in range(ngram_value)]))\n",
    "\n",
    "def add_ngram(sequences, token_index, ngram_range=2):\n",
    "    new_sequences = []\n",
    "    for input_list in sequences:\n",
    "        new_list = input_list[:]\n",
    "        for ngram_value in range(2, ngram_range + 1):\n",
    "            for ngram in create_ngram_set(input_list, ngram_value):\n",
    "                if ngram in token_index:\n",
    "                    new_list.append(token_index[ngram])\n",
    "                else:\n",
    "                    # Add a new token to the index\n",
    "                    token_index[ngram] = len(token_index) + 1\n",
    "                    new_list.append(token_index[ngram])\n",
    "        new_sequences.append(new_list)\n",
    "    return new_sequences\n",
    "\n",
    "# Tokenizer initialization and fitting\n",
    "tokenizer = Tokenizer(num_words=max_features, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(stripped_data['Text'])\n",
    "\n",
    "# Convert texts to sequences\n",
    "sequences = tokenizer.texts_to_sequences(stripped_data['Text'])\n",
    "\n",
    "# Add n-gram features\n",
    "token_index = tokenizer.word_index.copy()  # Copy the word index\n",
    "sequences_ngram = add_ngram(sequences, token_index, n_gram_range)\n",
    "\n",
    "# Pad sequences\n",
    "x_data = pad_sequences(sequences_ngram, maxlen=max_len)\n",
    "\n",
    "# Convert labels to one-hot encoded vectors\n",
    "y_data = pd.get_dummies(stripped_data['Emotion']).values\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, stratify=stripped_data['Emotion'], random_state=42)\n",
    "\n",
    "print('Shape of training data:')\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print('Shape of test data:')\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# CNN Model Architecture\n",
    "embedding_dim = 100\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(token_index) + 1, output_dim=embedding_dim))  # Adjust input_dim\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y_data.shape[1], activation='softmax'))  # Output layer with softmax activation for multi-class classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Training the model\n",
    "batch_size = 20\n",
    "epochs = 20\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Mark II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and Padding using one_hot\n",
    "max_features = 10000  # Max number of words in tokenizer\n",
    "max_len = 100  # Max length of each sequence (pad/truncate to this length)\n",
    "encoded_docs = [one_hot(d, max_features) for d in stripped_data['Text']]\n",
    "x_data = pad_sequences(encoded_docs, maxlen=max_len)\n",
    "y_data = pd.get_dummies(stripped_data['Emotion']).values  # Convert categorical labels to one-hot encoded vectors\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, stratify=stripped_data['Emotion'], random_state=42)\n",
    "print('Shape of training data: ')\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print('Shape of test data: ')\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# CNN Model Architecture\n",
    "embedding_dim = 100\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(max_len,), dtype='int32'))\n",
    "model.add(Embedding(input_dim=max_features, output_dim=embedding_dim))  # Specify input_dim instead of input_length\n",
    "model.add(Conv1D(100, 2, activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Conv1D(100, 3, activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Conv1D(100, 4, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y_data.shape[1], activation='softmax')) \n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Training the model\n",
    "batch_size = 20\n",
    "epochs = 20\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with SMOTE (for undistributed data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Tokenization and Padding using one_hot\n",
    "max_features = 10000  # Max number of words in tokenizer\n",
    "max_len = 100  # Max length of each sequence (pad/truncate to this length)\n",
    "encoded_docs = [one_hot(d, max_features) for d in stripped_data['Text']]\n",
    "x_data = pad_sequences(encoded_docs, maxlen=max_len)\n",
    "y_data = pd.get_dummies(stripped_data['Emotion']).values  # Convert categorical labels to one-hot encoded vectors\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, stratify=stripped_data['Emotion'], random_state=42)\n",
    "print('Shape of training data: ')\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print('Shape of test data: ')\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "smote = SMOTE()\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# CNN Model Architecture\n",
    "embedding_dim = 100\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_features, output_dim=embedding_dim))  # Specify input_dim instead of input_length\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y_data.shape[1], activation='softmax'))  # Output layer with softmax activation for multi-class classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Training the model\n",
    "batch_size = 20#30\n",
    "epochs = 20\n",
    "model.fit(X_train_resampled, y_train_resampled, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with Class Weights (for undistributed data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Tokenization and Padding using one_hot\n",
    "max_features = 10000  # Max number of words in tokenizer\n",
    "max_len = 100  # Max length of each sequence (pad/truncate to this length)\n",
    "encoded_docs = [one_hot(d, max_features) for d in stripped_data['Text']]\n",
    "x_data = pad_sequences(encoded_docs, maxlen=max_len)\n",
    "y_data = pd.get_dummies(stripped_data['Emotion']).values  # Convert categorical labels to one-hot encoded vectors\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, stratify=stripped_data['Emotion'], random_state=42)\n",
    "print('Shape of training data: ')\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print('Shape of test data: ')\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(stripped_data['Emotion']), y=stripped_data['Emotion'])\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "# CNN Model Architecture\n",
    "embedding_dim = 100\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_features, output_dim=embedding_dim))  # Specify input_dim instead of input_length\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y_data.shape[1], activation='softmax'))  # Output layer with softmax activation for multi-class classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Training the model\n",
    "batch_size = 20\n",
    "epochs = 20\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test), class_weight=class_weights)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "# Tokenization and Padding using one_hot\n",
    "max_features = 10000  # Max number of words in tokenizer\n",
    "max_len = 100  # Max length of each sequence (pad/truncate to this length)\n",
    "encoded_docs = [one_hot(d, max_features) for d in stripped_data['Text']]\n",
    "x_data = pad_sequences(encoded_docs, maxlen=max_len)\n",
    "y_data = pd.get_dummies(stripped_data['Emotion']).values  # Convert categorical labels to one-hot encoded vectors\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, stratify=stripped_data['Emotion'], random_state=42)\n",
    "print('Shape of training data: ')\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print('Shape of test data: ')\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(stripped_data['Emotion']), y=stripped_data['Emotion'])\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "adasyn = ADASYN(sampling_strategy='minority')\n",
    "X_res, y_res = adasyn.fit_resample(X_train, y_train)\n",
    "\n",
    "# CNN Model Architecture\n",
    "embedding_dim = 100\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_features, output_dim=embedding_dim))  # Specify input_dim instead of input_length\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y_data.shape[1], activation='softmax'))  # Output layer with softmax activation for multi-class classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Training the model\n",
    "batch_size = 20\n",
    "epochs = 20\n",
    "model.fit(X_res, y_res, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test), class_weight=class_weights)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with NearMiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import NearMiss\n",
    "\n",
    "# Tokenization and Padding using one_hot\n",
    "max_features = 10000  # Max number of words in tokenizer\n",
    "max_len = 100  # Max length of each sequence (pad/truncate to this length)\n",
    "encoded_docs = [one_hot(d, max_features) for d in stripped_data['Text']]\n",
    "x_data = pad_sequences(encoded_docs, maxlen=max_len)\n",
    "y_data = pd.get_dummies(stripped_data['Emotion']).values  # Convert categorical labels to one-hot encoded vectors\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, stratify=stripped_data['Emotion'], random_state=42)\n",
    "print('Shape of training data: ')\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print('Shape of test data: ')\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(stripped_data['Emotion']), y=stripped_data['Emotion'])\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "nearmiss = NearMiss(version=1)\n",
    "X_res, y_res = nearmiss.fit_resample(X_train, y_train)\n",
    "\n",
    "# CNN Model Architecture\n",
    "embedding_dim = 100\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_features, output_dim=embedding_dim))  # Specify input_dim instead of input_length\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y_data.shape[1], activation='softmax'))  # Output layer with softmax activation for multi-class classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Training the model\n",
    "batch_size = 20\n",
    "epochs = 20\n",
    "model.fit(X_res, y_res, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test), class_weight=class_weights)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with Borderline_SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "\n",
    "# Tokenization and Padding using one_hot\n",
    "max_features = 10000  # Max number of words in tokenizer\n",
    "max_len = 100  # Max length of each sequence (pad/truncate to this length)\n",
    "encoded_docs = [one_hot(d, max_features) for d in stripped_data['Text']]\n",
    "x_data = pad_sequences(encoded_docs, maxlen=max_len)\n",
    "y_data = pd.get_dummies(stripped_data['Emotion']).values  # Convert categorical labels to one-hot encoded vectors\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, stratify=stripped_data['Emotion'], random_state=42)\n",
    "print('Shape of training data: ')\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print('Shape of test data: ')\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(stripped_data['Emotion']), y=stripped_data['Emotion'])\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "borderline_smote = BorderlineSMOTE()\n",
    "X_res, y_res = borderline_smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# CNN Model Architecture\n",
    "embedding_dim = 100\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_features, output_dim=embedding_dim))  # Specify input_dim instead of input_length\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y_data.shape[1], activation='softmax'))  # Output layer with softmax activation for multi-class classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Training the model\n",
    "batch_size = 20\n",
    "epochs = 20\n",
    "model.fit(X_res, y_res, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test), class_weight=class_weights)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with SMOTE + Tomek Links (SMOTETomek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "# Tokenization and Padding using one_hot\n",
    "max_features = 10000  # Max number of words in tokenizer\n",
    "max_len = 100  # Max length of each sequence (pad/truncate to this length)\n",
    "encoded_docs = [one_hot(d, max_features) for d in stripped_data['Text']]\n",
    "x_data = pad_sequences(encoded_docs, maxlen=max_len)\n",
    "y_data = pd.get_dummies(stripped_data['Emotion']).values  # Convert categorical labels to one-hot encoded vectors\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, stratify=stripped_data['Emotion'], random_state=42)\n",
    "print('Shape of training data: ')\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print('Shape of test data: ')\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(stripped_data['Emotion']), y=stripped_data['Emotion'])\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "smote_tomek = SMOTETomek()\n",
    "X_res, y_res = smote_tomek.fit_resample(X_train, y_train)\n",
    "\n",
    "# CNN Model Architecture\n",
    "embedding_dim = 100\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_features, output_dim=embedding_dim))  # Specify input_dim instead of input_length\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y_data.shape[1], activation='softmax'))  # Output layer with softmax activation for multi-class classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Training the model\n",
    "batch_size = 20\n",
    "epochs = 20\n",
    "model.fit(X_res, y_res, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test), class_weight=class_weights)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with Word2Vec embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Path to the pre-trained Word2Vec file\n",
    "word2vec_path = '../data/GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "# Load Word2Vec model\n",
    "word2vec = KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000  # Max number of words in tokenizer\n",
    "max_len = 100  # Max length of each sequence (pad/truncate to this length)\n",
    "\n",
    "# Assuming 'stripped_data' contains your text data\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(stripped_data['Text'])\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(stripped_data['Text'])\n",
    "x_data = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# Convert categorical labels to one-hot encoded vectors\n",
    "y_data = pd.get_dummies(stripped_data['Emotion']).values\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "num_words = min(max_features, len(word_index) + 1)\n",
    "embedding_dim = 300  # Assuming your Word2Vec model has embeddings of size 300\n",
    "\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    if word in word2vec:\n",
    "        embedding_matrix[i] = word2vec[word]\n",
    "    # If word not found in embedding index, leave it as zero-vector or initialize randomly\n",
    "    \n",
    "# Optionally, you can normalize the embedding matrix\n",
    "# embedding_matrix = embedding_matrix / np.linalg.norm(embedding_matrix, axis=-1, keepdims=True)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, embedding_dim, weights=[embedding_matrix], trainable=False))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y_data.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, stratify=stripped_data['Emotion'], random_state=42)\n",
    "\n",
    "batch_size = 20\n",
    "epochs = 20\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test))\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN GloVe embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe embeddings\n",
    "embedding_dim = 100\n",
    "# gloVe_loc = \"../data/glove.840B.300d.txt\"  # Path to GloVe file\n",
    "gloVe_loc = \"../data/glove.6B.100d.txt\"\n",
    "embeddings_index = {}\n",
    "\n",
    "# Determine embedding dimension automatically\n",
    "with open(gloVe_loc, 'r', encoding='utf-8') as f:\n",
    "    first_line = f.readline()\n",
    "    embedding_dim = len(first_line.split()) - 1  # The number of dimensions is one less than the number of columns\n",
    "\n",
    "# Reload the GloVe file to build the embeddings_index\n",
    "with open(gloVe_loc, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        try:\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "        except ValueError:\n",
    "            print(f\"Skipping line: {line}\")\n",
    "            \n",
    "# Tokenization and Padding using one_hot\n",
    "max_features = 10000  # Max number of words in tokenizer\n",
    "max_len = 100  # Max length of each sequence (pad/truncate to this length)\n",
    "encoded_docs = [one_hot(d, max_features) for d in stripped_data['Text']]\n",
    "x_data = pad_sequences(encoded_docs, maxlen=max_len)\n",
    "y_data = pd.get_dummies(stripped_data['Emotion']).values  # Convert categorical labels to one-hot encoded vectors\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, stratify=stripped_data['Emotion'], random_state=42)\n",
    "print('Shape of training data: ')\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print('Shape of test data: ')\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(stripped_data['Emotion']), y=stripped_data['Emotion'])\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "# Create embedding matrix\n",
    "embedding_matrix = np.zeros((max_features, embedding_dim))\n",
    "word_index = {word: i for i, word in enumerate(embeddings_index.keys(), 1)}  # Build word index\n",
    "for word, i in word_index.items():\n",
    "    if i < max_features:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# Build CNN Model with GloVe embeddings\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(max_len,), dtype='int32'))\n",
    "model.add(Embedding(input_dim=max_features, output_dim=embedding_dim, weights=[embedding_matrix], trainable=False))  # Pre-trained GloVe embeddings\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(y_data.shape[1], activation='softmax'))  # Output layer with softmax activation for multi-class classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Training the model\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test), class_weight=class_weights)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "# Tokenization and Padding using one_hot\n",
    "max_features = 10000  # Max number of words in tokenizer\n",
    "max_len = 100  # Max length of each sequence (pad/truncate to this length)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_features, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(stripped_data['Text'])\n",
    "\n",
    "# Convert texts to sequences\n",
    "sequences = tokenizer.texts_to_sequences(stripped_data['Text'])\n",
    "\n",
    "# Pad sequences\n",
    "x_data = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# Convert labels to one-hot encoded vectors\n",
    "y_data = pd.get_dummies(stripped_data['Emotion']).values\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, stratify=stripped_data['Emotion'], random_state=42)\n",
    "\n",
    "print('Shape of training data:')\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print('Shape of test data:')\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(stripped_data['Emotion']), y=stripped_data['Emotion'])\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "# LSTM Model Architecture\n",
    "embedding_dim = 100\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_features, output_dim=embedding_dim))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(y_data.shape[1], activation='softmax'))  # Output layer with softmax activation for multi-class classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Training the model\n",
    "batch_size = 20\n",
    "epochs = 20\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test), class_weight=class_weights)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with Contextualised Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize and encode sequences in the dataset\n",
    "def encode_texts(texts, tokenizer, max_len):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for text in texts:\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=False,\n",
    "            truncation=True\n",
    "        )\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "    \n",
    "    return np.array(input_ids), np.array(attention_masks)\n",
    "\n",
    "max_len = 100  # Max length of each sequence\n",
    "input_ids, attention_masks = encode_texts(stripped_data['Text'].tolist(), tokenizer, max_len)\n",
    "\n",
    "# Convert labels to one-hot encoded vectors\n",
    "y_data = pd.get_dummies(stripped_data['Emotion']).values\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train_ids, X_test_ids, y_train, y_test = train_test_split(input_ids, y_data, test_size=0.2, stratify=stripped_data['Emotion'], random_state=42)\n",
    "X_train_masks, X_test_masks, _, _ = train_test_split(attention_masks, y_data, test_size=0.2, stratify=stripped_data['Emotion'], random_state=42)\n",
    "\n",
    "print('Shape of training data:')\n",
    "print(X_train_ids.shape)\n",
    "print(y_train.shape)\n",
    "print('Shape of test data:')\n",
    "print(X_test_ids.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# Build the model\n",
    "input_ids = Input(shape=(max_len,), dtype='int32', name='input_ids')\n",
    "input_masks = Input(shape=(max_len,), dtype='int32', name='attention_masks')\n",
    "\n",
    "bert_output = bert_model(input_ids, attention_mask=input_masks)[0]\n",
    "x = LSTM(128, return_sequences=True)(bert_output)\n",
    "x = Dropout(0.2)(x)\n",
    "x = LSTM(128)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "output = Dense(y_data.shape[1], activation='softmax')(x)  # Output layer with softmax activation for multi-class classification\n",
    "\n",
    "model = Model(inputs=[input_ids, input_masks], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=3e-5)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Training the model\n",
    "batch_size = 16\n",
    "epochs = 3\n",
    "model.fit([X_train_ids, X_train_masks], y_train, batch_size=batch_size, epochs=epochs, validation_data=([X_test_ids, X_test_masks], y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate([X_test_ids, X_test_masks], y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with ELMo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.elmo import Elmo, batch_to_ids\n",
    "# Load ELMo model\n",
    "options_file = 'https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway_options.json'\n",
    "weight_file = 'https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway_weights.hdf5'\n",
    "elmo = Elmo(options_file, weight_file, 2, dropout=0)\n",
    "\n",
    "# Tokenize text and convert to character ids\n",
    "def tokenize_texts(texts):\n",
    "    sentences = [text.split() for text in texts]\n",
    "    character_ids = batch_to_ids(sentences)\n",
    "    return character_ids\n",
    "\n",
    "# Convert texts to character ids for ELMo\n",
    "character_ids = tokenize_texts(stripped_data['Text'].tolist())\n",
    "\n",
    "# Extract ELMo embeddings\n",
    "def get_elmo_embeddings(character_ids):\n",
    "    embeddings = elmo(character_ids)['elmo_representations'][0]\n",
    "    return embeddings.numpy()\n",
    "\n",
    "elmo_embeddings = get_elmo_embeddings(character_ids)\n",
    "\n",
    "# Average the ELMo embeddings across tokens\n",
    "def average_elmo_embeddings(elmo_embeddings):\n",
    "    return np.mean(elmo_embeddings, axis=1)\n",
    "\n",
    "x_data = average_elmo_embeddings(elmo_embeddings)\n",
    "\n",
    "# Convert labels to one-hot encoded vectors\n",
    "y_data = pd.get_dummies(stripped_data['Emotion']).values\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, stratify=stripped_data['Emotion'], random_state=42)\n",
    "\n",
    "print('Shape of training data:')\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print('Shape of test data:')\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# Build the model\n",
    "input_dim = X_train.shape[1]  # Number of features in the ELMo embeddings\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "x = Dense(128, activation='relu')(input_layer)\n",
    "x = Dropout(0.2)(x)\n",
    "x = LSTM(128)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "output = Dense(y_data.shape[1], activation='softmax')(x)  # Output layer with softmax activation for multi-class classification\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Training the model\n",
    "batch_size = 20\n",
    "epochs = 20\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN with RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, TFRobertaModel\n",
    "\n",
    "# Load pre-trained RoBERTa model and tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "roberta_model = TFRobertaModel.from_pretrained('roberta-base')\n",
    "\n",
    "# Tokenize and encode sequences in the dataset\n",
    "def encode_texts(texts, tokenizer, max_len):\n",
    "    encodings = tokenizer(\n",
    "        texts, \n",
    "        truncation=True, \n",
    "        padding=True, \n",
    "        max_length=max_len, \n",
    "        return_tensors='tf'\n",
    "    )\n",
    "    return encodings['input_ids'], encodings['attention_mask']\n",
    "\n",
    "max_len = 100  # Max length of each sequence\n",
    "input_ids, attention_masks = encode_texts(stripped_data['Text'].tolist(), tokenizer, max_len)\n",
    "\n",
    "# Convert labels to one-hot encoded vectors\n",
    "y_data = pd.get_dummies(stripped_data['Emotion']).values\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train_ids, X_test_ids, y_train, y_test = train_test_split(input_ids, y_data, test_size=0.2, stratify=stripped_data['Emotion'], random_state=42)\n",
    "X_train_masks, X_test_masks, _, _ = train_test_split(attention_masks, y_data, test_size=0.2, stratify=stripped_data['Emotion'], random_state=42)\n",
    "\n",
    "print('Shape of training data:')\n",
    "print(X_train_ids.shape)\n",
    "print(y_train.shape)\n",
    "print('Shape of test data:')\n",
    "print(X_test_ids.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# Build the model\n",
    "input_ids_layer = Input(shape=(max_len,), dtype=tf.int32, name='input_ids')\n",
    "attention_mask_layer = Input(shape=(max_len,), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "roberta_output = roberta_model(input_ids_layer, attention_mask=attention_mask_layer)[0]\n",
    "x = tf.reduce_mean(roberta_output, axis=1)  # Average pooling over the sequence\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = LSTM(128)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "output = Dense(y_data.shape[1], activation='softmax')(x)  # Output layer with softmax activation for multi-class classification\n",
    "\n",
    "model = Model(inputs=[input_ids_layer, attention_mask_layer], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Training the model\n",
    "batch_size = 20\n",
    "epochs = 20\n",
    "model.fit([X_train_ids, X_train_masks], y_train, batch_size=batch_size, epochs=epochs, validation_data=([X_test_ids, X_test_masks], y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate([X_test_ids, X_test_masks], y_test)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
